{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83460ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.special import binom\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets.utils import download_url\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as tt\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data_utils\n",
    "import torch.nn.init as init\n",
    "\n",
    "from collections import Counter\n",
    "import operator\n",
    "import copy\n",
    "from itertools import product,combinations\n",
    "from time import time\n",
    "#from IPython.core.display import display\n",
    "\n",
    "#%matplotlib inline\n",
    "\n",
    "## code extracted from https://www.kaggle.com/code/graymant/breast-cancer-diagnosis-with-pytorch\n",
    "## SV code extracted from https://github.com/mburaksayici/ExplainableAI-Pure-Numpy/blob/main/KernelSHAP-Pure-Numpy.ipynb\n",
    "\n",
    "import yaml\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Combinatorial UCB\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4721426e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local 100\n",
      "mean:  99.3605076885526\n",
      "std:  0.1475832702741608\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "experiments = ['MNIST_IFCL_1','MNIST_IFCL_2','MNIST_IFCL_3','MNIST_IFCL_4','MNIST_IFCL_5']\n",
    "print('local 100')\n",
    "accuracies = []\n",
    "for i,experiment in enumerate(experiments): \n",
    "    test_accuracy = np.loadtxt(os.path.join('checkpoints_bandits',experiment,'test_accuracy.txt'))\n",
    "  #  print(test_accuracy)\n",
    "    accuracies.append(test_accuracy)\n",
    "print('mean: ',np.mean(accuracies))\n",
    "print('std: ',np.std(accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa350a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "local 10\n",
      "mean:  39.807692631915714\n",
      "std:  1.752137794883438\n",
      "5\n",
      "local 25\n",
      "mean:  38.259108972840714\n",
      "std:  3.6074997510040467\n",
      "5\n",
      "local 50\n",
      "mean:  38.79828817574191\n",
      "std:  1.2483442331395387\n",
      "local 100\n",
      "mean:  53.928405600855726\n",
      "std:  6.36173610362499\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print('5')\n",
    "experiments = ['BIH_IFCL_1_010','BIH_IFCL_2_010','BIH_IFCL_3_010','BIH_IFCL_4_010','BIH_IFCL_5_010']\n",
    "print('local 10')\n",
    "accuracies = []\n",
    "for i,experiment in enumerate(experiments): \n",
    "    test_accuracy = np.loadtxt(os.path.join('checkpoints_bandits',experiment,'test_accuracy.txt'))\n",
    "  #  print(test_accuracy)\n",
    "    accuracies.append(test_accuracy)\n",
    "print('mean: ',np.mean(accuracies)*100)\n",
    "print('std: ',np.std(accuracies)*100)\n",
    "\n",
    "print('5')\n",
    "experiments = ['BIH_IFCL_1_025','BIH_IFCL_2_025','BIH_IFCL_3_025','BIH_IFCL_4_025','BIH_IFCL_5_025']\n",
    "print('local 25')\n",
    "accuracies = []\n",
    "for i,experiment in enumerate(experiments): \n",
    "    test_accuracy = np.loadtxt(os.path.join('checkpoints_bandits',experiment,'test_accuracy.txt'))\n",
    "  #  print(test_accuracy)\n",
    "    accuracies.append(test_accuracy)\n",
    "print('mean: ',np.mean(accuracies)*100)\n",
    "print('std: ',np.std(accuracies)*100)\n",
    "\n",
    "print('5')\n",
    "experiments = ['BIH_IFCL_1_05','BIH_IFCL_2_05','BIH_IFCL_3_05','BIH_IFCL_4_05','BIH_IFCL_5_05']\n",
    "print('local 50')\n",
    "accuracies = []\n",
    "for i,experiment in enumerate(experiments): \n",
    "    test_accuracy = np.loadtxt(os.path.join('checkpoints_bandits',experiment,'test_accuracy.txt'))\n",
    "  #  print(test_accuracy)\n",
    "    accuracies.append(test_accuracy)\n",
    "print('mean: ',np.mean(accuracies)*100)\n",
    "print('std: ',np.std(accuracies)*100)\n",
    "\n",
    "experiments = ['BIH_IFCL_1','BIH_IFCL_2','BIH_IFCL_3','BIH_IFCL_4','BIH_IFCL_5']\n",
    "print('local 100')\n",
    "accuracies = []\n",
    "for i,experiment in enumerate(experiments): \n",
    "    test_accuracy = np.loadtxt(os.path.join('checkpoints_bandits',experiment,'test_accuracy.txt'))\n",
    "  #  print(test_accuracy)\n",
    "    accuracies.append(test_accuracy)\n",
    "print('mean: ',np.mean(accuracies)*100)\n",
    "print('std: ',np.std(accuracies)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "730dc700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "local 10\n",
      "mean:  37.36049714409319\n",
      "std:  2.301842232928358\n",
      "5\n",
      "local 25\n",
      "mean:  39.13407353845229\n",
      "std:  2.861349249952125\n",
      "5\n",
      "local 50\n",
      "mean:  39.3093443247819\n",
      "std:  1.8222328787678894\n",
      "local 100\n",
      "mean:  59.29874636567185\n",
      "std:  8.473420459572502\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print('10')\n",
    "experiments = ['BIH_IFCL_1_010_10','BIH_IFCL_2_010_10','BIH_IFCL_3_010_10','BIH_IFCL_4_010_10','BIH_IFCL_5_010_10']\n",
    "print('local 10')\n",
    "accuracies = []\n",
    "for i,experiment in enumerate(experiments): \n",
    "    test_accuracy = np.loadtxt(os.path.join('checkpoints_bandits',experiment,'test_accuracy.txt'))\n",
    "  #  print(test_accuracy)\n",
    "    accuracies.append(test_accuracy)\n",
    "print('mean: ',np.mean(accuracies)*100)\n",
    "print('std: ',np.std(accuracies)*100)\n",
    "\n",
    "print('5')\n",
    "experiments = ['BIH_IFCL_1_025_10','BIH_IFCL_2_025_10','BIH_IFCL_3_025_10','BIH_IFCL_4_025_10','BIH_IFCL_5_025_10']\n",
    "print('local 25')\n",
    "accuracies = []\n",
    "for i,experiment in enumerate(experiments): \n",
    "    test_accuracy = np.loadtxt(os.path.join('checkpoints_bandits',experiment,'test_accuracy.txt'))\n",
    "  #  print(test_accuracy)\n",
    "    accuracies.append(test_accuracy)\n",
    "print('mean: ',np.mean(accuracies)*100)\n",
    "print('std: ',np.std(accuracies)*100)\n",
    "\n",
    "print('5')\n",
    "experiments = ['BIH_IFCL_1_05_10','BIH_IFCL_2_05_10','BIH_IFCL_3_05_10','BIH_IFCL_4_05_10','BIH_IFCL_5_05_10']\n",
    "print('local 50')\n",
    "accuracies = []\n",
    "for i,experiment in enumerate(experiments): \n",
    "    test_accuracy = np.loadtxt(os.path.join('checkpoints_bandits',experiment,'test_accuracy.txt'))\n",
    "  #  print(test_accuracy)\n",
    "    accuracies.append(test_accuracy)\n",
    "print('mean: ',np.mean(accuracies)*100)\n",
    "print('std: ',np.std(accuracies)*100)\n",
    "\n",
    "experiments = ['BIH_IFCL_1_10','BIH_IFCL_2_10','BIH_IFCL_3_10','BIH_IFCL_4_10','BIH_IFCL_5_10']\n",
    "print('local 100')\n",
    "accuracies = []\n",
    "for i,experiment in enumerate(experiments): \n",
    "    test_accuracy = np.loadtxt(os.path.join('checkpoints_bandits',experiment,'test_accuracy.txt'))\n",
    "  #  print(test_accuracy)\n",
    "    accuracies.append(test_accuracy)\n",
    "print('mean: ',np.mean(accuracies)*100)\n",
    "print('std: ',np.std(accuracies)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23dbeeea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "local 10\n",
      "mean:  38.98346789078511\n",
      "std:  2.9586957773894293\n",
      "5\n",
      "local 25\n",
      "mean:  38.70498749668862\n",
      "std:  4.102937258083734\n",
      "5\n",
      "local 50\n",
      "mean:  39.47766706186692\n",
      "std:  1.6874293452251055\n",
      "local 100\n",
      "mean:  58.28754909324962\n",
      "std:  8.842187543076323\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print('20')\n",
    "experiments = ['BIH_IFCL_1_010_20','BIH_IFCL_2_010_20','BIH_IFCL_3_010_20','BIH_IFCL_4_010_20','BIH_IFCL_5_010_20']\n",
    "print('local 10')\n",
    "accuracies = []\n",
    "for i,experiment in enumerate(experiments): \n",
    "    test_accuracy = np.loadtxt(os.path.join('checkpoints_bandits',experiment,'test_accuracy.txt'))\n",
    "  #  print(test_accuracy)\n",
    "    accuracies.append(test_accuracy)\n",
    "print('mean: ',np.mean(accuracies)*100)\n",
    "print('std: ',np.std(accuracies)*100)\n",
    "\n",
    "print('5')\n",
    "experiments = ['BIH_IFCL_1_025_20','BIH_IFCL_2_025_20','BIH_IFCL_3_025_20','BIH_IFCL_4_025_20','BIH_IFCL_5_025_20']\n",
    "print('local 25')\n",
    "accuracies = []\n",
    "for i,experiment in enumerate(experiments): \n",
    "    test_accuracy = np.loadtxt(os.path.join('checkpoints_bandits',experiment,'test_accuracy.txt'))\n",
    "  #  print(test_accuracy)\n",
    "    accuracies.append(test_accuracy)\n",
    "print('mean: ',np.mean(accuracies)*100)\n",
    "print('std: ',np.std(accuracies)*100)\n",
    "\n",
    "print('5')\n",
    "experiments = ['BIH_IFCL_1_05_20','BIH_IFCL_2_05_20','BIH_IFCL_3_05_20','BIH_IFCL_4_05_20','BIH_IFCL_5_05_20']\n",
    "print('local 50')\n",
    "accuracies = []\n",
    "for i,experiment in enumerate(experiments): \n",
    "    test_accuracy = np.loadtxt(os.path.join('checkpoints_bandits',experiment,'test_accuracy.txt'))\n",
    "  #  print(test_accuracy)\n",
    "    accuracies.append(test_accuracy)\n",
    "print('mean: ',np.mean(accuracies)*100)\n",
    "print('std: ',np.std(accuracies)*100)\n",
    "\n",
    "experiments = ['BIH_IFCL_1_20','BIH_IFCL_2_20','BIH_IFCL_3_20','BIH_IFCL_4_20','BIH_IFCL_5_20']\n",
    "print('local 100')\n",
    "accuracies = []\n",
    "for i,experiment in enumerate(experiments): \n",
    "    test_accuracy = np.loadtxt(os.path.join('checkpoints_bandits',experiment,'test_accuracy.txt'))\n",
    "  #  print(test_accuracy)\n",
    "    accuracies.append(test_accuracy)\n",
    "print('mean: ',np.mean(accuracies)*100)\n",
    "print('std: ',np.std(accuracies)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a10dbf74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "local 25\n",
      "mean:  52.49404559373937\n",
      "std:  3.0941892067887555\n",
      "local 25\n",
      "mean:  55.95100374276965\n",
      "std:  3.660868886394478\n",
      "local 50\n",
      "mean:  59.92514460700918\n",
      "std:  3.2919355930623473\n",
      "local 100\n",
      "mean:  69.7107859816264\n",
      "std:  2.307639690530805\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print('5')\n",
    "experiments = ['MIT_IFCL_1_010','MIT_IFCL_2_010','MIT_IFCL_3_010','MIT_IFCL_4_010','MIT_IFCL_5_010']\n",
    "print('local 25')\n",
    "accuracies = []\n",
    "for i,experiment in enumerate(experiments): \n",
    "    test_accuracy = np.loadtxt(os.path.join('checkpoints_bandits',experiment,'test_accuracy.txt'))\n",
    "  #  print(test_accuracy)\n",
    "    accuracies.append(test_accuracy)\n",
    "print('mean: ',np.mean(accuracies))\n",
    "print('std: ',np.std(accuracies))\n",
    "\n",
    "experiments = ['MIT_IFCL_1_025','MIT_IFCL_2_025','MIT_IFCL_3_025','MIT_IFCL_4_025','MIT_IFCL_5_025']\n",
    "print('local 25')\n",
    "accuracies = []\n",
    "for i,experiment in enumerate(experiments): \n",
    "    test_accuracy = np.loadtxt(os.path.join('checkpoints_bandits',experiment,'test_accuracy.txt'))\n",
    "  #  print(test_accuracy)\n",
    "    accuracies.append(test_accuracy)\n",
    "print('mean: ',np.mean(accuracies))\n",
    "print('std: ',np.std(accuracies))\n",
    "experiments = ['MIT_IFCL_1_05','MIT_IFCL_2_05','MIT_IFCL_3_05','MIT_IFCL_4_05','MIT_IFCL_5_05']\n",
    "print('local 50')\n",
    "accuracies = []\n",
    "for i,experiment in enumerate(experiments): \n",
    "    test_accuracy = np.loadtxt(os.path.join('checkpoints_bandits',experiment,'test_accuracy.txt'))\n",
    "  #  print(test_accuracy)\n",
    "    accuracies.append(test_accuracy)\n",
    "print('mean: ',np.mean(accuracies))\n",
    "print('std: ',np.std(accuracies))\n",
    "\n",
    "experiments = ['MIT_IFCL_1','MIT_IFCL_2','MIT_IFCL_3','MIT_IFCL_4','MIT_IFCL_5']\n",
    "print('local 100')\n",
    "accuracies = []\n",
    "for i,experiment in enumerate(experiments): \n",
    "    test_accuracy = np.loadtxt(os.path.join('checkpoints_bandits',experiment,'test_accuracy.txt'))\n",
    "  #  print(test_accuracy)\n",
    "    accuracies.append(test_accuracy)\n",
    "print('mean: ',np.mean(accuracies))\n",
    "print('std: ',np.std(accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e4dd16dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "local 25\n",
      "mean:  51.03776794828172\n",
      "std:  2.919590324616714\n",
      "local 25\n",
      "mean:  57.16910513780198\n",
      "std:  4.215081097161855\n",
      "local 50\n",
      "mean:  62.39537257570602\n",
      "std:  4.391405079120286\n",
      "local 100\n",
      "mean:  67.85981626403539\n",
      "std:  3.221348948701304\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print('10')\n",
    "experiments = ['MIT_IFCL_1_010_10','MIT_IFCL_2_010_10','MIT_IFCL_3_010_10','MIT_IFCL_4_010_10','MIT_IFCL_5_010_10']\n",
    "print('local 25')\n",
    "accuracies = []\n",
    "for i,experiment in enumerate(experiments): \n",
    "    test_accuracy = np.loadtxt(os.path.join('checkpoints_bandits',experiment,'test_accuracy.txt'))\n",
    "  #  print(test_accuracy)\n",
    "    accuracies.append(test_accuracy)\n",
    "print('mean: ',np.mean(accuracies))\n",
    "print('std: ',np.std(accuracies))\n",
    "\n",
    "experiments = ['MIT_IFCL_1_025_10','MIT_IFCL_2_025_10','MIT_IFCL_3_025_10','MIT_IFCL_4_025_10','MIT_IFCL_5_025_10']\n",
    "print('local 25')\n",
    "accuracies = []\n",
    "for i,experiment in enumerate(experiments): \n",
    "    test_accuracy = np.loadtxt(os.path.join('checkpoints_bandits',experiment,'test_accuracy.txt'))\n",
    "  #  print(test_accuracy)\n",
    "    accuracies.append(test_accuracy)\n",
    "print('mean: ',np.mean(accuracies))\n",
    "print('std: ',np.std(accuracies))\n",
    "experiments = ['MIT_IFCL_1_05_10','MIT_IFCL_2_05_10','MIT_IFCL_3_05_10','MIT_IFCL_4_05_10','MIT_IFCL_5_05_10']\n",
    "print('local 50')\n",
    "accuracies = []\n",
    "for i,experiment in enumerate(experiments): \n",
    "    test_accuracy = np.loadtxt(os.path.join('checkpoints_bandits',experiment,'test_accuracy.txt'))\n",
    "  #  print(test_accuracy)\n",
    "    accuracies.append(test_accuracy)\n",
    "print('mean: ',np.mean(accuracies))\n",
    "print('std: ',np.std(accuracies))\n",
    "\n",
    "experiments = ['MIT_IFCL_1_10','MIT_IFCL_2_10','MIT_IFCL_3_10','MIT_IFCL_4_10','MIT_IFCL_5_10']\n",
    "print('local 100')\n",
    "accuracies = []\n",
    "for i,experiment in enumerate(experiments): \n",
    "    test_accuracy = np.loadtxt(os.path.join('checkpoints_bandits',experiment,'test_accuracy.txt'))\n",
    "  #  print(test_accuracy)\n",
    "    accuracies.append(test_accuracy)\n",
    "print('mean: ',np.mean(accuracies))\n",
    "print('std: ',np.std(accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "af8ff514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "local 25\n",
      "mean:  53.67131677441306\n",
      "std:  3.137453585287943\n",
      "local 25\n",
      "mean:  57.597822388567536\n",
      "std:  2.971346984380506\n",
      "local 50\n",
      "mean:  60.93909493024838\n",
      "std:  3.9436928758858834\n",
      "local 100\n",
      "mean:  68.45185437223547\n",
      "std:  1.828314408194172\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print('10')\n",
    "experiments = ['MIT_IFCL_1_010_20','MIT_IFCL_2_010_20','MIT_IFCL_3_010_20','MIT_IFCL_4_010_20','MIT_IFCL_5_010_20']\n",
    "print('local 25')\n",
    "accuracies = []\n",
    "for i,experiment in enumerate(experiments): \n",
    "    test_accuracy = np.loadtxt(os.path.join('checkpoints_bandits',experiment,'test_accuracy.txt'))\n",
    "  #  print(test_accuracy)\n",
    "    accuracies.append(test_accuracy)\n",
    "print('mean: ',np.mean(accuracies))\n",
    "print('std: ',np.std(accuracies))\n",
    "\n",
    "experiments = ['MIT_IFCL_1_025_20','MIT_IFCL_2_025_20','MIT_IFCL_3_025_20','MIT_IFCL_4_025_20','MIT_IFCL_5_025_20']\n",
    "print('local 25')\n",
    "accuracies = []\n",
    "for i,experiment in enumerate(experiments): \n",
    "    test_accuracy = np.loadtxt(os.path.join('checkpoints_bandits',experiment,'test_accuracy.txt'))\n",
    "  #  print(test_accuracy)\n",
    "    accuracies.append(test_accuracy)\n",
    "print('mean: ',np.mean(accuracies))\n",
    "print('std: ',np.std(accuracies))\n",
    "experiments = ['MIT_IFCL_1_05_20','MIT_IFCL_2_05_20','MIT_IFCL_3_05_20','MIT_IFCL_4_05_20','MIT_IFCL_5_05_20']\n",
    "print('local 50')\n",
    "accuracies = []\n",
    "for i,experiment in enumerate(experiments): \n",
    "    test_accuracy = np.loadtxt(os.path.join('checkpoints_bandits',experiment,'test_accuracy.txt'))\n",
    "  #  print(test_accuracy)\n",
    "    accuracies.append(test_accuracy)\n",
    "print('mean: ',np.mean(accuracies))\n",
    "print('std: ',np.std(accuracies))\n",
    "\n",
    "experiments = ['MIT_IFCL_1_20','MIT_IFCL_2_20','MIT_IFCL_3_20','MIT_IFCL_4_20','MIT_IFCL_5_20']\n",
    "print('local 100')\n",
    "accuracies = []\n",
    "for i,experiment in enumerate(experiments): \n",
    "    test_accuracy = np.loadtxt(os.path.join('checkpoints_bandits',experiment,'test_accuracy.txt'))\n",
    "  #  print(test_accuracy)\n",
    "    accuracies.append(test_accuracy)\n",
    "print('mean: ',np.mean(accuracies))\n",
    "print('std: ',np.std(accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9661642",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a370b44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init():\n",
    "    with open('settings/train_settings_bandits.yaml', 'r') as file:\n",
    "        settings = yaml.safe_load(file)\n",
    "    if not os.path.isdir('checkpoints_bandits'):\n",
    "        os.mkdir('checkpoints_bandits')\n",
    "    if not os.path.isdir(os.path.join('checkpoints_bandits', settings['experiment_name'])):\n",
    "        os.mkdir(os.path.join('checkpoints_bandits', settings['experiment_name']))\n",
    "    save_dir = os.path.join('checkpoints_bandits', settings['experiment_name'])\n",
    "    if not os.path.isdir(os.path.join(save_dir, 'model')):\n",
    "        os.mkdir(os.path.join(save_dir, 'model'))\n",
    "    shutil.copyfile('settings/train_settings_bandits.yaml', save_dir + '/train_settings.yaml')\n",
    "    return settings,save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98fedf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9c51fe17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experiment_name': 'MNIST_test_IFCL', 'Date': '30_06_2023', 'Dataset': 'MNIST', 'n_clients': 100, 'n_clients_UCB': 1, 'seed': 42, 'n_classes_total': 4, 'n_classes_per_user': 2, 'data_fraction': 0.5, 'alpha': 0.75, 'network': 'Net', 'n_epochs': 100, 'local_iterations': 1, 'type': 'ifca', 'calc_accuracy': 'True', 'disp_loss': 'True', 'log_interval': 25, 'save_models': 'True'}\n"
     ]
    }
   ],
   "source": [
    "settings, save_dir = init()\n",
    "print(settings)\n",
    "from models import Net \n",
    "from data import FEMNIST_dataset, Partition_MNIST_NIID\n",
    "from data import MNIST_NIID_dataset \n",
    "import tensorflow.keras as tk \n",
    "import random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0514c209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [3 1]\n",
      " [3 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [3 2]\n",
      " [0 2]\n",
      " [3 1]\n",
      " [1 2]\n",
      " [0 1]\n",
      " [0 3]\n",
      " [0 2]\n",
      " [0 1]\n",
      " [1 3]\n",
      " [0 2]\n",
      " [1 3]\n",
      " [3 2]\n",
      " [1 3]\n",
      " [1 2]\n",
      " [1 3]\n",
      " [2 0]\n",
      " [3 2]\n",
      " [0 3]\n",
      " [2 0]\n",
      " [2 3]\n",
      " [3 2]\n",
      " [1 3]\n",
      " [2 3]\n",
      " [3 2]\n",
      " [3 2]\n",
      " [0 1]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [1 3]\n",
      " [2 0]\n",
      " [3 2]\n",
      " [0 3]\n",
      " [3 1]\n",
      " [3 2]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [3 2]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 2]\n",
      " [1 3]\n",
      " [2 3]\n",
      " [2 1]\n",
      " [2 3]\n",
      " [0 1]\n",
      " [1 2]\n",
      " [1 3]\n",
      " [0 1]\n",
      " [2 3]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [2 1]\n",
      " [2 0]\n",
      " [2 1]\n",
      " [0 3]\n",
      " [1 2]\n",
      " [1 0]\n",
      " [2 0]\n",
      " [0 3]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 3]\n",
      " [0 2]\n",
      " [1 3]\n",
      " [2 0]\n",
      " [1 0]\n",
      " [2 3]\n",
      " [1 2]\n",
      " [2 3]\n",
      " [2 1]\n",
      " [2 1]\n",
      " [3 0]\n",
      " [0 1]\n",
      " [3 1]\n",
      " [3 0]\n",
      " [2 0]\n",
      " [2 3]\n",
      " [1 2]\n",
      " [3 0]\n",
      " [1 0]\n",
      " [3 0]\n",
      " [0 2]\n",
      " [1 3]\n",
      " [3 2]\n",
      " [1 2]\n",
      " [3 0]\n",
      " [0 1]\n",
      " [2 3]\n",
      " [3 0]\n",
      " [2 0]\n",
      " [1 0]\n",
      " [2 0]\n",
      " [0 1]]\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = tk.datasets.mnist.load_data()\n",
    "instance = Partition_MNIST_NIID(train_data[0], train_data[1])\n",
    "classes_per_user = settings['n_classes_per_user']\n",
    "total_clients = 100 \n",
    "n_classes_total = settings['n_classes_total']\n",
    "train_partition = instance.create_partition(n_classes_total, classes_per_user, total_clients)\n",
    "test_instance = Partition_MNIST_NIID(test_data[0], test_data[1])\n",
    "test_partition = test_instance.create_partition_test(instance.sample_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3e8ca5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[243, 245, 245, 249, 250, 245, 245, 248, 244, 249, 243, 245, 250, 244, 245, 249, 244, 249, 248, 249, 243, 249, 244, 248, 250, 244, 248, 248, 249, 248, 248, 248, 245, 243, 249, 249, 244, 248, 250, 249, 248, 245, 245, 248, 245, 245, 243, 249, 248, 243, 248, 245, 243, 249, 245, 248, 243, 249, 243, 244, 243, 250, 243, 245, 244, 250, 245, 245, 249, 244, 249, 244, 245, 248, 243, 248, 243, 243, 250, 245, 249, 250, 244, 248, 243, 250, 245, 250, 244, 249, 248, 243, 250, 245, 248, 250, 244, 245, 244, 245]\n",
      "[194, 196, 196, 199, 200, 196, 196, 198, 195, 199, 194, 196, 200, 195, 196, 199, 195, 199, 198, 199, 194, 199, 195, 198, 200, 195, 198, 198, 199, 198, 198, 198, 196, 194, 199, 199, 195, 198, 200, 199, 198, 196, 196, 198, 196, 196, 194, 199, 198, 194, 198, 196, 194, 199, 196, 198, 194, 199, 194, 195, 194, 200, 194, 196, 195, 200, 196, 196, 199, 195, 199, 195, 196, 198, 194, 198, 194, 194, 200, 196, 199, 200, 195, 198, 194, 200, 196, 200, 195, 199, 198, 194, 200, 196, 198, 200, 195, 196, 195, 196]\n",
      "[49, 49, 49, 50, 50, 49, 49, 50, 49, 50, 49, 49, 50, 49, 49, 50, 49, 50, 50, 50, 49, 50, 49, 50, 50, 49, 50, 50, 50, 50, 50, 50, 49, 49, 50, 50, 49, 50, 50, 50, 50, 49, 49, 50, 49, 49, 49, 50, 50, 49, 50, 49, 49, 50, 49, 50, 49, 50, 49, 49, 49, 50, 49, 49, 49, 50, 49, 49, 50, 49, 50, 49, 49, 50, 49, 50, 49, 49, 50, 49, 50, 50, 49, 50, 49, 50, 49, 50, 49, 50, 50, 49, 50, 49, 50, 50, 49, 49, 49, 49]\n"
     ]
    }
   ],
   "source": [
    " # split train dataset into train and val\n",
    "print([len(x) for x in train_partition.values()])\n",
    "fraction = 0.8\n",
    "#print([int(np.floor(len(x)*0.8)) for x in train_partition.values()])\n",
    "train_length = [int(np.floor(len(x)*0.8)) for x in train_partition.values()]\n",
    "\n",
    "train_partition2 = {}\n",
    "val_partition = {}\n",
    "\n",
    "for key in train_partition.keys():\n",
    "    og_length = len(train_partition[key])\n",
    "    og_samples = np.array(train_partition[key])\n",
    "    list1 = [x for x in range(og_length)]\n",
    "    train_samples = np.random.choice(list1,size=int(np.floor(og_length*fraction)),replace=False)\n",
    "    val_samples = [x for x in range(og_length) if x not in train_samples]\n",
    "    train_partition2[key] = og_samples[train_samples]\n",
    "    val_partition[key] = og_samples[val_samples]\n",
    "print([len(x) for x in train_partition2.values()])\n",
    "print([len(x) for x in val_partition.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "938eb99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mimer/NOBACKUP/groups/naiss2023-22-980/arthur/code/Federated_Averaging/models/FEMNIST_model.py:67: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full train loss:  tensor(0.0171, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.0221, device='cuda:0', dtype=torch.float64)\n",
      "we are done\n",
      "val accuracy:  98.76792567158151\n",
      "test accuracy:  99.20462762111352\n",
      "0\n",
      "accuracy is best accuracy\n",
      "[99.4059405940594, 99.6923076923077, 99.6923076923077, 99.7584541062802, 99.51807228915662, 99.6923076923077, 100.0, 97.47899159663865, 98.41897233201581, 99.7584541062802]\n",
      "1\n",
      "full train loss:  tensor(0.0109, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.0163, device='cuda:0', dtype=torch.float64)\n",
      "we are done\n",
      "val accuracy:  99.07089476873358\n",
      "test accuracy:  99.44564955410942\n",
      "98.76792567158151\n",
      "accuracy is best accuracy\n",
      "[99.60396039603961, 99.6923076923077, 99.6923076923077, 99.7584541062802, 99.51807228915662, 99.6923076923077, 100.0, 98.82352941176471, 98.41897233201581, 100.0]\n",
      "2\n",
      "full train loss:  tensor(0.0059, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.0141, device='cuda:0', dtype=torch.float64)\n",
      "we are done\n",
      "val accuracy:  99.4344576853161\n",
      "test accuracy:  99.61436490720655\n",
      "99.07089476873358\n",
      "accuracy is best accuracy\n",
      "[99.60396039603961, 100.0, 99.6923076923077, 99.7584541062802, 100.0, 99.6923076923077, 100.0, 98.99159663865547, 99.2094861660079, 99.7584541062802]\n",
      "3\n",
      "full train loss:  tensor(0.0036, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.0112, device='cuda:0', dtype=torch.float64)\n",
      "we are done\n",
      "val accuracy:  99.47485356493637\n",
      "test accuracy:  99.80718245360329\n",
      "99.4344576853161\n",
      "accuracy is best accuracy\n",
      "[99.60396039603961, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.49579831932773, 99.40711462450594, 100.0]\n",
      "4\n",
      "full train loss:  tensor(0.0030, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.0179, device='cuda:0', dtype=torch.float64)\n",
      "we are done\n",
      "val accuracy:  99.59604120379721\n",
      "test accuracy:  99.7589780670041\n",
      "99.47485356493637\n",
      "accuracy is best accuracy\n",
      "[99.60396039603961, 100.0, 100.0, 99.7584541062802, 99.75903614457832, 99.6923076923077, 100.0, 99.49579831932773, 99.60474308300395, 100.0]\n",
      "5\n",
      "full train loss:  tensor(0.0038, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.0223, device='cuda:0', dtype=torch.float64)\n",
      "we are done\n",
      "val accuracy:  99.53544738436679\n",
      "test accuracy:  99.80718245360329\n",
      "6\n",
      "full train loss:  tensor(0.0035, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.0186, device='cuda:0', dtype=torch.float64)\n",
      "we are done\n",
      "val accuracy:  99.59604120379721\n",
      "test accuracy:  99.85538684020246\n",
      "7\n",
      "full train loss:  tensor(0.0020, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.0204, device='cuda:0', dtype=torch.float64)\n",
      "we are done\n",
      "val accuracy:  99.73742678246819\n",
      "test accuracy:  99.85538684020246\n",
      "99.59604120379721\n",
      "accuracy is best accuracy\n",
      "[99.8019801980198, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.66386554621849, 99.40711462450594, 100.0]\n",
      "8\n",
      "full train loss:  tensor(0.0012, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.0245, device='cuda:0', dtype=torch.float64)\n",
      "we are done\n",
      "val accuracy:  99.71722884265805\n",
      "test accuracy:  99.83128464690287\n",
      "9\n",
      "full train loss:  tensor(0.0015, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.0263, device='cuda:0', dtype=torch.float64)\n",
      "we are done\n",
      "val accuracy:  99.57584326398707\n",
      "test accuracy:  99.80718245360329\n",
      "10\n",
      "full train loss:  tensor(0.0012, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.0239, device='cuda:0', dtype=torch.float64)\n",
      "we are done\n",
      "val accuracy:  99.63643708341749\n",
      "test accuracy:  99.90359122680164\n",
      "11\n",
      "full train loss:  tensor(0.0013, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.0245, device='cuda:0', dtype=torch.float64)\n",
      "we are done\n",
      "val accuracy:  99.63643708341749\n",
      "test accuracy:  99.85538684020246\n",
      "12\n",
      "full train loss:  tensor(0.0005, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.0226, device='cuda:0', dtype=torch.float64)\n",
      "we are done\n",
      "val accuracy:  99.71722884265805\n",
      "test accuracy:  99.87948903350204\n",
      "13\n",
      "full train loss:  tensor(0.0004, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.0241, device='cuda:0', dtype=torch.float64)\n",
      "we are done\n",
      "val accuracy:  99.71722884265805\n",
      "test accuracy:  99.90359122680164\n",
      "14\n",
      "full train loss:  tensor(0.0003, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.0245, device='cuda:0', dtype=torch.float64)\n",
      "we are done\n",
      "val accuracy:  99.65663502322764\n",
      "test accuracy:  99.92769342010122\n",
      "15\n",
      "full train loss:  tensor(0.0011, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.0259, device='cuda:0', dtype=torch.float64)\n",
      "we are done\n",
      "val accuracy:  99.61623914360736\n",
      "test accuracy:  99.87948903350204\n",
      "16\n",
      "full train loss:  tensor(0.0001, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.0264, device='cuda:0', dtype=torch.float64)\n",
      "we are done\n",
      "val accuracy:  99.67683296303777\n",
      "test accuracy:  99.90359122680164\n",
      "17\n",
      "full train loss:  tensor(0.0002, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.0276, device='cuda:0', dtype=torch.float64)\n",
      "we are done\n",
      "val accuracy:  99.6970309028479\n",
      "test accuracy:  99.92769342010122\n",
      "18\n",
      "full train loss:  tensor(0.0002, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.0297, device='cuda:0', dtype=torch.float64)\n",
      "we are done\n",
      "val accuracy:  99.65663502322764\n",
      "test accuracy:  99.90359122680164\n",
      "19\n",
      "full train loss:  tensor(0.0001, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.0328, device='cuda:0', dtype=torch.float64)\n",
      "we are done\n",
      "val accuracy:  99.63643708341749\n",
      "test accuracy:  99.92769342010122\n",
      "20\n",
      "full train loss:  tensor(3.8013e-05, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.0326, device='cuda:0', dtype=torch.float64)\n",
      "we are done\n",
      "val accuracy:  99.63643708341749\n",
      "test accuracy:  99.90359122680164\n",
      "21\n",
      "full train loss:  tensor(4.1430e-05, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.0310, device='cuda:0', dtype=torch.float64)\n",
      "we are done\n",
      "val accuracy:  99.67683296303777\n",
      "test accuracy:  99.90359122680164\n",
      "22\n",
      "full train loss:  tensor(0.0001, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.0318, device='cuda:0', dtype=torch.float64)\n",
      "we are done\n",
      "val accuracy:  99.65663502322764\n",
      "test accuracy:  99.92769342010122\n",
      "23\n",
      "full train loss:  tensor(7.1322e-06, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.0318, device='cuda:0', dtype=torch.float64)\n",
      "we are done\n",
      "val accuracy:  99.6970309028479\n",
      "test accuracy:  99.92769342010122\n",
      "24\n",
      "full train loss:  tensor(4.6321e-05, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.0351, device='cuda:0', dtype=torch.float64)\n",
      "we are done\n",
      "val accuracy:  99.65663502322764\n",
      "test accuracy:  99.92769342010122\n",
      "25\n",
      "full train loss:  tensor(1.9177e-05, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.0304, device='cuda:0', dtype=torch.float64)\n",
      "we are done\n",
      "val accuracy:  99.75762472227832\n",
      "test accuracy:  99.92769342010122\n",
      "99.73742678246819\n",
      "accuracy is best accuracy\n",
      "[99.8019801980198, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.83193277310924, 99.80237154150198, 100.0]\n",
      "26\n",
      "full train loss:  tensor(1.1102e-06, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.0343, device='cuda:0', dtype=torch.float64)\n",
      "we are done\n",
      "val accuracy:  99.65663502322764\n",
      "test accuracy:  99.92769342010122\n",
      "27\n",
      "full train loss:  tensor(5.0913e-06, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.0349, device='cuda:0', dtype=torch.float64)\n",
      "we are done\n",
      "val accuracy:  99.63643708341749\n",
      "test accuracy:  99.92769342010122\n",
      "28\n",
      "full train loss:  tensor(8.3183e-06, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.0341, device='cuda:0', dtype=torch.float64)\n",
      "we are done\n",
      "val accuracy:  99.65663502322764\n",
      "test accuracy:  99.92769342010122\n",
      "29\n",
      "full train loss:  tensor(9.3753e-07, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.0345, device='cuda:0', dtype=torch.float64)\n",
      "we are done\n",
      "val accuracy:  99.6970309028479\n",
      "test accuracy:  99.92769342010122\n",
      "30\n",
      "full train loss:  tensor(7.8979e-06, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.0337, device='cuda:0', dtype=torch.float64)\n",
      "we are done\n",
      "val accuracy:  99.73742678246819\n",
      "test accuracy:  99.90359122680164\n",
      "31\n",
      "full train loss:  tensor(1.0037e-06, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.0339, device='cuda:0', dtype=torch.float64)\n",
      "we are done\n",
      "val accuracy:  99.6970309028479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy:  99.92769342010122\n",
      "32\n",
      "full train loss:  tensor(3.1618e-06, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.0355, device='cuda:0', dtype=torch.float64)\n",
      "we are done\n",
      "val accuracy:  99.67683296303777\n",
      "test accuracy:  99.92769342010122\n",
      "33\n",
      "full train loss:  tensor(4.6766e-06, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.0352, device='cuda:0', dtype=torch.float64)\n",
      "we are done\n",
      "val accuracy:  99.63643708341749\n",
      "test accuracy:  99.92769342010122\n",
      "34\n",
      "full train loss:  tensor(2.0160e-06, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.0354, device='cuda:0', dtype=torch.float64)\n",
      "we are done\n",
      "val accuracy:  99.65663502322764\n",
      "test accuracy:  99.92769342010122\n",
      "35\n",
      "full train loss:  tensor(7.6000e-08, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.0347, device='cuda:0', dtype=torch.float64)\n",
      "we are done\n",
      "val accuracy:  99.6970309028479\n",
      "test accuracy:  99.92769342010122\n",
      "36\n",
      "full train loss:  tensor(6.7618e-08, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.0356, device='cuda:0', dtype=torch.float64)\n",
      "we are done\n",
      "val accuracy:  99.67683296303777\n",
      "test accuracy:  99.92769342010122\n",
      "37\n",
      "full train loss:  tensor(7.3913e-07, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.0336, device='cuda:0', dtype=torch.float64)\n",
      "we are done\n",
      "val accuracy:  99.71722884265805\n",
      "test accuracy:  99.90359122680164\n",
      "38\n",
      "full train loss:  tensor(3.0929e-07, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.0370, device='cuda:0', dtype=torch.float64)\n",
      "we are done\n",
      "val accuracy:  99.67683296303777\n",
      "test accuracy:  99.92769342010122\n",
      "39\n",
      "full train loss:  tensor(4.8799e-07, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.0386, device='cuda:0', dtype=torch.float64)\n",
      "we are done\n",
      "val accuracy:  99.65663502322764\n",
      "test accuracy:  99.92769342010122\n",
      "40\n",
      "full train loss:  tensor(7.2336e-07, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.0391, device='cuda:0', dtype=torch.float64)\n",
      "we are done\n",
      "val accuracy:  99.67683296303777\n",
      "test accuracy:  99.92769342010122\n",
      "41\n",
      "full train loss:  tensor(7.4754e-08, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.0397, device='cuda:0', dtype=torch.float64)\n",
      "we are done\n",
      "val accuracy:  99.65663502322764\n",
      "test accuracy:  99.90359122680164\n",
      "42\n",
      "full train loss:  tensor(1.8778e-07, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.0406, device='cuda:0', dtype=torch.float64)\n",
      "we are done\n",
      "val accuracy:  99.67683296303777\n",
      "test accuracy:  99.90359122680164\n",
      "43\n",
      "full train loss:  tensor(2.1076e-08, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.0377, device='cuda:0', dtype=torch.float64)\n",
      "we are done\n",
      "val accuracy:  99.67683296303777\n",
      "test accuracy:  99.92769342010122\n",
      "44\n",
      "full train loss:  tensor(3.1445e-08, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.0367, device='cuda:0', dtype=torch.float64)\n",
      "we are done\n",
      "val accuracy:  99.71722884265805\n",
      "test accuracy:  99.92769342010122\n",
      "45\n",
      "full train loss:  tensor(4.7801e-08, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.0383, device='cuda:0', dtype=torch.float64)\n",
      "we are done\n",
      "val accuracy:  99.67683296303777\n",
      "test accuracy:  99.92769342010122\n",
      "46\n",
      "full train loss:  tensor(1.9767e-07, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.0368, device='cuda:0', dtype=torch.float64)\n",
      "we are done\n",
      "val accuracy:  99.67683296303777\n",
      "test accuracy:  99.92769342010122\n",
      "47\n",
      "full train loss:  tensor(1.4976e-07, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.0359, device='cuda:0', dtype=torch.float64)\n",
      "we are done\n",
      "val accuracy:  99.67683296303777\n",
      "test accuracy:  99.90359122680164\n",
      "48\n",
      "full train loss:  tensor(4.5129e-08, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.0375, device='cuda:0', dtype=torch.float64)\n",
      "we are done\n",
      "val accuracy:  99.65663502322764\n",
      "test accuracy:  99.92769342010122\n",
      "49\n",
      "full train loss:  tensor(2.8981e-08, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.0379, device='cuda:0', dtype=torch.float64)\n",
      "we are done\n",
      "val accuracy:  99.6970309028479\n",
      "test accuracy:  99.92769342010122\n",
      "50\n",
      "full train loss:  tensor(1.3739e-08, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.0370, device='cuda:0', dtype=torch.float64)\n",
      "we are done\n",
      "val accuracy:  99.71722884265805\n",
      "test accuracy:  99.92769342010122\n",
      "51\n",
      "full train loss:  tensor(1.7858e-07, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.0371, device='cuda:0', dtype=torch.float64)\n",
      "we are done\n",
      "val accuracy:  99.71722884265805\n",
      "test accuracy:  99.92769342010122\n",
      "52\n",
      "full train loss:  tensor(8.0626e-07, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.0369, device='cuda:0', dtype=torch.float64)\n",
      "we are done\n",
      "val accuracy:  99.71722884265805\n",
      "test accuracy:  99.92769342010122\n",
      "53\n",
      "full train loss:  tensor(5.1628e-07, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.0370, device='cuda:0', dtype=torch.float64)\n",
      "we are done\n",
      "val accuracy:  99.71722884265805\n",
      "test accuracy:  99.92769342010122\n",
      "54\n",
      "full train loss:  tensor(3.6576e-08, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.0385, device='cuda:0', dtype=torch.float64)\n",
      "we are done\n",
      "val accuracy:  99.67683296303777\n",
      "test accuracy:  99.92769342010122\n",
      "55\n",
      "full train loss:  tensor(3.2268e-08, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.0389, device='cuda:0', dtype=torch.float64)\n",
      "we are done\n",
      "val accuracy:  99.6970309028479\n",
      "test accuracy:  99.92769342010122\n",
      "56\n",
      "full train loss:  tensor(1.9298e-08, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.0384, device='cuda:0', dtype=torch.float64)\n",
      "we are done\n",
      "val accuracy:  99.6970309028479\n",
      "test accuracy:  99.92769342010122\n",
      "57\n",
      "full train loss:  tensor(3.3283e-08, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.0378, device='cuda:0', dtype=torch.float64)\n",
      "we are done\n",
      "val accuracy:  99.6970309028479\n",
      "test accuracy:  99.92769342010122\n",
      "58\n",
      "full train loss:  tensor(5.8218e-09, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.0376, device='cuda:0', dtype=torch.float64)\n",
      "we are done\n",
      "val accuracy:  99.71722884265805\n",
      "test accuracy:  99.92769342010122\n",
      "59\n",
      "full train loss:  tensor(1.9786e-09, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.0381, device='cuda:0', dtype=torch.float64)\n",
      "we are done\n",
      "val accuracy:  99.71722884265805\n",
      "test accuracy:  99.92769342010122\n",
      "60\n",
      "full train loss:  tensor(2.4024e-09, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.0385, device='cuda:0', dtype=torch.float64)\n",
      "we are done\n",
      "val accuracy:  99.71722884265805\n",
      "test accuracy:  99.92769342010122\n",
      "61\n",
      "full train loss:  tensor(3.5295e-09, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.0388, device='cuda:0', dtype=torch.float64)\n",
      "we are done\n",
      "val accuracy:  99.6970309028479\n",
      "test accuracy:  99.92769342010122\n",
      "62\n",
      "full train loss:  tensor(2.3349e-08, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.0386, device='cuda:0', dtype=torch.float64)\n",
      "we are done\n",
      "val accuracy:  99.6970309028479\n",
      "test accuracy:  99.90359122680164\n",
      "63\n",
      "full train loss:  tensor(7.9293e-09, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.0384, device='cuda:0', dtype=torch.float64)\n",
      "we are done\n",
      "val accuracy:  99.6970309028479\n",
      "test accuracy:  99.92769342010122\n",
      "64\n",
      "full train loss:  tensor(7.9411e-10, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.0391, device='cuda:0', dtype=torch.float64)\n",
      "we are done\n",
      "val accuracy:  99.67683296303777\n",
      "test accuracy:  99.92769342010122\n",
      "65\n",
      "full train loss:  tensor(3.7758e-09, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.0382, device='cuda:0', dtype=torch.float64)\n",
      "we are done\n",
      "val accuracy:  99.71722884265805\n",
      "test accuracy:  99.92769342010122\n",
      "66\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(test)\n\u001b[1;32m     15\u001b[0m p2p \u001b[38;5;241m=\u001b[39m P2P_AFPL(total_clients, train_data, train_partition2, val_partition, test_data, test_partition,settings[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_clients_UCB\u001b[39m\u001b[38;5;124m'\u001b[39m], settings[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m'\u001b[39m],test)\n\u001b[0;32m---> 16\u001b[0m phis \u001b[38;5;241m=\u001b[39m \u001b[43mp2p\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloop\u001b[49m\u001b[43m(\u001b[49m\u001b[43msettings\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mn_epochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msettings\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mexperiment_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 720\u001b[0m, in \u001b[0;36mP2P_AFPL.loop\u001b[0;34m(self, epochs, p2p, experiment_name)\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselected_clients \u001b[38;5;241m=\u001b[39m [x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_clients)]\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcentralized\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 720\u001b[0m     loss_train, loss_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_local_models\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselected_clients\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    722\u001b[0m     loss_train, loss_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcentralized(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselected_clients)\n",
      "Cell \u001b[0;32mIn[10], line 106\u001b[0m, in \u001b[0;36mP2P_AFPL.update_local_models\u001b[0;34m(self, selected_clients)\u001b[0m\n\u001b[1;32m    103\u001b[0m         loss_global\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    104\u001b[0m         optimizer_global\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m--> 106\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient_models[\u001b[38;5;28mstr\u001b[39m(i)]\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m/mimer/NOBACKUP/groups/naiss2023-22-980/arthur/anaconda3/envs/py38/lib/python3.8/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mimer/NOBACKUP/groups/naiss2023-22-980/arthur/anaconda3/envs/py38/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import collections\n",
    "from time import time\n",
    "import random\n",
    "\n",
    "seed = settings['seed']\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "test = 'local' #settings['type']\n",
    "data_fraction = 1  # we're using all of the data \n",
    "print(test)\n",
    "p2p = P2P_AFPL(total_clients, train_data, train_partition2, val_partition, test_data, test_partition,settings['n_clients_UCB'], settings['alpha'],test)\n",
    "phis = p2p.loop(settings['n_epochs'], p2p, settings['experiment_name'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8a20edd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IFCA():\n",
    "    def __init__(self ,total_clients ,train_data ,train_partition ,val_partition ,test_data ,test_partition,n_clients_selected\n",
    "                 ,alpha = 0.25, test='AFPL'):\n",
    "        self.network = Net('MNIST_niid')\n",
    "        self.total_clients = total_clients\n",
    "        self.client_models = {}\n",
    "        self.optimizers = {}\n",
    "        self.dataloaders = {}\n",
    "        self.len = {}\n",
    "        self.len_test = {}\n",
    "        self.len_really_test = {}\n",
    "        self.dataloaders_test = {}\n",
    "        self.dataloaders_really_test = {}\n",
    "        self.best_test_loss = {}\n",
    "        self.best_test_loss_global = 1000000\n",
    "        self.current_test_loss = {}\n",
    "        self.current_train_loss = {}\n",
    "        self.test = test\n",
    "       \n",
    "\n",
    "        for i in range(total_clients):\n",
    "            self.client_models[str(i)] = copy.deepcopy(self.network).double().cuda()\n",
    "            self.optimizers[str(i)] = torch.optim.SGD(self.client_models[str(i)].parameters() ,lr=0.008 ,momentum=0.5)\n",
    "            if data_fraction != 1:\n",
    "                dataset_train= MNIST_NIID_dataset(train_data[0][blub] ,train_data[1][blub] ,train_partition ,i)\n",
    "            else:\n",
    "                dataset_train = MNIST_NIID_dataset(train_data[0], train_data[1], train_partition, i)\n",
    "\n",
    "            if i == 1:\n",
    "                if data_fraction != 1:\n",
    "                    dataset = torch.utils.data.ConcatDataset([dataset_train,MNIST_NIID_dataset(train_data[0][blub] ,train_data[1][blub] ,train_partition,i )])\n",
    "                else:\n",
    "                    dataset = torch.utils.data.ConcatDataset(\n",
    "                        [dataset_train, MNIST_NIID_dataset(train_data[0], train_data[1], train_partition, i)])\n",
    "            if i > 1 :\n",
    "                if data_fraction != 1:\n",
    "                    dataset = torch.utils.data.ConcatDataset(\n",
    "                    [dataset,MNIST_NIID_dataset(train_data[0][blub] ,train_data[1][blub] ,train_partition,i )])\n",
    "                else:\n",
    "                    dataset = torch.utils.data.ConcatDataset(\n",
    "                        [dataset, MNIST_NIID_dataset(train_data[0], train_data[1], train_partition, i)])\n",
    "\n",
    "\n",
    "            self.len[str(i) ]= len(dataset_train)\n",
    "            self.dataloaders[str(i)] = DataLoader(dataset_train ,batch_size=16 ,shuffle=True)\n",
    "            if data_fraction !=1:\n",
    "                dataset_test= MNIST_NIID_dataset(train_data[0][blub] ,train_data[1][blub] ,val_partition,i  )\n",
    "            else:\n",
    "                dataset_test = MNIST_NIID_dataset(train_data[0], train_data[1], val_partition, i)\n",
    "\n",
    "            dataset_really_test = MNIST_NIID_dataset(test_data[0],test_data[1],test_partition,i)\n",
    "            self.len_really_test[str(i)] = len(dataset_really_test)\n",
    "            self.dataloaders_really_test[str(i)] = DataLoader(dataset_really_test,batch_size=16,shuffle=True)\n",
    "            self.len_test[str(i)] = len(dataset_test)\n",
    "            self.dataloaders_test[str(i)] = DataLoader(dataset_test ,batch_size=16 ,shuffle=False)\n",
    "            self.best_test_loss[str(i)] = 10000000\n",
    "            self.current_test_loss[str(i)] = 100000\n",
    "            self.current_train_loss[str(i)] = 1000000\n",
    "          \n",
    "        self.dataset_train = dataset_train\n",
    "        self.dataloader_centralized = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "        \n",
    "    def init_ifca(self, k): \n",
    "        # setup models \n",
    "        self.global_models_ifca = {}\n",
    "        for i in range(k):\n",
    "            self.global_models_ifca[str(i)] = copy.deepcopy(self.network).double().cuda()\n",
    "\n",
    "        # initialize cluster assignment to be random \n",
    "        self.cluster_assign = np.random.randint(0, k, size=self.total_clients)\n",
    "    \n",
    "    def run_ifca(self, selected_clients): \n",
    "        \n",
    "        for idx, i in enumerate(selected_clients): \n",
    "            # find current cluster assignment \n",
    "            k_i = self.cluster_assign[i]\n",
    "            # extract the appropriate model \n",
    "            model = copy.deepcopy(self.global_models_ifca[str(k_i)])\n",
    "            dataloader = self.dataloaders[str(i)]\n",
    "            optimizer = torch.optim.Adam(model.parameters() ,lr=0.001 *0.95**self.iteration)\n",
    "            # perform local training \n",
    "            for batch_idx, (data, target) in enumerate(dataloader):\n",
    "                data = data.double().cuda()\n",
    "                target =target.long().cuda()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                output = model(data)\n",
    "                loss = F.nll_loss(output ,target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            # save the trained model at the client side \n",
    "            self.client_models[str(i)] = copy.deepcopy(model)\n",
    "            \n",
    "    def cluster_ifca(self, selected_clients, k): \n",
    "        \n",
    "        losses = np.zeros((len(selected_clients),k))\n",
    "        for idx, i in enumerate(selected_clients): \n",
    "            dataloader = self.dataloaders[str(i)]\n",
    "            for batch_idx, (data, target) in enumerate(dataloader):\n",
    "                for k_i in range(k): \n",
    "                    data = data.double().cuda()\n",
    "                    target =target.long().cuda()\n",
    "                    output = self.global_models_ifca[str(k_i)](data)\n",
    "                    loss = F.nll_loss(output ,target)\n",
    "                    losses[idx,k_i] += loss \n",
    "                    \n",
    "        #print(losses)\n",
    "        print(np.min(losses,axis=1))\n",
    "        self.cluster_assign = np.argmin(losses,axis=1)\n",
    "        \n",
    "    def combine_ifca(self, selected_clients, k):\n",
    "        print(self.cluster_assign)\n",
    "        for k_i in range(k): \n",
    "            clients_in_ki = [i for i in selected_clients if self.cluster_assign[i] == k_i]\n",
    "            \n",
    "            if len(clients_in_ki) > 0: \n",
    "                # do this only if there's at least once client per thing \n",
    "                shared_model = copy.deepcopy(self.global_models_ifca[str(k_i)]).double().cuda()\n",
    "                n_clients = len(clients_in_ki)\n",
    "                weight = [1/n_clients for x in range(n_clients)]\n",
    "\n",
    "                for idx, i in enumerate(clients_in_ki):\n",
    "                    for (name, param), (name2, param2) in zip(shared_model.named_parameters()\n",
    "                            , self.client_models[str(i)].named_parameters()):\n",
    "                        if idx == 0:\n",
    "                            param.data = torch.zeros(param.shape).cuda().double()\n",
    "                        param.data += weight[idx] * param2.data\n",
    "\n",
    "                self.global_models_ifca[str(k_i)] = shared_model.double().eval()\n",
    "\n",
    "    def update_local_models(self ,selected_clients):\n",
    "        self.dw = {}\n",
    "        loss_test = 0\n",
    "        loss_test2 = 0\n",
    "        losses = 0\n",
    "        losses2 = 0\n",
    "        loss_test3 = 0\n",
    "        losses3 = 0\n",
    "\n",
    "        for idx ,i in enumerate(selected_clients):\n",
    "            self.client_models[str(i)].eval()\n",
    "            dataloader_test = self.dataloaders_test[str(i)]\n",
    "            loss_test = 0\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (data, target) in enumerate(dataloader_test):\n",
    "                    data = data.double().cuda()\n",
    "                    target =target.long().cuda()\n",
    "\n",
    "                    output = self.client_models[str(i)](data)\n",
    "                    loss_test += F.nll_loss(output ,target)\n",
    "                self.current_test_loss[str(i)] = loss_test /self.len_test[str(i)]\n",
    "                if self.current_test_loss[str(i)] < self.best_test_loss[str(i)]:\n",
    "                    torch.save(self.client_models[str(i)].state_dict(), os.path.join(save_dir, 'model', 'best_model ' +str(i ) +'.pt'))\n",
    "                    self.best_test_loss[str(i)] = self.current_test_loss[str(i)]\n",
    "\n",
    "            losses += loss_test /self.len_test[str(i)]\n",
    "            loss_test2 = 0\n",
    "            self.client_models[str(i)].eval()\n",
    "            dataloader = self.dataloaders[str(i)]\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (data, target) in enumerate(dataloader):\n",
    "                    data = data.double().cuda()\n",
    "                    target = target.long().cuda()\n",
    "\n",
    "                    output = self.client_models[str(i)](data)\n",
    "                    loss_test2 += F.nll_loss(output, target)\n",
    "\n",
    "            losses2 += loss_test2 / self.len[str(i)]\n",
    "            self.current_train_loss[str(i)] = loss_test2 / self.len[str(i)]\n",
    "\n",
    "        print('full train loss: ', losses2)\n",
    "        print('full loss: ', losses)\n",
    "\n",
    "        return losses2, losses\n",
    "\n",
    "\n",
    "    \n",
    "    def calc_accuracy(self, dataloader, length):\n",
    "        accuracies = np.zeros(len(self.selected_clients))\n",
    "        total = 0\n",
    "        self.accuracy_list = []\n",
    "        for i in self.selected_clients:\n",
    "            intermediate_accuracy = 0\n",
    "\n",
    "            for batch_idx, (data, target) in enumerate(dataloader[str(i)]):\n",
    "                data = data.double().cuda()\n",
    "                target = target.long().cuda()\n",
    "                output = self.client_models[str(i)](data)\n",
    "                output_array = output.detach().cpu().numpy()\n",
    "                output_class = np.argmax(output_array, axis=-1)\n",
    "                target_array = target.detach().cpu().numpy()\n",
    "                intermediate_accuracy += np.sum(output_class == target_array)\n",
    "            accuracy = intermediate_accuracy / length[str(i)]* 100\n",
    "            total += length[str(i)]\n",
    "\n",
    "            self.accuracy_list.append(accuracy)\n",
    "            accuracies[i] = intermediate_accuracy\n",
    "        overall_accuracy = np.sum(accuracies) / total * 100\n",
    "        return overall_accuracy\n",
    "\n",
    "    def loop(self, epochs, experiment_name, k):\n",
    "\n",
    "        loss_tests = []\n",
    "        loss_trains = []\n",
    "        loss_tests2 = []\n",
    "        loss_trains2 = []\n",
    "        accuracies = []\n",
    "        accuracies_train = []\n",
    "        best_accuracy = 0\n",
    "\n",
    "        self.selected_clients_arr = np.zeros((epochs, self.total_clients, self.total_clients))\n",
    "        self.init_ifca(k)\n",
    "\n",
    "        for i in range(epochs):\n",
    "            print(i)\n",
    "            list1 = []\n",
    "            self.selected_clients = [x for x in range(self.total_clients)]\n",
    "            self.iteration = i\n",
    "            if i != 0: \n",
    "                self.cluster_ifca(self.selected_clients,k)\n",
    "            self.run_ifca(self.selected_clients)\n",
    "            self.combine_ifca(self.selected_clients,k)\n",
    "\n",
    "            loss_train, loss_test = self.update_local_models(self.selected_clients)\n",
    "            \n",
    "            loss_tests.append(loss_test.detach().cpu().numpy())\n",
    "            loss_trains.append(loss_train.detach().cpu().numpy())\n",
    "            \n",
    "            accuracy_val = self.calc_accuracy(self.dataloaders_test, self.len_test)\n",
    "            print('val accuracy: ', accuracy_val)\n",
    "\n",
    "            accuracy = self.calc_accuracy(self.dataloaders_really_test, self.len_really_test)\n",
    "            print('test accuracy: ', accuracy)\n",
    "            accuracies.append(accuracy)\n",
    "            if accuracy_val > best_accuracy:\n",
    "                print(best_accuracy)\n",
    "                print('accuracy is best accuracy')\n",
    "                print(self.accuracy_list)\n",
    "                best_accuracy = accuracy_val\n",
    "\n",
    "                # save all of this in a .txt file\n",
    "                fname = os.path.join('checkpoints_bandits', experiment_name, 'test_accuracies.txt')\n",
    "                np.savetxt(fname, self.accuracy_list)\n",
    "                fname = os.path.join('checkpoints_bandits', experiment_name, 'test_accuracy.txt')\n",
    "                np.savetxt(fname, [accuracy])\n",
    "\n",
    "\n",
    "        # print(self.phis)\n",
    "        fname = os.path.join('checkpoints_bandits', experiment_name, 'accuracies.txt')\n",
    "        np.savetxt(fname, accuracies)\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_trains, label='train loss before')\n",
    "        plt.plot(loss_tests, label='test loss before')\n",
    "        plt.plot(loss_trains2, label='train loss after')\n",
    "        plt.plot(loss_tests2, label='test loss after')\n",
    "        plt.title('loss curve')\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.savefig(os.path.join('checkpoints_bandits', experiment_name, 'loss_curve.png'))\n",
    "        plt.clf()\n",
    "        plt.plot(accuracies, label='test')\n",
    "        plt.plot(accuracies_train, label='train')\n",
    "        plt.title('accuracy progression')\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join('checkpoints_bandits', experiment_name, 'accuracy_progression.png'))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "14ef3032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST_test_IFCL\n",
      "0\n",
      "[3 4 2 4 4 1 2 2 2 4 3 2 5 4 1 3 5 5 1 3 4 0 3 1 5 4 3 0 0 2 2 1 3 3 5 5 5\n",
      " 2 3 3 0 2 4 2 4 0 1 3 0 3 5 1 1 0 1 4 1 3 3 3 3 4 2 5 0 3 1 3 1 5 5 5 1 3\n",
      " 5 4 1 1 3 1 1 5 3 5 5 3 0 5 4 4 1 4 1 0 3 3 3 4 0 4]\n",
      "full train loss:  tensor(2.9837, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(3.7401, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  86.37833468067906\n",
      "test accuracy:  87.42982670246522\n",
      "0\n",
      "accuracy is best accuracy\n",
      "[95.1219512195122, 100.0, 100.0, 80.48780487804879, 51.21951219512195, 100.0, 100.0, 88.09523809523809, 87.8048780487805, 92.6829268292683, 95.1219512195122, 100.0, 70.73170731707317, 97.5609756097561, 100.0, 87.8048780487805, 90.2439024390244, 92.6829268292683, 83.33333333333334, 82.92682926829268, 100.0, 75.60975609756098, 90.2439024390244, 50.0, 97.5609756097561, 100.0, 92.85714285714286, 50.0, 70.73170731707317, 73.80952380952381, 61.904761904761905, 50.0, 100.0, 87.8048780487805, 87.8048780487805, 95.1219512195122, 60.97560975609756, 50.0, 92.6829268292683, 85.36585365853658, 85.71428571428571, 100.0, 97.5, 73.80952380952381, 100.0, 95.0, 97.5609756097561, 78.04878048780488, 64.28571428571429, 100.0, 83.33333333333334, 100.0, 97.5609756097561, 100.0, 100.0, 88.09523809523809, 100.0, 95.1219512195122, 75.60975609756098, 60.97560975609756, 100.0, 65.85365853658537, 100.0, 100.0, 100.0, 90.2439024390244, 100.0, 100.0, 100.0, 80.48780487804879, 100.0, 90.2439024390244, 100.0, 76.19047619047619, 85.36585365853658, 80.95238095238095, 51.21951219512195, 100.0, 78.04878048780488, 97.5, 92.6829268292683, 97.5609756097561, 100.0, 90.47619047619048, 100.0, 48.78048780487805, 100.0, 100.0, 92.6829268292683, 95.1219512195122, 78.57142857142857, 56.09756097560976, 100.0, 100.0, 80.95238095238095, 80.48780487804879, 95.1219512195122, 100.0, 90.2439024390244, 100.0]\n",
      "1\n",
      "[15.00249956 11.03357025 11.06704892 12.76752555 12.05398739 11.22618882\n",
      " 11.15607755 15.32792188 15.59658603 12.82705757 14.9791188  11.38182457\n",
      " 12.20923777 15.67685291 11.18369412 12.76957301 16.0210843  13.01441679\n",
      " 15.32581267 12.92525777 14.59928175 13.11761585 14.99989007 15.82270264\n",
      " 12.11148953 15.43540262 15.36451369 15.2349484  12.78793557 15.33333646\n",
      " 15.4306408  15.26086887 11.24613599 15.41218304 13.12390532 12.82867982\n",
      " 15.60336971 15.63531411 12.082417   12.83213922 15.55154121 11.15242196\n",
      " 11.09651916 15.73440322 11.12161178 11.14939858 14.88707917 12.81827747\n",
      " 15.70706557 15.17690239 15.1725856  11.17913701 15.1825931  12.97804459\n",
      " 11.19419417 15.18527506 14.06120559 12.80887069 15.71530587 15.84783034\n",
      " 15.80019184 12.30107412 14.63992794 11.01072973 15.51703144 12.29775633\n",
      " 10.85285045 11.2653386  12.74445753 15.21159248 12.83552757 15.46971446\n",
      " 10.95200809 15.00248361 15.39011017 15.35959825 14.93495674 13.83783906\n",
      " 12.15925239 11.35491171 13.0524553  12.08906196 14.89348811 15.12235069\n",
      " 14.73190529 12.38164877 11.07980271 12.36747881 15.34515444 12.51005929\n",
      " 15.34445153 15.14270611 12.14271798 11.04939839 15.55140217 12.21751132\n",
      " 15.31854874 10.84533743 15.56063143 10.71970528]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(1.2098, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(1.5162, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  93.43168957154407\n",
      "test accuracy:  93.67830119599707\n",
      "86.37833468067906\n",
      "accuracy is best accuracy\n",
      "[97.5609756097561, 100.0, 100.0, 95.1219512195122, 95.1219512195122, 100.0, 97.5, 66.66666666666666, 97.5609756097561, 97.5609756097561, 97.5609756097561, 100.0, 100.0, 100.0, 100.0, 92.6829268292683, 97.5609756097561, 100.0, 88.09523809523809, 90.2439024390244, 78.04878048780488, 100.0, 95.1219512195122, 66.66666666666666, 95.1219512195122, 95.1219512195122, 69.04761904761905, 92.85714285714286, 95.1219512195122, 100.0, 95.23809523809523, 73.80952380952381, 100.0, 97.5609756097561, 100.0, 100.0, 97.5609756097561, 61.904761904761905, 97.5609756097561, 97.5609756097561, 85.71428571428571, 100.0, 97.5, 50.0, 100.0, 97.5, 92.6829268292683, 100.0, 64.28571428571429, 100.0, 73.80952380952381, 100.0, 97.5609756097561, 100.0, 100.0, 88.09523809523809, 100.0, 100.0, 78.04878048780488, 97.5609756097561, 100.0, 100.0, 100.0, 100.0, 100.0, 97.5609756097561, 100.0, 100.0, 100.0, 87.8048780487805, 100.0, 95.1219512195122, 100.0, 92.85714285714286, 100.0, 85.71428571428571, 90.2439024390244, 97.5609756097561, 97.5609756097561, 97.5, 97.5609756097561, 100.0, 97.5609756097561, 90.47619047619048, 97.5609756097561, 87.8048780487805, 100.0, 95.1219512195122, 100.0, 100.0, 80.95238095238095, 85.36585365853658, 100.0, 100.0, 97.61904761904762, 100.0, 87.8048780487805, 100.0, 73.17073170731707, 100.0]\n",
      "2\n",
      "[ 2.40778443  0.14927796  0.37676738  1.33514323  2.08464241  0.33679762\n",
      "  0.3008185  16.13733355 19.14506671  1.49024575  2.61739765  0.34843549\n",
      "  2.42934097 19.2729627   0.35019753  1.44962119 19.49530227  1.43557505\n",
      " 16.10403189  1.40055791  2.03310488  2.71269379 19.0413562  16.37531888\n",
      "  2.23148942 19.18682081 16.04769765 15.96245515  1.24310126 16.19698828\n",
      " 16.18870051 16.06141844  0.16734868  2.04777626  1.91205445  1.6876085\n",
      " 19.43133018 16.04946413  2.25945091  1.82039954 16.2084727   0.14559422\n",
      "  0.14542391 16.29580569  0.17028272  0.5875329   1.8671711   1.40940953\n",
      " 16.18076747  2.3742598  16.14650089  0.35761035  2.21721453  1.65807999\n",
      "  0.54511494 16.24396155  2.18140648  1.22235078  2.32696103 19.29088902\n",
      "  1.91736734  2.47083146  2.07624908  0.21223242 19.1585661   2.56511732\n",
      "  0.13246311  0.10981718  1.27413307 19.23154355  1.98757825 19.36450065\n",
      "  0.31020213 16.1360076   1.78121722 15.95431665  2.03585295  1.70471176\n",
      "  2.66848958  0.2614913   1.95165953  2.39292225 19.06547058 15.96728709\n",
      "  2.02660323  2.28271393  0.13616673  2.60155014 19.11308602  1.10617682\n",
      " 16.29452747  1.96640836  2.31062005  0.15301432 16.16063461  2.3012395\n",
      " 18.99365409  0.14993035 19.10554003  0.37670236]\n",
      "[1 4 4 0 5 4 4 3 3 0 1 4 5 3 4 0 3 0 3 0 1 0 3 3 5 3 3 3 0 3 3 3 4 1 0 0 3\n",
      " 3 5 0 3 4 4 3 4 4 1 0 3 1 3 4 1 0 4 3 1 0 1 3 1 5 1 4 3 5 4 4 0 3 0 3 4 3\n",
      " 1 3 1 1 5 4 0 5 3 3 1 5 4 5 3 0 3 1 5 4 3 5 3 4 3 4]\n",
      "full train loss:  tensor(1.0097, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(1.2547, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  94.52303961196444\n",
      "test accuracy:  94.84989016353428\n",
      "93.43168957154407\n",
      "accuracy is best accuracy\n",
      "[100.0, 100.0, 100.0, 92.6829268292683, 90.2439024390244, 100.0, 100.0, 88.09523809523809, 92.6829268292683, 100.0, 100.0, 100.0, 100.0, 95.1219512195122, 95.0, 92.6829268292683, 82.92682926829268, 100.0, 90.47619047619048, 90.2439024390244, 97.5609756097561, 100.0, 95.1219512195122, 95.23809523809523, 95.1219512195122, 82.92682926829268, 90.47619047619048, 97.61904761904762, 95.1219512195122, 100.0, 92.85714285714286, 71.42857142857143, 100.0, 95.1219512195122, 100.0, 97.5609756097561, 95.1219512195122, 92.85714285714286, 100.0, 92.6829268292683, 59.523809523809526, 100.0, 97.5, 80.95238095238095, 100.0, 97.5, 97.5609756097561, 100.0, 100.0, 95.1219512195122, 88.09523809523809, 100.0, 97.5609756097561, 100.0, 100.0, 83.33333333333334, 100.0, 100.0, 100.0, 97.5609756097561, 100.0, 100.0, 100.0, 100.0, 100.0, 97.5609756097561, 100.0, 100.0, 100.0, 90.2439024390244, 97.5609756097561, 95.1219512195122, 95.0, 64.28571428571429, 97.5609756097561, 71.42857142857143, 97.5609756097561, 97.5609756097561, 78.04878048780488, 100.0, 97.5609756097561, 100.0, 95.1219512195122, 90.47619047619048, 100.0, 100.0, 100.0, 100.0, 97.5609756097561, 100.0, 61.904761904761905, 100.0, 100.0, 100.0, 97.61904761904762, 97.5609756097561, 92.6829268292683, 100.0, 92.6829268292683, 95.0]\n",
      "3\n",
      "[1.16079975e+00 2.56873238e-02 3.47973564e-01 7.24966733e-01\n",
      " 6.97462751e-01 1.59821959e-01 1.84823661e-01 1.88935279e+01\n",
      " 2.27343184e+01 1.21959811e+00 1.00196493e+00 1.54329568e-01\n",
      " 1.04439100e+00 2.26650769e+01 1.23972943e-01 9.83784481e-01\n",
      " 2.53680855e+01 7.30832726e-01 1.85484557e+01 6.79761572e-01\n",
      " 7.68726815e-01 2.04696069e+00 2.19851506e+01 2.04905952e+01\n",
      " 9.14851565e-01 2.13205686e+01 1.90359079e+01 1.80579418e+01\n",
      " 4.86557135e-01 1.92687333e+01 1.99621583e+01 1.83045864e+01\n",
      " 2.99669446e-02 8.09945281e-01 9.77547431e-01 1.20446149e+00\n",
      " 2.24331228e+01 1.99647752e+01 8.16375736e-01 1.01077195e+00\n",
      " 1.97228992e+01 2.78986217e-02 2.06809032e-02 2.03032274e+01\n",
      " 5.18301454e-02 4.88389680e-01 5.41625602e-01 7.80758464e-01\n",
      " 1.96569968e+01 1.16641543e+00 1.84455941e+01 2.29356957e-01\n",
      " 1.41090762e+00 9.88982784e-01 4.94780705e-01 1.80075562e+01\n",
      " 7.87606588e-01 4.41289633e-01 1.15967933e+00 2.47441656e+01\n",
      " 6.28903709e-01 8.75919915e-01 7.72689677e-01 5.01720847e-02\n",
      " 2.28313880e+01 9.95769438e-01 2.53189155e-02 1.28534307e-02\n",
      " 4.89190438e-01 2.29272236e+01 1.24897178e+00 2.25876354e+01\n",
      " 2.53077779e-01 1.81665537e+01 9.18373778e-01 1.85931234e+01\n",
      " 6.62933933e-01 4.79233314e-01 1.02961421e+00 5.41626883e-02\n",
      " 1.32595145e+00 1.11536907e+00 2.06285812e+01 1.86408804e+01\n",
      " 8.49387829e-01 7.67401876e-01 1.69081060e-02 1.21890511e+00\n",
      " 2.21825258e+01 5.24522430e-01 1.93181986e+01 7.39951304e-01\n",
      " 8.04855749e-01 3.96664805e-02 1.89753293e+01 7.30424228e-01\n",
      " 2.09891003e+01 3.10352314e-02 2.39775100e+01 3.48266608e-01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.9197, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(1.1264, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  95.27081649151172\n",
      "test accuracy:  95.58213326824506\n",
      "94.52303961196444\n",
      "accuracy is best accuracy\n",
      "[97.5609756097561, 100.0, 95.0, 95.1219512195122, 97.5609756097561, 100.0, 100.0, 90.47619047619048, 92.6829268292683, 95.1219512195122, 100.0, 100.0, 100.0, 95.1219512195122, 100.0, 95.1219512195122, 100.0, 100.0, 90.47619047619048, 90.2439024390244, 100.0, 100.0, 97.5609756097561, 95.23809523809523, 97.5609756097561, 92.6829268292683, 90.47619047619048, 95.23809523809523, 97.5609756097561, 83.33333333333334, 88.09523809523809, 85.71428571428571, 100.0, 95.1219512195122, 95.1219512195122, 100.0, 90.2439024390244, 92.85714285714286, 100.0, 97.5609756097561, 83.33333333333334, 100.0, 97.5, 78.57142857142857, 100.0, 97.5, 97.5609756097561, 100.0, 95.23809523809523, 100.0, 85.71428571428571, 100.0, 100.0, 100.0, 100.0, 90.47619047619048, 100.0, 100.0, 92.6829268292683, 100.0, 100.0, 97.5609756097561, 97.5609756097561, 97.5, 95.1219512195122, 97.5609756097561, 100.0, 100.0, 100.0, 87.8048780487805, 100.0, 90.2439024390244, 100.0, 83.33333333333334, 100.0, 85.71428571428571, 87.8048780487805, 97.5609756097561, 97.5609756097561, 100.0, 97.5609756097561, 100.0, 82.92682926829268, 92.85714285714286, 100.0, 100.0, 100.0, 100.0, 92.6829268292683, 97.5609756097561, 83.33333333333334, 100.0, 100.0, 100.0, 64.28571428571429, 97.5609756097561, 97.5609756097561, 100.0, 95.1219512195122, 100.0]\n",
      "4\n",
      "[1.15162894e+00 8.08068402e-03 4.21546717e-01 6.20770623e-01\n",
      " 5.08184324e-01 9.26268495e-02 1.47814995e-01 1.03593566e+01\n",
      " 1.35124776e+01 8.39386040e-01 1.05907030e+00 9.75484212e-02\n",
      " 3.76647040e-01 1.22163615e+01 8.40706168e-02 7.65567350e-01\n",
      " 1.48993719e+01 7.42186073e-01 1.02240335e+01 6.38973853e-01\n",
      " 4.76616142e-01 1.61272070e+00 1.13597972e+01 1.11902251e+01\n",
      " 5.41672422e-01 1.35047430e+01 1.01333497e+01 9.73747607e+00\n",
      " 3.63163939e-01 1.01275825e+01 1.12999547e+01 1.00276027e+01\n",
      " 6.19267368e-03 7.02938532e-01 7.62698742e-01 1.13039699e+00\n",
      " 1.22401310e+01 1.08151961e+01 4.70018249e-01 1.01183459e+00\n",
      " 1.10454958e+01 5.54397040e-03 3.88571896e-03 1.20817211e+01\n",
      " 2.97137802e-02 1.27850907e+00 2.53885344e-01 4.79168038e-01\n",
      " 1.08998592e+01 1.37612604e+00 1.05453259e+01 1.86631117e-01\n",
      " 1.43017010e+00 9.63673221e-01 6.46670054e-01 1.09479937e+01\n",
      " 7.53764030e-01 2.30000303e-01 1.73792331e+00 1.36602287e+01\n",
      " 4.86374219e-01 4.38233450e-01 6.21605306e-01 1.65922665e-02\n",
      " 1.26655863e+01 4.78645242e-01 4.20173613e-03 2.64020057e-03\n",
      " 2.50168007e-01 1.25172820e+01 1.09558055e+00 1.31031586e+01\n",
      " 3.14517327e-01 1.06323611e+01 5.52435594e-01 9.98031290e+00\n",
      " 3.88297484e-01 3.00738795e-01 5.46124773e-01 1.62802119e-02\n",
      " 1.70952306e+00 5.27644121e-01 1.11955385e+01 1.00952427e+01\n",
      " 9.98052032e-01 4.00440275e-01 3.38599104e-03 5.67358583e-01\n",
      " 1.14221346e+01 3.77684400e-01 1.10265255e+01 7.48463473e-01\n",
      " 3.09666893e-01 1.89152887e-02 9.84529895e+00 3.65733418e-01\n",
      " 1.25417174e+01 6.08384654e-03 1.23297072e+01 4.09749961e-01]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.5476, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.7527, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  96.66531932093775\n",
      "test accuracy:  96.99780327068586\n",
      "95.27081649151172\n",
      "accuracy is best accuracy\n",
      "[100.0, 100.0, 100.0, 87.8048780487805, 97.5609756097561, 100.0, 100.0, 85.71428571428571, 95.1219512195122, 100.0, 82.92682926829268, 100.0, 100.0, 97.5609756097561, 100.0, 97.5609756097561, 100.0, 97.5609756097561, 88.09523809523809, 95.1219512195122, 97.5609756097561, 100.0, 97.5609756097561, 95.23809523809523, 97.5609756097561, 95.1219512195122, 97.61904761904762, 97.61904761904762, 97.5609756097561, 95.23809523809523, 90.47619047619048, 92.85714285714286, 100.0, 95.1219512195122, 90.2439024390244, 100.0, 90.2439024390244, 92.85714285714286, 100.0, 97.5609756097561, 88.09523809523809, 100.0, 97.5, 80.95238095238095, 100.0, 97.5, 100.0, 100.0, 100.0, 97.5609756097561, 85.71428571428571, 100.0, 100.0, 100.0, 100.0, 95.23809523809523, 100.0, 100.0, 100.0, 97.5609756097561, 100.0, 97.5609756097561, 100.0, 100.0, 97.5609756097561, 100.0, 100.0, 100.0, 100.0, 100.0, 97.5609756097561, 95.1219512195122, 95.0, 90.47619047619048, 100.0, 95.23809523809523, 100.0, 97.5609756097561, 100.0, 97.5, 95.1219512195122, 100.0, 97.5609756097561, 100.0, 100.0, 100.0, 100.0, 100.0, 97.5609756097561, 97.5609756097561, 95.23809523809523, 97.5609756097561, 100.0, 100.0, 85.71428571428571, 100.0, 92.6829268292683, 100.0, 95.1219512195122, 100.0]\n",
      "5\n",
      "[8.93623842e-01 1.29724162e-03 4.19287085e-01 5.49296247e-01\n",
      " 2.25645783e-01 4.92312778e-02 1.61774379e-01 1.01664287e+01\n",
      " 1.06486041e+01 7.12469525e-01 6.45569254e-01 3.40382601e-02\n",
      " 3.05384916e-01 1.04553563e+01 2.48147057e-02 7.92463414e-01\n",
      " 1.24652389e+01 5.53720306e-01 9.95044510e+00 3.18246713e-01\n",
      " 3.06496833e-01 1.68998377e+00 1.00546727e+01 1.13952282e+01\n",
      " 5.09652319e-01 1.20975420e+01 9.37468352e+00 9.07305177e+00\n",
      " 1.91663328e-01 9.59300992e+00 1.06932944e+01 8.87137536e+00\n",
      " 1.25800651e-03 4.33940197e-01 6.15887142e-01 8.75337780e-01\n",
      " 1.06700448e+01 1.08546330e+01 3.05687809e-01 6.55072973e-01\n",
      " 1.12122687e+01 3.74289185e-03 1.92426046e-03 1.21675229e+01\n",
      " 3.57454716e-02 5.65981695e-01 1.46822707e-01 5.08012587e-01\n",
      " 1.06403979e+01 9.90936112e-01 1.02894689e+01 1.45116329e-01\n",
      " 1.22084900e+00 6.76664373e-01 5.87122053e-01 1.04618577e+01\n",
      " 5.24102804e-01 1.69593395e-01 1.08503520e+00 1.07470994e+01\n",
      " 2.08181376e-01 3.22235940e-01 3.50998890e-01 4.78660245e-03\n",
      " 9.38235051e+00 2.99923337e-01 2.81767780e-03 5.47346801e-04\n",
      " 1.96334974e-01 1.01679956e+01 8.09354042e-01 1.10519436e+01\n",
      " 2.88226267e-01 1.03821378e+01 2.83610463e-01 8.83385105e+00\n",
      " 2.33961414e-01 1.16001753e-01 2.62718273e-01 1.22379055e-02\n",
      " 1.23478748e+00 4.30717023e-01 9.47283363e+00 9.77384001e+00\n",
      " 5.30137138e-01 3.36731562e-01 7.95073208e-04 5.39374186e-01\n",
      " 9.34588264e+00 2.66011771e-01 1.18839982e+01 5.06478363e-01\n",
      " 1.57994876e-01 3.75706290e-03 9.18117782e+00 1.52194904e-01\n",
      " 1.09517853e+01 1.22624207e-03 1.03696704e+01 3.01289625e-01]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.4835, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.6490, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  96.86742118027486\n",
      "test accuracy:  97.5103734439834\n",
      "96.66531932093775\n",
      "accuracy is best accuracy\n",
      "[100.0, 100.0, 100.0, 97.5609756097561, 97.5609756097561, 100.0, 100.0, 90.47619047619048, 95.1219512195122, 97.5609756097561, 100.0, 100.0, 100.0, 97.5609756097561, 100.0, 97.5609756097561, 100.0, 100.0, 90.47619047619048, 92.6829268292683, 97.5609756097561, 100.0, 95.1219512195122, 95.23809523809523, 97.5609756097561, 95.1219512195122, 97.61904761904762, 97.61904761904762, 92.6829268292683, 100.0, 90.47619047619048, 90.47619047619048, 100.0, 92.6829268292683, 100.0, 100.0, 92.6829268292683, 92.85714285714286, 100.0, 97.5609756097561, 92.85714285714286, 100.0, 100.0, 83.33333333333334, 100.0, 97.5, 100.0, 100.0, 100.0, 92.6829268292683, 90.47619047619048, 100.0, 100.0, 100.0, 97.5, 100.0, 100.0, 100.0, 100.0, 97.5609756097561, 100.0, 100.0, 100.0, 100.0, 100.0, 95.1219512195122, 100.0, 100.0, 100.0, 100.0, 95.1219512195122, 95.1219512195122, 100.0, 92.85714285714286, 100.0, 100.0, 90.2439024390244, 97.5609756097561, 97.5609756097561, 97.5, 100.0, 100.0, 95.1219512195122, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 90.47619047619048, 87.8048780487805, 90.2439024390244, 100.0, 95.1219512195122, 100.0]\n",
      "6\n",
      "[8.47844889e-01 3.04778791e-04 4.61803325e-01 5.39966279e-01\n",
      " 2.23200973e-01 2.68424026e-02 1.72518985e-01 6.51069304e+00\n",
      " 7.02537431e+00 7.21767823e-01 6.22907799e-01 1.42580343e-02\n",
      " 1.78765485e-01 7.51879403e+00 8.84791661e-03 6.94632743e-01\n",
      " 9.73685640e+00 3.69876317e-01 6.07256504e+00 3.76791510e-01\n",
      " 2.20949839e-01 1.28105653e+00 6.93205361e+00 7.65302050e+00\n",
      " 4.50987168e-01 8.08490941e+00 5.99739212e+00 5.56032096e+00\n",
      " 1.96977294e-01 6.33669301e+00 6.97417149e+00 5.39471784e+00\n",
      " 3.65666952e-04 3.87636796e-01 6.77867257e-01 8.06193637e-01\n",
      " 7.14459359e+00 6.28194979e+00 2.32365901e-01 8.14030223e-01\n",
      " 7.52754828e+00 1.80798897e-03 5.94189570e-04 8.17838667e+00\n",
      " 2.92253062e-02 5.80053061e-01 6.94130564e-02 3.61742345e-01\n",
      " 6.54078173e+00 1.09683070e+00 6.84922619e+00 1.20943704e-01\n",
      " 1.30392340e+00 6.88069547e-01 6.03479745e-01 7.47626623e+00\n",
      " 5.75238294e-01 9.40436919e-02 1.27060040e+00 8.66320098e+00\n",
      " 1.69031680e-01 2.72278600e-01 3.10503665e-01 1.64927452e-03\n",
      " 6.68091506e+00 2.45768833e-01 1.26744437e-03 1.06406326e-04\n",
      " 1.12154531e-01 6.93946255e+00 7.99417340e-01 7.85718142e+00\n",
      " 2.98016704e-01 6.77426099e+00 2.39547949e-01 6.23030998e+00\n",
      " 1.55616260e-01 8.55958230e-02 2.36494930e-01 6.15782226e-03\n",
      " 1.32107067e+00 2.92580832e-01 6.86357900e+00 7.22611407e+00\n",
      " 5.92567388e-01 3.41996698e-01 1.60049607e-04 3.50045999e-01\n",
      " 6.95278271e+00 2.57562806e-01 7.84926661e+00 5.47529431e-01\n",
      " 9.52338995e-02 1.07896894e-03 5.49561947e+00 1.05414618e-01\n",
      " 7.31900231e+00 7.97375601e-04 7.10776443e+00 2.77570904e-01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.3182, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.5159, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  97.67582861762328\n",
      "test accuracy:  97.99853551379057\n",
      "96.86742118027486\n",
      "accuracy is best accuracy\n",
      "[97.5609756097561, 100.0, 100.0, 97.5609756097561, 97.5609756097561, 100.0, 100.0, 92.85714285714286, 100.0, 97.5609756097561, 100.0, 100.0, 100.0, 97.5609756097561, 100.0, 100.0, 95.1219512195122, 100.0, 97.61904761904762, 95.1219512195122, 97.5609756097561, 100.0, 95.1219512195122, 92.85714285714286, 97.5609756097561, 95.1219512195122, 97.61904761904762, 97.61904761904762, 95.1219512195122, 97.61904761904762, 90.47619047619048, 92.85714285714286, 100.0, 95.1219512195122, 100.0, 100.0, 92.6829268292683, 95.23809523809523, 100.0, 95.1219512195122, 92.85714285714286, 100.0, 100.0, 85.71428571428571, 100.0, 97.5, 97.5609756097561, 100.0, 100.0, 100.0, 90.47619047619048, 100.0, 100.0, 100.0, 97.5, 100.0, 100.0, 100.0, 100.0, 97.5609756097561, 100.0, 97.5609756097561, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 97.5609756097561, 100.0, 92.85714285714286, 100.0, 100.0, 100.0, 100.0, 97.5609756097561, 100.0, 100.0, 100.0, 97.5609756097561, 97.61904761904762, 100.0, 100.0, 100.0, 100.0, 97.5609756097561, 92.6829268292683, 100.0, 97.5609756097561, 100.0, 100.0, 92.85714285714286, 97.5609756097561, 92.6829268292683, 100.0, 95.1219512195122, 100.0]\n",
      "7\n",
      "[8.45263137e-01 9.32478627e-05 5.37774255e-01 4.82293908e-01\n",
      " 1.76355763e-01 1.13581284e-02 1.69921091e-01 5.78673466e+00\n",
      " 5.74313416e+00 6.37429531e-01 5.80349162e-01 8.97597444e-03\n",
      " 1.67192893e-01 6.09973316e+00 3.60731359e-03 6.51250641e-01\n",
      " 8.08380601e+00 3.65410335e-01 5.54574289e+00 2.11229991e-01\n",
      " 1.50222163e-01 1.05537250e+00 6.08425903e+00 7.44868480e+00\n",
      " 4.45187257e-01 6.63118803e+00 4.98462176e+00 5.10371991e+00\n",
      " 1.22823520e-01 4.93933818e+00 6.54184074e+00 4.64787278e+00\n",
      " 1.08288837e-04 3.21992641e-01 5.94885799e-01 7.41343921e-01\n",
      " 5.90763658e+00 5.37827396e+00 1.80750444e-01 6.19011986e-01\n",
      " 7.73935841e+00 8.55571237e-04 1.19590084e-04 7.52939540e+00\n",
      " 6.53634894e-02 5.85721449e-01 4.80345668e-02 3.42581051e-01\n",
      " 6.00146524e+00 1.03799679e+00 6.65902739e+00 9.01971883e-02\n",
      " 1.27044174e+00 4.81796869e-01 6.69712620e-01 6.31351278e+00\n",
      " 5.20809669e-01 5.60552750e-02 1.17432439e+00 6.93069617e+00\n",
      " 9.85097382e-02 3.06157213e-01 2.19949016e-01 3.72502535e-04\n",
      " 5.27509857e+00 1.69618233e-01 3.97388392e-04 2.30994738e-05\n",
      " 8.12943031e-02 5.66910826e+00 6.29937241e-01 6.90334854e+00\n",
      " 3.38736826e-01 7.67124288e+00 1.75768630e-01 5.76311643e+00\n",
      " 1.19423946e-01 5.38077058e-02 1.47271108e-01 1.96656902e-03\n",
      " 1.26098725e+00 2.50336037e-01 6.14846634e+00 6.75988377e+00\n",
      " 4.79736011e-01 3.13782919e-01 2.73093843e-05 3.09928505e-01\n",
      " 5.34657020e+00 1.79051825e-01 7.65730274e+00 4.87983812e-01\n",
      " 5.83884576e-02 1.24344833e-03 4.72374549e+00 5.71893382e-02\n",
      " 6.52651971e+00 6.50901859e-05 5.63003075e+00 3.01751082e-01]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.2793, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.4526, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.32255456750202\n",
      "test accuracy:  98.19380034171344\n",
      "97.67582861762328\n",
      "accuracy is best accuracy\n",
      "[100.0, 100.0, 100.0, 95.1219512195122, 97.5609756097561, 100.0, 100.0, 90.47619047619048, 100.0, 100.0, 100.0, 100.0, 97.5609756097561, 97.5609756097561, 100.0, 95.1219512195122, 95.1219512195122, 100.0, 90.47619047619048, 92.6829268292683, 97.5609756097561, 100.0, 95.1219512195122, 97.61904761904762, 100.0, 95.1219512195122, 97.61904761904762, 97.61904761904762, 97.5609756097561, 100.0, 90.47619047619048, 95.23809523809523, 100.0, 97.5609756097561, 100.0, 100.0, 92.6829268292683, 92.85714285714286, 100.0, 97.5609756097561, 90.47619047619048, 100.0, 100.0, 85.71428571428571, 100.0, 100.0, 100.0, 100.0, 100.0, 97.5609756097561, 95.23809523809523, 100.0, 100.0, 100.0, 100.0, 97.61904761904762, 100.0, 100.0, 100.0, 97.5609756097561, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 97.5609756097561, 100.0, 97.61904761904762, 100.0, 100.0, 100.0, 97.5609756097561, 100.0, 100.0, 100.0, 100.0, 97.5609756097561, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 92.85714285714286, 100.0, 90.2439024390244, 100.0, 87.8048780487805, 100.0]\n",
      "8\n",
      "[7.26259480e-01 4.83006257e-05 6.26953859e-01 4.48219079e-01\n",
      " 2.47243020e-01 7.16593792e-03 1.42650266e-01 4.05016241e+00\n",
      " 4.31012427e+00 5.86496096e-01 2.87057152e+00 2.92912259e-03\n",
      " 1.01211713e-01 4.31700470e+00 1.97660772e-03 6.34696978e-01\n",
      " 5.66069537e+00 3.82963088e-01 4.11721053e+00 1.55231687e-01\n",
      " 1.50556487e-01 1.16967198e+00 3.70432494e+00 5.53595989e+00\n",
      " 4.21587268e-01 5.38175930e+00 3.41949531e+00 3.36096042e+00\n",
      " 8.31203582e-02 3.17870018e+00 4.03154149e+00 3.01384025e+00\n",
      " 1.73311273e-05 2.80450626e-01 5.47886764e-01 7.12380879e-01\n",
      " 4.18482107e+00 3.52916216e+00 2.01218288e-01 5.33879140e-01\n",
      " 5.61700216e+00 1.20461607e-04 2.34693762e-05 5.55318512e+00\n",
      " 1.03162528e-02 6.76000323e-01 4.59950389e-02 3.20795300e-01\n",
      " 4.03130181e+00 9.96183856e-01 4.85229005e+00 9.62271567e-02\n",
      " 1.22578883e+00 6.02875501e-01 8.30242021e-01 4.65894384e+00\n",
      " 4.81588989e-01 4.02525361e-02 1.06036779e+00 4.46284719e+00\n",
      " 6.15562671e-02 2.68331388e-01 1.67309188e-01 9.47450064e-05\n",
      " 3.48949386e+00 2.02834507e-01 6.99724957e-05 5.60863063e-06\n",
      " 7.06052435e-02 4.23479465e+00 5.72123342e-01 4.86754170e+00\n",
      " 4.30104743e-01 5.02442711e+00 1.67928506e-01 4.19335547e+00\n",
      " 1.02690185e-01 4.00220797e-02 2.33622670e-01 7.56470725e-04\n",
      " 1.23567477e+00 1.65619409e-01 4.06170352e+00 4.75951427e+00\n",
      " 3.74907741e-01 3.98706131e-01 6.39496903e-06 2.97198403e-01\n",
      " 3.75421789e+00 1.43507055e-01 5.21040531e+00 4.45847947e-01\n",
      " 5.63493293e-02 1.05330759e-04 3.15159254e+00 7.81103229e-02\n",
      " 4.21706885e+00 1.38728819e-05 4.06172718e+00 3.57569665e-01]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.2321, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.4357, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.05982215036379\n",
      "test accuracy:  98.2182084452038\n",
      "9\n",
      "[6.22071830e-01 2.28575309e-05 6.61575294e-01 4.39145406e-01\n",
      " 1.17440720e-01 2.86618944e-03 1.37299823e-01 3.81212867e+00\n",
      " 3.13422356e+00 5.52172836e-01 5.63731909e-01 1.44966286e-03\n",
      " 2.19903966e-01 3.66195480e+00 8.95193293e-04 6.28445452e-01\n",
      " 6.06838513e+00 3.66791504e-01 4.08048779e+00 1.08410672e-01\n",
      " 1.07411611e-01 1.00784926e+00 3.28573592e+00 5.15283889e+00\n",
      " 9.36003280e-01 4.37001258e+00 3.03881434e+00 3.07383525e+00\n",
      " 6.10751424e-02 2.97738591e+00 3.61692756e+00 2.85292137e+00\n",
      " 1.30326411e-05 2.42150998e-01 5.63807388e-01 6.95927453e-01\n",
      " 2.99599856e+00 3.14129763e+00 1.31385711e-01 4.89313034e-01\n",
      " 5.65476560e+00 5.41790469e-05 7.72883968e-06 5.72145841e+00\n",
      " 5.99172932e-03 6.67218573e-01 2.46065670e-02 3.19081986e-01\n",
      " 3.86324062e+00 1.01292548e+00 5.08499910e+00 7.71861887e-02\n",
      " 1.23459830e+00 3.36106474e-01 8.66447234e-01 4.35845456e+00\n",
      " 4.72709889e-01 2.89324744e-02 1.05563399e+00 3.69831428e+00\n",
      " 4.89243680e-02 2.05076568e-01 1.40713517e-01 3.79539845e-05\n",
      " 2.86088249e+00 1.03509479e-01 3.26836878e-05 2.02205879e-06\n",
      " 5.87231685e-02 3.35239442e+00 4.95594532e-01 4.21903222e+00\n",
      " 4.50124432e-01 4.99696212e+00 1.01989377e-01 4.13392656e+00\n",
      " 7.25343729e-02 3.73570079e-02 6.06631587e-02 3.28618197e-04\n",
      " 1.23610819e+00 2.66806073e-01 3.45351943e+00 4.83210658e+00\n",
      " 3.62007188e-01 3.29435791e-01 1.94610203e-06 2.15029562e-01\n",
      " 3.03783456e+00 1.29438328e-01 5.16907578e+00 4.27431903e-01\n",
      " 2.05114360e-02 4.05704760e-05 2.57112911e+00 1.73345487e-02\n",
      " 3.36081442e+00 4.75970554e-06 3.11374451e+00 3.51214922e-01]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full train loss:  tensor(0.2261, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.4664, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  97.97898140662895\n",
      "test accuracy:  98.2182084452038\n",
      "10\n",
      "[6.91699806e-01 1.92603363e-05 7.12483900e-01 3.89410166e-01\n",
      " 1.88940683e-01 1.30634034e-03 9.90496253e-02 2.95125952e+00\n",
      " 2.50485551e+00 5.10782916e-01 5.98498101e-01 1.16843701e-03\n",
      " 1.25128760e-01 2.98633553e+00 6.99820353e-04 6.52577978e-01\n",
      " 4.16061557e+00 4.66813297e-01 3.40895931e+00 7.50939627e-02\n",
      " 7.46070589e-02 1.24822480e+00 2.86731475e+00 4.13317728e+00\n",
      " 4.23158650e-01 3.81363118e+00 2.21930356e+00 2.27419834e+00\n",
      " 6.70047082e-02 2.15701324e+00 3.22155736e+00 2.11220705e+00\n",
      " 4.68838636e-06 2.62404385e-01 4.80136608e-01 7.20223025e-01\n",
      " 2.66392782e+00 2.12955635e+00 1.30302671e-01 3.95846971e-01\n",
      " 4.52610886e+00 2.03293819e-05 1.93305321e-06 4.33933442e+00\n",
      " 2.86039943e-03 6.47579522e-01 1.72864459e-02 3.73598782e-01\n",
      " 3.14552936e+00 4.07846984e+00 3.76629181e+00 5.32065733e-02\n",
      " 1.51114415e+00 2.78121166e-01 1.94394728e+00 3.25477074e+00\n",
      " 5.38203752e-01 4.01655642e-02 1.24535957e+00 3.03097462e+00\n",
      " 3.36243394e-02 2.20262634e-01 1.43549125e-01 1.90714133e-05\n",
      " 2.18767739e+00 1.63556865e-01 1.05988083e-05 1.18963840e-06\n",
      " 7.97789651e-02 2.98587563e+00 4.87335732e-01 4.37610497e+00\n",
      " 4.85240067e-01 4.16546634e+00 9.67524115e-02 3.30467777e+00\n",
      " 5.85980153e-02 2.24070520e-02 1.60457150e-01 1.12382575e-04\n",
      " 1.16318921e+00 1.45024894e-01 3.12083523e+00 4.24288779e+00\n",
      " 3.86228435e-01 3.64342979e-01 8.24154612e-07 2.13793723e-01\n",
      " 2.74594028e+00 1.16206071e-01 3.76593315e+00 4.76502785e-01\n",
      " 2.28297337e-02 3.26047983e-05 1.85824467e+00 2.68865136e-02\n",
      " 2.67002088e+00 2.41051404e-06 2.57393346e+00 3.88717800e-01]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.2138, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.5013, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.12045270816492\n",
      "test accuracy:  98.31584085916523\n",
      "11\n",
      "[6.73544411e-01 1.11338850e-06 5.67765320e-01 3.95669536e-01\n",
      " 1.31405207e-01 7.96586089e-03 2.41378835e-01 3.47907765e+00\n",
      " 2.12139230e+00 4.87060986e-01 6.16237236e-01 2.83364291e-04\n",
      " 1.82411347e-01 2.57902482e+00 2.53224532e-05 6.01840399e-01\n",
      " 3.66776336e+00 3.78348786e-01 3.24979810e+00 6.48706542e-02\n",
      " 1.98703865e-01 1.02294185e+00 2.05602574e+00 4.10748376e+00\n",
      " 5.23314061e-01 3.56475457e+00 2.26173343e+00 2.27885392e+00\n",
      " 4.89375188e-02 1.90314178e+00 2.53503983e+00 2.08004558e+00\n",
      " 5.31066072e-06 1.97813010e-01 5.14576826e-01 6.51400171e-01\n",
      " 2.11871086e+00 2.24962843e+00 1.16814387e-01 4.04118751e-01\n",
      " 4.66331875e+00 1.23934692e-04 5.66071159e-06 4.75121001e+00\n",
      " 1.41463315e-02 6.55469783e-01 4.40843336e-02 2.76035405e-01\n",
      " 3.03889106e+00 1.03973115e+00 3.88432035e+00 1.13696838e-01\n",
      " 1.25389681e+00 2.59169380e-01 5.96991809e-01 3.43940664e+00\n",
      " 4.24735002e-01 1.81595867e-02 2.60496899e+00 2.53276540e+00\n",
      " 3.32167818e-02 2.16646366e-01 9.27284122e-02 3.18588716e-05\n",
      " 1.67485515e+00 9.47443798e-02 6.57073842e-05 5.68374589e-07\n",
      " 4.94137049e-02 2.25997314e+00 4.34339874e-01 3.92557931e+00\n",
      " 3.17718011e-01 4.28900605e+00 8.30220468e-02 3.51988489e+00\n",
      " 1.07716243e-01 6.21787663e-02 5.21601976e-02 5.20774178e-04\n",
      " 1.86600041e+00 2.01575764e-01 2.73899272e+00 4.07567635e+00\n",
      " 1.75748905e-01 2.99618156e-01 3.03910395e-06 1.68625671e-01\n",
      " 2.09181622e+00 7.73939133e-02 3.76013505e+00 4.09893861e-01\n",
      " 9.61998863e-03 7.56031014e-07 1.77352468e+00 1.03301666e-02\n",
      " 2.28206607e+00 2.07697273e-06 2.10625661e+00 1.41378921e-01]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1778, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.3738, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.38318512530316\n",
      "test accuracy:  98.55992189406884\n",
      "98.32255456750202\n",
      "accuracy is best accuracy\n",
      "[100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 92.85714285714286, 100.0, 97.5609756097561, 95.1219512195122, 100.0, 100.0, 97.5609756097561, 100.0, 97.5609756097561, 97.5609756097561, 100.0, 97.61904761904762, 95.1219512195122, 97.5609756097561, 100.0, 92.6829268292683, 97.61904761904762, 100.0, 95.1219512195122, 97.61904761904762, 100.0, 97.5609756097561, 100.0, 95.23809523809523, 95.23809523809523, 100.0, 97.5609756097561, 100.0, 100.0, 95.1219512195122, 92.85714285714286, 100.0, 97.5609756097561, 90.47619047619048, 100.0, 100.0, 90.47619047619048, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 97.61904761904762, 100.0, 100.0, 100.0, 97.5, 100.0, 100.0, 100.0, 100.0, 97.5609756097561, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 97.5609756097561, 100.0, 97.61904761904762, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 97.5609756097561, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 97.61904761904762, 97.5609756097561, 100.0, 100.0, 100.0, 100.0, 90.2439024390244, 100.0, 82.92682926829268, 100.0]\n",
      "12\n",
      "[5.88122006e-01 7.95095877e-06 7.08394691e-01 3.77539634e-01\n",
      " 1.54569487e-01 1.08858498e-03 1.33265826e-01 2.18844070e+00\n",
      " 2.28595009e+00 4.22070125e-01 5.93817393e-01 1.43634513e-04\n",
      " 1.53783323e-01 2.42861402e+00 4.34826445e-05 5.48663352e-01\n",
      " 2.95905657e+00 3.15143654e-01 2.44611535e+00 6.82838700e-02\n",
      " 1.10053605e-01 1.03874654e+00 1.86102678e+00 2.96808816e+00\n",
      " 4.95332698e-01 3.17558781e+00 1.61729067e+00 1.62201317e+00\n",
      " 4.81178253e-02 1.18602575e+00 1.85964994e+00 1.40609552e+00\n",
      " 5.10678147e-07 1.80834561e-01 5.26352648e-01 5.87970080e-01\n",
      " 2.12043162e+00 1.22014774e+00 1.00354207e-01 3.98657852e-01\n",
      " 4.07179669e+00 8.49030122e-06 4.50020019e-07 3.14222416e+00\n",
      " 2.29588614e-03 1.69367041e+00 1.75095267e-02 2.24219672e-01\n",
      " 2.00212104e+00 1.09644785e+00 2.87587766e+00 5.36328237e-02\n",
      " 1.34023175e+00 1.97461503e-01 8.61922491e-01 2.85128214e+00\n",
      " 4.62306531e-01 1.40996090e-02 9.31774958e-01 2.29439370e+00\n",
      " 1.68736297e-02 2.08373872e-01 7.37066442e-02 3.43249089e-06\n",
      " 1.51164266e+00 6.39556154e-02 5.33846934e-06 8.65651377e-08\n",
      " 3.08832287e-02 1.96486687e+00 3.83191953e-01 2.77362488e+00\n",
      " 4.58928813e-01 3.44735764e+00 4.83305054e-02 2.43432701e+00\n",
      " 6.05010388e-02 3.77110340e-02 6.28406530e-02 5.32183134e-05\n",
      " 1.14323570e+00 1.38341722e-01 2.84546514e+00 3.20177833e+00\n",
      " 1.96793440e-01 3.38345215e-01 9.15380131e-08 1.71463601e-01\n",
      " 1.94797144e+00 6.17211515e-02 2.69515947e+00 4.18220432e-01\n",
      " 1.04163628e-02 1.65491602e-06 1.19900731e+00 1.07154109e-02\n",
      " 2.61097454e+00 2.36646868e-07 1.90603853e+00 2.67795851e-01]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1944, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.4268, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.38318512530316\n",
      "test accuracy:  98.63314620453991\n",
      "13\n",
      "[6.15935519e-01 1.85189816e-06 7.59250897e-01 3.47811222e-01\n",
      " 1.89162003e-01 4.91635542e-04 1.08699071e-01 2.11632292e+00\n",
      " 1.45349124e+00 3.99917708e-01 6.15329485e-01 9.63187341e-05\n",
      " 1.36042137e-01 2.73590625e+00 3.29176879e-05 5.59496132e-01\n",
      " 2.77834434e+00 3.66123907e-01 2.61323920e+00 3.64935101e-02\n",
      " 1.41608112e-01 8.99568143e-01 1.67995373e+00 2.95972424e+00\n",
      " 4.31245648e-01 2.77864900e+00 1.71672781e+00 1.58476283e+00\n",
      " 4.45788062e-02 1.10917224e+00 1.67745662e+00 1.44333750e+00\n",
      " 2.85026622e-07 1.61177147e-01 4.90513258e-01 5.99054566e-01\n",
      " 1.62723531e+00 1.18418622e+00 9.67124800e-02 3.44742267e-01\n",
      " 3.65112286e+00 3.52577165e-06 1.38726128e-07 3.15070505e+00\n",
      " 1.26356034e-03 6.42894635e-01 2.09654488e-02 2.37249124e-01\n",
      " 1.96426355e+00 1.11532411e+00 2.98544432e+00 3.06631260e-02\n",
      " 1.32410475e+00 1.49306367e-01 9.33436341e-01 2.45241135e+00\n",
      " 4.36339073e-01 1.28765580e-02 8.44019852e-01 1.90245446e+00\n",
      " 1.68116900e-02 1.97732644e-01 6.53431728e-02 1.60546201e-06\n",
      " 1.21190632e+00 6.93892504e-02 1.98172880e-06 4.72462711e-08\n",
      " 3.53059453e-02 1.75211609e+00 3.93767862e-01 2.53011446e+00\n",
      " 5.00381112e-01 3.52413181e+00 4.58601557e-02 2.59576886e+00\n",
      " 7.25633492e-02 5.48517201e-02 8.42656778e-02 2.28012731e-05\n",
      " 1.10520870e+00 1.02134003e-01 2.17573536e+00 3.31895730e+00\n",
      " 1.47891493e-01 3.47432774e-01 3.26148993e-08 1.97400807e-01\n",
      " 1.57401957e+00 4.70175937e-02 2.56329826e+00 4.16152957e-01\n",
      " 1.09453051e-02 1.22673805e-06 1.29925792e+00 1.24789043e-02\n",
      " 1.63445941e+00 1.04391008e-07 1.67581355e+00 3.00388048e-01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1781, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.3708, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.38318512530316\n",
      "test accuracy:  98.48669758359776\n",
      "14\n",
      "[5.60370432e-01 3.25496614e-07 6.92481215e-01 3.62084168e-01\n",
      " 1.13605577e-01 1.12767807e-03 1.72819789e-01 1.96796186e+00\n",
      " 1.26529095e+00 3.89733618e-01 6.14564093e-01 3.08440617e-05\n",
      " 2.18252749e-01 2.18022907e+00 5.60976575e-06 5.55052419e-01\n",
      " 2.47696426e+00 3.78580331e-01 2.06937634e+00 3.18894379e-02\n",
      " 7.65112889e-02 8.54617215e-01 1.34361091e+00 2.45739554e+00\n",
      " 4.83798691e-01 2.50089637e+00 1.28005951e+00 1.33949873e+00\n",
      " 3.41217360e-02 8.82965564e-01 1.34301642e+00 1.30769682e+00\n",
      " 3.35995708e-07 1.89072371e-01 5.19362294e-01 6.01269338e-01\n",
      " 1.70211097e+00 1.02882555e+00 7.11979591e-02 3.61393803e-01\n",
      " 3.03882025e+00 1.06854464e-05 1.77633561e-07 2.69973688e+00\n",
      " 2.39728701e-03 6.17601192e-01 9.63422003e-03 2.19294538e-01\n",
      " 1.62238996e+00 1.26266890e+00 2.58893371e+00 4.60901587e-02\n",
      " 1.43051113e+00 1.36288706e-01 7.54379497e-01 2.06435929e+00\n",
      " 5.10655918e-01 8.67352005e-03 1.05341085e+00 1.68311991e+00\n",
      " 9.24631509e-03 1.61856778e-01 6.48992181e-02 1.76934979e-06\n",
      " 1.01987856e+00 5.27920250e-02 4.60305908e-06 8.42693834e-08\n",
      " 2.11536744e-02 1.52204849e+00 3.44258942e-01 2.11542528e+00\n",
      " 4.09622744e-01 3.78888041e+00 3.83424433e-02 2.02604925e+00\n",
      " 8.96856202e-02 2.75012888e-02 3.37546490e-02 4.05340044e-05\n",
      " 1.12887148e+00 1.74966428e-01 2.19074880e+00 2.85818229e+00\n",
      " 2.02573347e-01 2.73181899e-01 2.69398932e-08 1.32110154e-01\n",
      " 1.33768555e+00 4.16948618e-02 2.03916514e+00 4.42281478e-01\n",
      " 5.97603781e-03 1.86285920e-07 1.00314350e+00 4.83578202e-03\n",
      " 1.47991827e+00 8.05652449e-08 2.11048331e+00 1.91325683e-01]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1680, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.3497, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.464025869038\n",
      "test accuracy:  98.43788137661704\n",
      "98.38318512530316\n",
      "accuracy is best accuracy\n",
      "[100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 92.85714285714286, 100.0, 100.0, 100.0, 100.0, 97.5609756097561, 97.5609756097561, 100.0, 97.5609756097561, 100.0, 97.5609756097561, 97.61904761904762, 95.1219512195122, 97.5609756097561, 100.0, 92.6829268292683, 95.23809523809523, 100.0, 95.1219512195122, 97.61904761904762, 97.61904761904762, 100.0, 100.0, 95.23809523809523, 95.23809523809523, 100.0, 97.5609756097561, 100.0, 100.0, 92.6829268292683, 92.85714285714286, 100.0, 100.0, 92.85714285714286, 100.0, 100.0, 90.47619047619048, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 97.61904761904762, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 97.5609756097561, 100.0, 97.5609756097561, 100.0, 100.0, 97.5609756097561, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 97.5609756097561, 100.0, 100.0, 100.0, 100.0, 100.0, 97.5609756097561, 97.5609756097561, 100.0, 100.0, 100.0, 97.5609756097561, 92.85714285714286, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 92.85714285714286, 97.5609756097561, 90.2439024390244, 100.0, 82.92682926829268, 100.0]\n",
      "15\n",
      "[6.10663163e-01 4.45001318e-06 8.23436613e-01 3.62113403e-01\n",
      " 1.38085128e-01 1.71833005e-04 8.94996672e-02 1.74487098e+00\n",
      " 1.14740590e+00 3.76229861e-01 6.61548539e-01 5.82909580e-05\n",
      " 2.00997449e-01 1.89224401e+00 1.57214910e-05 5.43681660e-01\n",
      " 2.06541583e+00 3.28825022e-01 2.04397428e+00 2.74515765e-02\n",
      " 1.39119105e-01 1.04118984e+00 1.25413299e+00 2.45620757e+00\n",
      " 4.67569581e-01 2.41223347e+00 1.22924977e+00 1.33480292e+00\n",
      " 2.82654784e-02 8.54874677e-01 1.30151951e+00 1.17628694e+00\n",
      " 1.17681290e-07 1.56906517e-01 5.03372365e-01 5.80924515e-01\n",
      " 1.45606948e+00 8.34996205e-01 6.32543203e-02 3.63588014e-01\n",
      " 3.16855924e+00 1.15483613e-06 2.36355875e-08 2.87547946e+00\n",
      " 3.95370787e-04 6.28612786e-01 2.08108995e-02 2.12005637e-01\n",
      " 1.53796532e+00 1.18104766e+00 2.53478841e+00 1.44661205e-02\n",
      " 1.36277189e+00 1.18054673e-01 9.95715219e-01 2.26610047e+00\n",
      " 4.43834574e-01 7.97333647e-03 8.64199689e-01 1.66997966e+00\n",
      " 1.13929107e-02 1.47738350e-01 5.11438181e-02 5.21837391e-07\n",
      " 9.43537439e-01 4.84702485e-02 5.90512600e-07 1.67280784e-08\n",
      " 1.69913370e-02 1.48671607e+00 3.18735284e-01 2.12401414e+00\n",
      " 5.36058456e-01 3.24358415e+00 4.00278130e-02 2.10439978e+00\n",
      " 8.10238290e-02 5.33203011e-02 4.03103852e-02 4.97289494e-06\n",
      " 1.11336648e+00 1.31135322e-01 1.89793621e+00 2.95207930e+00\n",
      " 1.23034061e-01 2.94972726e-01 7.45071085e-09 1.41094317e-01\n",
      " 1.26404949e+00 3.84743903e-02 2.15259093e+00 4.17298129e-01\n",
      " 5.69969288e-03 5.66801133e-07 1.07862758e+00 4.90318753e-03\n",
      " 1.32756750e+00 2.61971096e-08 1.35348754e+00 3.25528087e-01]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1662, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.3565, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.50444624090542\n",
      "test accuracy:  98.65755430803027\n",
      "98.464025869038\n",
      "accuracy is best accuracy\n",
      "[100.0, 100.0, 100.0, 100.0, 97.5609756097561, 100.0, 100.0, 95.23809523809523, 100.0, 100.0, 100.0, 100.0, 100.0, 97.5609756097561, 100.0, 97.5609756097561, 95.1219512195122, 97.5609756097561, 97.61904761904762, 95.1219512195122, 100.0, 100.0, 92.6829268292683, 97.61904761904762, 100.0, 95.1219512195122, 97.61904761904762, 97.61904761904762, 100.0, 100.0, 97.61904761904762, 92.85714285714286, 100.0, 97.5609756097561, 100.0, 100.0, 95.1219512195122, 92.85714285714286, 100.0, 97.5609756097561, 90.47619047619048, 100.0, 100.0, 92.85714285714286, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 97.61904761904762, 100.0, 100.0, 100.0, 97.5, 100.0, 100.0, 100.0, 100.0, 97.5609756097561, 100.0, 97.5609756097561, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 97.5609756097561, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 97.61904761904762, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 92.85714285714286, 100.0, 90.2439024390244, 100.0, 87.8048780487805, 100.0]\n",
      "16\n",
      "[5.84709165e-01 1.35188102e-07 6.93117230e-01 3.57519966e-01\n",
      " 8.17665574e-02 9.08091761e-04 1.85258162e-01 1.52434969e+00\n",
      " 1.00025381e+00 3.64019221e-01 6.63316430e-01 1.08489738e-05\n",
      " 2.97592639e-01 1.55464839e+00 1.68091782e-06 5.45234591e-01\n",
      " 1.95134975e+00 3.32258808e-01 1.81593451e+00 2.12013009e-02\n",
      " 1.33839963e-01 8.99659526e-01 1.08505201e+00 2.12691472e+00\n",
      " 5.57827464e-01 2.37780768e+00 1.13953358e+00 1.31735897e+00\n",
      " 2.38186729e-02 6.99306068e-01 1.12886674e+00 1.00427798e+00\n",
      " 1.67871065e-07 1.28847843e-01 4.98185602e-01 5.89467231e-01\n",
      " 1.29285635e+00 6.71358985e-01 5.56001541e-02 3.49753653e-01\n",
      " 2.78192202e+00 6.15125994e-06 6.78408564e-08 2.26556307e+00\n",
      " 1.60851644e-03 5.89320820e-01 1.41912264e-02 2.12569283e-01\n",
      " 1.53239287e+00 1.21327438e+00 2.26327945e+00 3.72262345e-02\n",
      " 1.39751912e+00 9.66680405e-02 7.18029362e-01 2.26272805e+00\n",
      " 4.41556979e-01 7.46607638e-03 8.26204738e-01 1.34990925e+00\n",
      " 9.77764562e-03 1.27903654e-01 4.74961155e-02 7.80964975e-07\n",
      " 8.20315651e-01 3.36788724e-02 2.86856539e-06 1.37251408e-08\n",
      " 1.37252548e-02 1.27691995e+00 3.08719984e-01 1.84457286e+00\n",
      " 3.97985913e-01 3.01732549e+00 3.17666857e-02 2.00661176e+00\n",
      " 6.68229849e-02 6.22396886e-02 1.62081786e-02 2.11753996e-05\n",
      " 1.12181438e+00 2.39034431e-01 1.73853463e+00 2.71420749e+00\n",
      " 1.02963149e-01 2.48393723e-01 8.96569710e-09 1.08565965e-01\n",
      " 1.45329626e+00 3.36303379e-02 1.84673530e+00 4.22685912e-01\n",
      " 4.03382614e-03 3.63744699e-08 7.91338362e-01 2.74990761e-03\n",
      " 1.04948839e+00 2.72308663e-08 1.64497379e+00 1.54929191e-01]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full train loss:  tensor(0.1565, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.3445, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.4842360549717\n",
      "test accuracy:  98.63314620453991\n",
      "17\n",
      "[9.52172090e-01 7.73276983e-08 6.62499339e-01 3.54332450e-01\n",
      " 8.34814694e-02 1.08524285e-03 1.98637449e-01 1.56161863e+00\n",
      " 8.29403338e-01 3.47883597e-01 6.73105288e-01 7.46195605e-06\n",
      " 2.90662689e-01 1.29693379e+00 1.39808360e-06 5.57532893e-01\n",
      " 1.57454289e+00 3.47078186e-01 1.86200324e+00 1.56644893e-02\n",
      " 8.93491591e-02 9.43524128e-01 9.24860458e-01 2.19962041e+00\n",
      " 5.22106644e-01 1.99794826e+00 1.10301587e+00 2.20793582e+00\n",
      " 2.27821873e-02 7.47697307e-01 1.31001912e+00 1.09329779e+00\n",
      " 2.10933228e-07 1.60231315e-01 5.05617663e-01 5.83908881e-01\n",
      " 1.08232037e+00 7.09691087e-01 8.35253573e-02 3.33055889e-01\n",
      " 3.06167503e+00 7.33828035e-06 5.49688138e-08 2.38950183e+00\n",
      " 6.38790649e-03 5.52962962e-01 9.00587087e-03 2.03359619e-01\n",
      " 1.33913696e+00 5.50897122e+00 2.58137859e+00 3.59317173e-02\n",
      " 1.48046456e+00 7.77070253e-02 6.46717451e-01 1.90635748e+00\n",
      " 5.09454720e-01 4.87776270e-03 9.96684220e-01 1.22741773e+00\n",
      " 5.57929003e-03 1.10977323e-01 4.17249086e-02 7.31518160e-07\n",
      " 6.73532698e-01 2.88614906e-02 3.24285609e-06 1.68014078e-08\n",
      " 1.37700190e-02 1.16663509e+00 3.04162677e-01 2.57326765e+00\n",
      " 3.61274149e-01 3.14965468e+00 2.57853868e-02 2.17203677e+00\n",
      " 5.38767173e-02 3.65666415e-02 1.67932885e-02 2.09963756e-05\n",
      " 1.13448065e+00 2.23706719e-01 1.77260705e+00 2.84293030e+00\n",
      " 1.32030433e-01 2.46862369e-01 7.16664651e-09 9.79141774e-02\n",
      " 9.44084582e-01 2.57528635e-02 2.00598803e+00 4.52417421e-01\n",
      " 3.43838350e-03 1.55291562e-08 8.30333172e-01 2.59327858e-03\n",
      " 1.00162023e+00 2.19612495e-08 1.03303340e+00 1.14456253e-01]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1461, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.3205, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.62570735650768\n",
      "test accuracy:  98.80400292897242\n",
      "98.50444624090542\n",
      "accuracy is best accuracy\n",
      "[100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 95.23809523809523, 100.0, 100.0, 100.0, 100.0, 100.0, 97.5609756097561, 100.0, 97.5609756097561, 100.0, 100.0, 97.61904761904762, 95.1219512195122, 100.0, 100.0, 92.6829268292683, 95.23809523809523, 100.0, 97.5609756097561, 97.61904761904762, 100.0, 100.0, 100.0, 97.61904761904762, 97.61904761904762, 100.0, 97.5609756097561, 100.0, 100.0, 97.5609756097561, 92.85714285714286, 100.0, 100.0, 92.85714285714286, 100.0, 100.0, 92.85714285714286, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 97.61904761904762, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 97.5609756097561, 100.0, 100.0, 100.0, 100.0, 97.5609756097561, 100.0, 100.0, 100.0, 100.0, 97.5609756097561, 100.0, 97.5609756097561, 100.0, 95.23809523809523, 100.0, 100.0, 100.0, 100.0, 97.5609756097561, 100.0, 100.0, 100.0, 100.0, 97.61904761904762, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 92.85714285714286, 97.5609756097561, 90.2439024390244, 100.0, 87.8048780487805, 100.0]\n",
      "18\n",
      "[6.28254522e-01 6.15577135e-07 8.34029846e-01 3.66651449e-01\n",
      " 1.05514437e-01 7.09923703e-05 6.36644978e-02 1.33676905e+00\n",
      " 8.31540568e-01 3.24498570e-01 7.43354880e-01 2.23119055e-05\n",
      " 2.55778134e-01 1.33821632e+00 3.74003327e-06 5.57327665e-01\n",
      " 1.63543530e+00 3.04181731e-01 1.62605496e+00 1.59512118e-02\n",
      " 1.80089448e-01 7.75865131e-01 9.91245861e-01 1.85444447e+00\n",
      " 5.03267925e-01 2.02380846e+00 9.08621182e-01 1.09914543e+00\n",
      " 1.98504236e-02 6.18734521e-01 9.40283491e-01 8.80617717e-01\n",
      " 6.39257263e-08 1.31160208e-01 5.45022349e-01 5.52240972e-01\n",
      " 1.10692826e+00 5.49099754e-01 4.71228495e-02 3.55215392e-01\n",
      " 2.50029144e+00 5.36498583e-07 4.65214470e-09 2.01923156e+00\n",
      " 1.58774267e-04 5.62662231e-01 2.25291337e-02 1.75374840e-01\n",
      " 1.13321386e+00 1.25275812e+00 2.05945097e+00 5.69133085e-03\n",
      " 1.37306683e+00 7.38973504e-02 9.67897474e-01 1.91679590e+00\n",
      " 4.30911973e-01 3.76907393e-03 7.55292872e-01 1.18505594e+00\n",
      " 1.15478032e-02 1.15040416e-01 4.56393661e-02 1.82002422e-07\n",
      " 7.01047067e-01 2.37115049e-02 2.19101986e-07 4.85893705e-09\n",
      " 8.56838331e-03 1.11679535e+00 2.73188006e-01 2.14760397e+00\n",
      " 5.28568697e-01 2.87906145e+00 4.02295783e-02 1.54161963e+00\n",
      " 1.13581227e-01 8.46801089e-02 2.21955731e-02 1.39732046e-06\n",
      " 1.17597566e+00 1.67735263e-01 1.75877384e+00 2.80352006e+00\n",
      " 7.28444693e-02 2.56351162e-01 2.08642602e-09 1.07965397e-01\n",
      " 9.57994829e-01 2.28255449e-02 1.68733703e+00 4.24853599e-01\n",
      " 3.12173503e-03 1.60348504e-07 6.71174838e-01 2.44670160e-03\n",
      " 8.85146779e-01 7.71421157e-09 1.37128027e+00 2.82893304e-01]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1461, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.3424, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.6661277283751\n",
      "test accuracy:  98.7551867219917\n",
      "98.62570735650768\n",
      "accuracy is best accuracy\n",
      "[100.0, 100.0, 100.0, 100.0, 97.5609756097561, 100.0, 100.0, 95.23809523809523, 100.0, 100.0, 100.0, 100.0, 97.5609756097561, 97.5609756097561, 100.0, 97.5609756097561, 100.0, 100.0, 97.61904761904762, 97.5609756097561, 100.0, 100.0, 92.6829268292683, 95.23809523809523, 100.0, 97.5609756097561, 95.23809523809523, 100.0, 100.0, 100.0, 97.61904761904762, 95.23809523809523, 100.0, 97.5609756097561, 100.0, 100.0, 97.5609756097561, 92.85714285714286, 100.0, 100.0, 90.47619047619048, 100.0, 100.0, 90.47619047619048, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 97.61904761904762, 100.0, 100.0, 100.0, 97.5, 100.0, 100.0, 100.0, 100.0, 97.5609756097561, 100.0, 97.5609756097561, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 97.5609756097561, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 97.5609756097561, 100.0, 100.0, 100.0, 100.0, 97.61904761904762, 100.0, 100.0, 100.0, 100.0, 97.5609756097561, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 95.1219512195122, 90.2439024390244, 100.0, 87.8048780487805, 100.0]\n",
      "19\n",
      "[4.79279488e-01 1.74539544e-07 7.71834207e-01 3.55463057e-01\n",
      " 1.01727403e-01 1.40134042e-04 9.62514235e-02 1.44637650e+00\n",
      " 8.07037415e-01 3.32756526e-01 6.66134229e-01 7.08767304e-06\n",
      " 2.67957210e-01 1.20833506e+00 1.08643653e-06 5.55690172e-01\n",
      " 1.52898725e+00 3.50213036e-01 1.51799644e+00 1.34359929e-02\n",
      " 1.77510648e-01 9.93016537e-01 8.47565271e-01 1.72755812e+00\n",
      " 5.14502582e-01 1.90259314e+00 8.42384480e-01 1.01748059e+00\n",
      " 1.73398928e-02 5.81599905e-01 8.70885345e-01 1.00253912e+00\n",
      " 5.00898656e-08 1.54736714e-01 4.88012154e-01 5.79406988e-01\n",
      " 1.03376590e+00 4.87646043e-01 4.24019985e-02 3.33127229e-01\n",
      " 2.40806696e+00 1.27638484e-06 5.48266094e-09 2.39242812e+00\n",
      " 2.76754197e-04 5.23209306e-01 5.93688823e-03 1.96917939e-01\n",
      " 9.99042410e-01 1.26871085e+00 1.90618607e+00 7.00180914e-03\n",
      " 1.46466206e+00 6.03602488e-02 8.26772810e-01 1.80839287e+00\n",
      " 5.01504570e-01 3.63787630e-03 9.99928207e-01 1.07146372e+00\n",
      " 3.94007852e-03 1.10764718e-01 3.59823706e-02 1.18205940e-07\n",
      " 6.41348355e-01 2.51861392e-02 4.16672753e-07 2.96108748e-09\n",
      " 9.72444026e-03 1.03727483e+00 4.11399653e-01 1.95524807e+00\n",
      " 4.57728246e-01 2.86821400e+00 1.94388612e-02 1.41483546e+00\n",
      " 4.72166691e-02 3.70923282e-02 1.93501523e-02 2.59564309e-06\n",
      " 1.12829741e+00 1.58741640e-01 1.49380247e+00 3.37741372e+00\n",
      " 1.11778413e-01 2.66808245e-01 1.13100361e-09 1.02063843e-01\n",
      " 8.47102464e-01 1.92535041e-02 1.23188436e+00 4.40640536e-01\n",
      " 2.58497968e-03 4.45865005e-08 6.61213898e-01 1.63017611e-03\n",
      " 8.14511182e-01 4.40120739e-09 9.52017739e-01 1.99449503e-01]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1420, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.3118, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.56507679870656\n",
      "test accuracy:  98.73077861850135\n",
      "20\n",
      "[4.86250145e-01 2.30972422e-07 8.05905279e-01 3.49316875e-01\n",
      " 7.81391339e-02 7.98547757e-05 7.34327745e-02 1.09480908e+00\n",
      " 8.24325284e-01 3.27676923e-01 6.81628983e-01 8.71105502e-06\n",
      " 3.09589389e-01 1.41812927e+00 1.20138193e-06 5.58165528e-01\n",
      " 1.62592046e+00 7.88353448e-01 1.64368156e+00 1.04311053e-02\n",
      " 9.05896660e-02 1.25481898e+00 9.57190521e-01 1.59685248e+00\n",
      " 5.52303209e-01 2.07598250e+00 7.26374700e-01 9.18939274e-01\n",
      " 1.78928697e-02 4.71738066e-01 7.61846923e-01 7.06992107e-01\n",
      " 4.44371533e-08 1.15168198e-01 4.74465209e-01 5.80811832e-01\n",
      " 1.35518191e+00 4.01926970e-01 4.13071760e-02 3.20729936e-01\n",
      " 2.15405018e+00 7.85701406e-07 2.84469322e-09 1.67270746e+00\n",
      " 1.42770747e-04 5.06170864e-01 7.19910413e-03 1.85655102e-01\n",
      " 8.55946038e-01 1.26700044e+00 1.78228153e+00 4.06621405e-03\n",
      " 1.42181412e+00 5.06909024e-02 8.75551510e-01 1.44111032e+00\n",
      " 4.55306869e-01 3.48585622e-03 8.60155333e-01 1.18809500e+00\n",
      " 4.86457417e-03 1.56616933e-01 3.12043611e-02 8.21417442e-08\n",
      " 7.35155438e-01 1.83297877e-02 2.11881071e-07 2.30162014e-09\n",
      " 1.02796832e-02 1.12182014e+00 2.77557184e-01 1.62874248e+00\n",
      " 4.81957217e-01 2.69901512e+00 1.85067896e-02 1.48626182e+00\n",
      " 5.50511338e-02 4.79154292e-01 1.27608971e-02 1.34087595e-06\n",
      " 1.13427423e+00 1.99176979e-01 1.95880973e+00 2.33328287e+00\n",
      " 7.57115121e-02 2.46715931e-01 9.19274599e-10 8.25911955e-02\n",
      " 9.38303376e-01 1.60302351e-02 1.16529403e+00 4.21938383e-01\n",
      " 2.76540551e-03 5.60745156e-08 5.32013752e-01 1.12416207e-03\n",
      " 8.92456834e-01 3.37901106e-09 1.06859566e+00 2.25401261e-01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1349, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.3369, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.60549717057397\n",
      "test accuracy:  98.77959482548206\n",
      "21\n",
      "[4.64860147e-01 5.83134977e-08 7.19365008e-01 3.26779401e-01\n",
      " 1.89497905e-01 2.22791130e-04 1.21154990e-01 1.23311689e+00\n",
      " 7.62006204e-01 3.34488719e-01 6.88802952e-01 2.51000047e-06\n",
      " 1.71239650e-01 1.33367784e+00 5.65394449e-07 5.58590012e-01\n",
      " 1.44047199e+00 4.24543189e-01 1.68899787e+00 8.63299388e-03\n",
      " 7.19467648e-02 1.27748604e+00 9.04252087e-01 1.57590988e+00\n",
      " 4.62566944e-01 2.00812019e+00 8.32313290e-01 1.85726594e+00\n",
      " 2.22490884e-02 4.52420958e-01 8.86811396e-01 8.17954944e-01\n",
      " 7.55363524e-08 1.27672537e-01 4.24387544e-01 6.19147876e-01\n",
      " 1.09816314e+00 3.78761426e-01 5.51022542e-02 3.00292104e-01\n",
      " 2.18826539e+00 2.35837796e-06 5.18786162e-09 1.69845750e+00\n",
      " 3.20534146e-04 4.64752442e-01 5.39925783e-03 2.09483587e-01\n",
      " 8.34696382e-01 1.30935498e+00 1.78688536e+00 7.44674874e-03\n",
      " 1.48325723e+00 4.27444709e-02 6.98612326e-01 1.44948443e+00\n",
      " 4.88464282e-01 4.56139046e-03 9.43878548e-01 1.22357467e+00\n",
      " 3.29794712e-03 1.59668527e-01 2.79118582e-02 1.02073670e-07\n",
      " 7.45453967e-01 3.46904743e-02 5.84165648e-07 4.05000657e-09\n",
      " 1.85515137e-02 1.09798541e+00 3.02427696e-01 1.64987277e+00\n",
      " 3.90576694e-01 2.78708633e+00 1.49911482e-02 1.24017998e+00\n",
      " 5.73990141e-02 5.14043912e-02 5.07267635e-02 3.86569934e-06\n",
      " 1.05812388e+00 6.27823319e-02 1.54650415e+00 2.38099422e+00\n",
      " 8.07535610e-02 3.44235521e-01 7.70450715e-10 1.75065424e-01\n",
      " 8.76603235e-01 1.57786316e-02 1.03995046e+00 4.40003787e-01\n",
      " 4.88594559e-03 1.15546294e-08 5.21207533e-01 3.93923788e-03\n",
      " 8.33371350e-01 3.03233367e-09 1.00039667e+00 1.31391325e-01]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1334, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.3274, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.60549717057397\n",
      "test accuracy:  98.82841103246278\n",
      "22\n",
      "[4.69908615e-01 5.89310754e-08 7.29471299e-01 3.83326191e-01\n",
      " 6.11102490e-02 1.65767474e-04 1.05371220e-01 1.03390492e+00\n",
      " 6.33695815e-01 2.93747734e-01 7.02744483e-01 4.42628506e-06\n",
      " 3.43419212e-01 1.17676093e+00 4.61972800e-07 5.70004173e-01\n",
      " 1.67869731e+00 2.92743883e-01 1.33696739e+00 8.95341990e-03\n",
      " 7.84954997e-02 7.78285820e-01 8.18787040e-01 1.47152788e+00\n",
      " 5.78999413e-01 2.02328989e+00 1.11234661e+00 8.88555079e-01\n",
      " 1.10551331e-02 4.31344785e-01 7.16643632e-01 7.28044185e-01\n",
      " 4.89256874e-08 1.20613522e-01 5.64280645e-01 5.38361282e-01\n",
      " 9.58963093e-01 3.63782794e-01 4.27786784e-02 3.74168322e-01\n",
      " 2.04768537e+00 1.43995810e-06 3.32404905e-09 1.70636767e+00\n",
      " 2.55902679e-04 4.59209154e-01 5.25114483e-03 2.02858312e-01\n",
      " 7.67258767e-01 1.32025155e+00 1.67662298e+00 5.86378744e-03\n",
      " 1.49417081e+00 5.59807309e-02 7.21898555e-01 1.37361791e+00\n",
      " 4.73054687e-01 2.03911418e-03 9.25839311e-01 1.00180733e+00\n",
      " 3.23918281e-03 1.08920165e-01 2.62289455e-02 7.12316193e-08\n",
      " 5.93490662e-01 1.70591552e-02 3.85295864e-07 2.47177065e-09\n",
      " 3.68208840e-03 1.03097800e+00 2.33394223e-01 1.35987447e+00\n",
      " 4.05779592e-01 2.69025527e+00 1.43090887e-02 1.14965414e+00\n",
      " 5.01541286e-02 4.97928612e-02 7.84711984e-03 2.65609498e-06\n",
      " 1.22762340e+00 2.49547102e-01 1.52409375e+00 2.28235296e+00\n",
      " 6.95922699e-02 2.10184519e-01 5.59174831e-10 7.31704665e-02\n",
      " 1.21140433e+00 1.64151564e-02 9.59323326e-01 4.32502232e-01\n",
      " 1.75638004e-03 9.86924899e-09 4.99659546e-01 9.69243654e-04\n",
      " 6.79876820e-01 2.20633047e-09 8.68556185e-01 1.32347966e-01]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1319, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.2987, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.60549717057397\n",
      "test accuracy:  98.8772272394435\n",
      "23\n",
      "[4.46568651e-01 1.79989562e-07 8.12522398e-01 3.73492000e-01\n",
      " 1.11439414e-01 3.83475131e-05 5.01526047e-02 9.79881936e-01\n",
      " 6.17887783e-01 3.00937281e-01 6.90406908e-01 6.13775446e-06\n",
      " 2.68844296e-01 1.12267028e+00 6.35714822e-07 5.75334045e-01\n",
      " 1.22266063e+00 3.39649810e-01 1.29377419e+00 6.62476346e-03\n",
      " 4.57336541e-01 1.00048329e+00 7.40371255e-01 1.41127468e+00\n",
      " 5.06430738e-01 1.79493121e+00 6.57364717e-01 8.66155663e-01\n",
      " 1.09639469e-02 3.91769090e-01 7.30654584e-01 6.18196859e-01\n",
      " 4.06431261e-08 1.06467500e-01 5.25496793e-01 5.65659043e-01\n",
      " 9.42291822e-01 3.65578864e-01 3.17915668e-02 3.55795742e-01\n",
      " 2.36991302e+00 4.05699376e-07 1.13426563e-09 1.50486578e+00\n",
      " 7.26729108e-05 4.54253269e-01 4.44691244e-03 1.47945964e-01\n",
      " 7.19274504e-01 1.30087503e+00 1.63340305e+00 6.49231053e-03\n",
      " 1.47192251e+00 4.16718231e-02 8.71553876e-01 1.59432798e+00\n",
      " 4.46908287e-01 1.78273590e-03 9.00442577e-01 1.44588698e+00\n",
      " 3.14977900e-03 1.06464326e-01 2.38675503e-02 4.94858784e-08\n",
      " 5.94345558e-01 1.42033371e-02 1.05087473e-07 1.26887063e-09\n",
      " 4.38046493e-03 9.32402834e-01 2.33376929e-01 1.33717386e+00\n",
      " 4.84037574e-01 2.66937094e+00 1.25129368e-02 1.08232105e+00\n",
      " 3.98472776e-02 4.37042987e-02 1.90641579e-02 6.44670595e-07\n",
      " 1.21279778e+00 1.18624893e-01 1.40656464e+00 2.45119407e+00\n",
      " 6.27868410e-02 2.59006053e-01 6.31425244e-10 1.16938019e-01\n",
      " 8.75003111e-01 1.23881640e-02 8.60642262e-01 4.16225653e-01\n",
      " 1.72572008e-03 3.38474738e-08 4.75903240e-01 1.20029385e-03\n",
      " 6.50678706e-01 2.07223528e-09 8.58591240e-01 2.08119761e-01]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1364, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.3035, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.6661277283751\n",
      "test accuracy:  98.92604344642422\n",
      "24\n",
      "[4.17978392e-01 1.01374664e-07 7.87283868e-01 3.57197742e-01\n",
      " 7.82010112e-02 5.09995355e-05 6.22463644e-02 9.66552852e-01\n",
      " 5.84271545e-01 2.96700363e-01 6.90051604e-01 3.52295505e-06\n",
      " 3.21404327e-01 1.07089254e+00 1.18888984e-06 5.66135103e-01\n",
      " 1.14231932e+00 3.68379073e-01 1.31953816e+00 5.46369792e-03\n",
      " 6.07405137e-02 1.01041058e+00 7.12170790e-01 1.39253332e+00\n",
      " 5.31322372e-01 1.73143825e+00 6.26086735e-01 8.51754381e-01\n",
      " 1.19746598e-02 4.33072724e-01 6.91519926e-01 6.14678657e-01\n",
      " 3.48352157e-08 1.09253431e-01 4.82254603e-01 5.55236043e-01\n",
      " 9.06206603e-01 3.45581920e-01 2.86627302e-02 3.34828479e-01\n",
      " 1.96220006e+00 5.67610130e-07 1.09171885e-09 1.50526223e+00\n",
      " 8.55841277e-05 4.33796967e-01 2.95413484e-03 1.47678327e-01\n",
      " 7.05097773e-01 1.31697981e+00 1.62824152e+00 1.98933496e-03\n",
      " 1.51776776e+00 3.07608766e-02 8.08667097e-01 1.57797505e+00\n",
      " 4.65975126e-01 1.99598678e-03 9.53904272e-01 9.34497647e-01\n",
      " 2.37196862e-03 8.78970303e-02 2.27443163e-02 3.64453387e-08\n",
      " 5.47958475e-01 1.15711342e-02 1.39904018e-07 1.18371359e-09\n",
      " 5.31519615e-03 9.38849864e-01 2.33772927e-01 1.26919842e+00\n",
      " 4.50856601e-01 2.67523390e+00 1.00960821e-02 1.10222873e+00\n",
      " 3.23982449e-02 3.65073296e-02 1.16376968e-02 8.68171733e-07\n",
      " 1.95730913e+00 2.21107152e-01 1.34571252e+00 2.23869773e+00\n",
      " 6.73111755e-02 2.43868861e-01 4.33484637e-10 8.90170024e-02\n",
      " 7.33041673e-01 9.97032835e-03 8.48062406e-01 4.25707347e-01\n",
      " 1.34691067e-03 1.83665251e-08 4.83222313e-01 6.63170117e-04\n",
      " 6.06194675e-01 1.40792953e-09 8.07986395e-01 1.74209740e-01]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1246, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.3332, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.68633791430881\n",
      "test accuracy:  98.95045154991458\n",
      "98.6661277283751\n",
      "accuracy is best accuracy\n",
      "[100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 95.23809523809523, 100.0, 100.0, 100.0, 100.0, 100.0, 97.5609756097561, 100.0, 97.5609756097561, 97.5609756097561, 100.0, 97.61904761904762, 95.1219512195122, 97.5609756097561, 100.0, 92.6829268292683, 97.61904761904762, 100.0, 95.1219512195122, 97.61904761904762, 100.0, 100.0, 100.0, 97.61904761904762, 97.61904761904762, 100.0, 97.5609756097561, 100.0, 100.0, 97.5609756097561, 95.23809523809523, 100.0, 100.0, 92.85714285714286, 100.0, 100.0, 95.23809523809523, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 95.23809523809523, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 97.5609756097561, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 97.5609756097561, 100.0, 100.0, 100.0, 100.0, 92.6829268292683, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 97.61904761904762, 100.0, 100.0, 100.0, 100.0, 97.5609756097561, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 90.2439024390244, 100.0, 92.6829268292683, 100.0]\n",
      "25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.04870017e-01 5.69553390e-08 7.64636030e-01 3.92905366e-01\n",
      " 9.22886440e-02 5.78682476e-05 6.89881551e-02 8.86464828e-01\n",
      " 5.83234719e-01 4.92361493e-01 7.03799659e-01 2.05992632e-06\n",
      " 3.00764027e-01 1.05098394e+00 7.60987744e-07 5.93333232e-01\n",
      " 1.37512414e+00 3.06149903e-01 1.20407893e+00 6.48394420e-03\n",
      " 5.02898765e-02 7.55514115e-01 7.20240180e-01 1.32151669e+00\n",
      " 5.17113235e-01 1.69302591e+00 6.17166714e-01 8.09661368e-01\n",
      " 1.04144881e-02 3.62447025e-01 6.84555946e-01 6.20172822e-01\n",
      " 3.46151965e-08 1.23671630e-01 5.75444922e-01 5.22545954e-01\n",
      " 9.97360830e-01 3.17998716e-01 2.55333677e-02 3.89137620e-01\n",
      " 1.76800570e+00 7.17637778e-07 1.03050264e-09 1.34971932e+00\n",
      " 1.01591562e-04 4.20348333e-01 2.35723649e-03 1.27882414e-01\n",
      " 6.17794921e-01 1.34934119e+00 1.49968945e+00 2.12418036e-03\n",
      " 1.56620679e+00 3.96172328e-02 7.58009187e-01 1.22974113e+00\n",
      " 4.94552454e-01 1.27887613e-03 1.01506652e+00 8.88951889e-01\n",
      " 1.78064719e-03 8.46216062e-02 2.15792624e-02 3.00669238e-08\n",
      " 5.27636135e-01 1.31712065e-02 1.57788921e-07 1.15833438e-09\n",
      " 2.32492352e-03 9.08408249e-01 2.18383061e-01 1.43082002e+00\n",
      " 4.25392584e-01 2.56088385e+00 9.24588533e-03 1.04043812e+00\n",
      " 2.87435331e-02 3.00350811e-02 1.42994771e-02 1.09178638e-06\n",
      " 1.55147464e+00 1.35607482e-01 1.32066993e+00 2.09759761e+00\n",
      " 7.58381859e-02 2.68421051e-01 3.04240743e-10 1.03957812e-01\n",
      " 6.59332011e-01 1.06318438e-02 7.21706667e-01 4.42099765e-01\n",
      " 1.27749567e-03 1.01092598e-08 4.29643767e-01 6.90097158e-04\n",
      " 5.73441587e-01 1.06991368e-09 7.78638396e-01 1.45629829e-01]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1306, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.2977, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.82780921584478\n",
      "test accuracy:  99.07249206736637\n",
      "98.68633791430881\n",
      "accuracy is best accuracy\n",
      "[100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 97.61904761904762, 100.0, 100.0, 100.0, 100.0, 100.0, 97.5609756097561, 100.0, 97.5609756097561, 97.5609756097561, 100.0, 97.61904761904762, 97.5609756097561, 100.0, 100.0, 95.1219512195122, 97.61904761904762, 100.0, 97.5609756097561, 97.61904761904762, 100.0, 100.0, 100.0, 97.61904761904762, 95.23809523809523, 100.0, 97.5609756097561, 100.0, 100.0, 97.5609756097561, 95.23809523809523, 100.0, 100.0, 92.85714285714286, 100.0, 100.0, 95.23809523809523, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 95.23809523809523, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 97.5609756097561, 100.0, 97.5609756097561, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 97.5609756097561, 100.0, 97.61904761904762, 100.0, 100.0, 100.0, 100.0, 97.5609756097561, 100.0, 100.0, 100.0, 100.0, 97.61904761904762, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 90.2439024390244, 100.0, 92.6829268292683, 100.0]\n",
      "26\n",
      "[1.02964733e+00 5.74148308e-08 7.76178025e-01 3.77846329e-01\n",
      " 5.35658189e-02 4.35998569e-05 6.03734251e-02 8.95551325e-01\n",
      " 6.07345436e-01 2.78690368e-01 7.45828367e-01 2.07218750e-06\n",
      " 3.69225909e-01 9.93202910e-01 2.63626131e-07 5.88971990e-01\n",
      " 1.20253748e+00 3.42653086e-01 1.20795937e+00 4.71389480e-03\n",
      " 5.70467116e-02 9.12726443e-01 6.47297315e-01 1.29304650e+00\n",
      " 5.59059307e-01 1.62061237e+00 5.93400139e-01 8.20343834e-01\n",
      " 9.45426621e-03 3.72152932e-01 6.86653651e-01 5.62843109e-01\n",
      " 3.17090305e-08 1.31720043e-01 5.35267826e-01 5.41271502e-01\n",
      " 8.21412116e-01 3.81124698e-01 2.88150743e-02 3.49026477e-01\n",
      " 1.94971504e+00 2.05072880e-06 7.38898149e-10 1.53808699e+00\n",
      " 7.87828405e-05 4.16664039e-01 2.81394309e-03 1.28441408e-01\n",
      " 6.82587851e-01 1.42260859e+00 1.78969051e+00 1.66734328e-03\n",
      " 1.62509247e+00 2.80323794e-02 2.62590601e+00 1.30504411e+00\n",
      " 5.20042405e-01 1.27091430e-03 1.04650762e+00 8.60043866e-01\n",
      " 1.54650139e-03 7.25053414e-02 2.00377553e-02 2.48669333e-08\n",
      " 5.09357109e-01 1.07381089e-02 1.17666470e-07 8.51198775e-10\n",
      " 2.96590805e-03 8.22578331e-01 2.13769509e-01 1.14843978e+00\n",
      " 4.37803674e-01 2.66627816e+00 1.13755489e-02 9.61096476e-01\n",
      " 3.95975773e-02 3.62128409e-02 7.06432699e-03 8.08308808e-07\n",
      " 1.47317829e+00 2.19764790e-01 1.26095954e+00 2.18305849e+00\n",
      " 6.99763583e-02 2.35391935e-01 2.76531547e-10 1.22434836e-01\n",
      " 6.40695514e-01 8.03433684e-03 7.53663823e-01 4.69233584e-01\n",
      " 1.06219628e-03 9.66558604e-09 4.49676029e-01 3.89824461e-04\n",
      " 5.25163247e-01 8.97959940e-10 7.34953302e-01 1.50601339e-01]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1376, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.3065, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.6661277283751\n",
      "test accuracy:  98.97485965340493\n",
      "27\n",
      "[4.19052852e-01 5.35680428e-08 7.80563301e-01 3.82410485e-01\n",
      " 1.14256079e-01 3.66990831e-05 5.78966921e-02 1.01837086e+00\n",
      " 5.02178486e-01 2.67146110e-01 7.58871573e-01 1.83339554e-06\n",
      " 2.81408515e-01 1.03875767e+00 5.39643572e-07 5.94528500e-01\n",
      " 1.34304629e+00 3.36044270e-01 1.17420800e+00 4.26250213e-03\n",
      " 5.75506624e-02 8.58680009e-01 6.38100529e-01 1.41161021e+00\n",
      " 4.86620260e-01 1.61398759e+00 5.34509694e-01 7.90200278e-01\n",
      " 9.01809567e-03 3.53440562e-01 6.24446261e-01 5.42620168e-01\n",
      " 2.60919446e-08 1.21387619e-01 5.49637794e-01 5.26785069e-01\n",
      " 9.16388340e-01 3.56166633e-01 3.37486606e-02 3.52157530e-01\n",
      " 1.72503385e+00 4.37504152e-07 6.03303184e-10 1.29689535e+00\n",
      " 7.03875376e-05 4.12936441e-01 2.58643628e-03 1.16222853e-01\n",
      " 5.90857177e-01 1.43537649e+00 1.45184357e+00 1.47708609e-03\n",
      " 1.62848834e+00 2.54067912e-02 7.79026361e-01 1.20543792e+00\n",
      " 5.13292110e-01 1.15378621e-03 1.01755144e+00 8.31425068e-01\n",
      " 1.63882233e-03 7.77295886e-02 1.71577303e-02 2.10180227e-08\n",
      " 7.15837314e-01 1.02774129e-02 1.01706344e-07 7.30169422e-10\n",
      " 2.57632853e-03 8.18678140e-01 2.04151810e-01 1.12169351e+00\n",
      " 4.39792567e-01 3.37580880e+00 7.92369642e-03 8.87584300e-01\n",
      " 3.86468927e-02 4.08985876e-02 1.81961418e-02 7.06054172e-07\n",
      " 1.25261476e+00 9.70213574e-02 1.25953968e+00 2.08974789e+00\n",
      " 5.85659239e-02 3.11491014e-01 2.46793544e-10 1.18818168e-01\n",
      " 6.14710217e-01 6.91921019e-03 6.68935314e-01 4.67232888e-01\n",
      " 1.30865411e-03 8.06863971e-09 4.09564482e-01 7.10198815e-04\n",
      " 4.95348682e-01 7.50367683e-10 7.23952954e-01 1.49343896e-01]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1266, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.2720, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.80759902991107\n",
      "test accuracy:  99.02367586038565\n",
      "28\n",
      "[3.57984731e-01 5.58358992e-08 7.89972814e-01 3.64792405e-01\n",
      " 1.01007935e-01 2.63878289e-05 5.07000444e-02 8.18394135e-01\n",
      " 4.68019865e-01 2.68747880e-01 7.38401141e-01 1.98106404e-06\n",
      " 2.94023641e-01 1.21377356e+00 2.11515993e-07 5.88553861e-01\n",
      " 1.24091896e+00 3.69487808e-01 1.14029910e+00 4.64086168e-03\n",
      " 3.91570277e-02 9.95018075e-01 6.00932722e-01 1.13988160e+00\n",
      " 5.04425919e-01 1.53840835e+00 5.23369077e-01 7.79600351e-01\n",
      " 9.94003266e-03 3.47270587e-01 7.34903088e-01 5.61473153e-01\n",
      " 2.58325826e-08 1.23253962e-01 5.04558362e-01 5.40834226e-01\n",
      " 8.18620954e-01 2.83793285e-01 2.22308179e-02 4.36390463e-01\n",
      " 1.66097574e+00 3.63749556e-07 4.72263616e-10 1.23928288e+00\n",
      " 5.66850339e-05 4.03607356e-01 1.44361604e-03 1.14427638e-01\n",
      " 6.02803548e-01 1.43759657e+00 1.41526387e+00 1.14413438e-03\n",
      " 1.66546531e+00 1.80609393e-02 7.89728120e-01 1.16288557e+00\n",
      " 5.29660776e-01 1.28650651e-03 1.06369554e+00 9.57160828e-01\n",
      " 1.03382250e-03 7.54240966e-02 1.67264878e-02 1.92501347e-08\n",
      " 6.83305573e-01 8.36387595e-03 8.27332943e-08 6.30944267e-10\n",
      " 3.35762012e-03 7.85952893e-01 2.03058214e-01 1.05869892e+00\n",
      " 4.47425470e-01 3.20027193e+00 6.57889827e-03 8.61255084e-01\n",
      " 2.29945771e-02 3.19927465e-02 1.33991386e-02 5.58135292e-07\n",
      " 1.19780847e+00 1.47365248e-01 1.22767991e+00 2.51917013e+00\n",
      " 6.60814674e-02 2.94210026e-01 2.59292365e-10 1.06696820e-01\n",
      " 5.73482837e-01 5.74537292e-03 7.20378006e-01 4.70189552e-01\n",
      " 8.78992031e-04 8.45335185e-09 4.10170233e-01 5.64659826e-04\n",
      " 4.65555851e-01 7.16049219e-10 6.85423031e-01 1.54430644e-01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1246, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.2932, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.78738884397737\n",
      "test accuracy:  98.92604344642422\n",
      "29\n",
      "[4.14798912e-01 3.02634083e-08 7.53698702e-01 3.90357320e-01\n",
      " 5.24284554e-02 3.78317946e-05 6.76928996e-02 7.73041009e-01\n",
      " 5.56476770e-01 2.50365770e-01 7.81510922e-01 1.09936879e-06\n",
      " 3.66623219e-01 1.01905248e+00 2.19118229e-07 6.08666222e-01\n",
      " 1.01037111e+00 3.28993318e-01 1.12440105e+00 3.80562651e-03\n",
      " 6.27678294e-02 8.12919094e-01 6.21418820e-01 1.20906029e+00\n",
      " 5.61931634e-01 1.61145235e+00 5.06902968e-01 8.60480534e-01\n",
      " 8.34870427e-03 3.20783281e-01 5.95080383e-01 4.70761586e-01\n",
      " 3.20616514e-08 9.50180077e-02 5.64496845e-01 5.10530856e-01\n",
      " 8.12890578e-01 2.64284446e-01 2.87988413e-02 3.83569373e-01\n",
      " 2.18019531e+00 6.50606251e-07 5.93009557e-10 1.21054006e+00\n",
      " 7.14440990e-05 3.74478475e-01 2.49892105e-03 9.73284069e-02\n",
      " 5.70074958e-01 1.45853014e+00 1.39110600e+00 1.25567495e-03\n",
      " 1.61641430e+00 2.21794911e-02 2.45246274e+00 1.14584025e+00\n",
      " 4.78988358e-01 9.21443353e-04 9.36509733e-01 8.46147548e-01\n",
      " 1.38872485e-03 6.34779538e-02 1.43202597e-02 1.73069763e-08\n",
      " 4.86360837e-01 8.04299123e-03 1.33399847e-07 9.97202578e-10\n",
      " 1.76908963e-03 8.03980243e-01 1.98116185e-01 1.09249439e+00\n",
      " 4.00666409e-01 2.53166825e+00 7.10925228e-03 7.93953569e-01\n",
      " 3.95013822e-02 5.10297528e-02 5.40092959e-03 9.95719988e-07\n",
      " 1.28035664e+00 2.16760437e-01 1.23381734e+00 2.03467871e+00\n",
      " 3.74448528e-02 2.36340614e-01 1.98236288e-10 6.75326268e-02\n",
      " 5.81361063e-01 6.20319188e-03 6.12574618e-01 4.52266955e-01\n",
      " 6.70241789e-04 4.34266304e-09 3.90895093e-01 2.88729650e-04\n",
      " 4.71064317e-01 5.57106790e-10 7.65748083e-01 1.17688036e-01]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1528, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.2808, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.74696847210994\n",
      "test accuracy:  99.02367586038565\n",
      "30\n",
      "[3.37175111e-01 3.40202820e-08 7.61738366e-01 3.88385089e-01\n",
      " 4.99624530e-02 3.04213074e-05 5.33428053e-02 8.08885675e-01\n",
      " 4.39407504e-01 2.46382120e-01 7.55201927e-01 1.17137126e-06\n",
      " 3.68139049e-01 1.44680242e+00 1.94715808e-07 6.12484584e-01\n",
      " 8.95097302e-01 3.41297109e-01 1.14652949e+00 3.47668514e-03\n",
      " 3.00158979e-02 8.54593558e-01 6.10359102e-01 1.18418785e+00\n",
      " 5.64472736e-01 1.48282592e+00 5.35916164e-01 7.81339962e-01\n",
      " 7.64252576e-03 3.38745553e-01 6.17890811e-01 6.37288198e-01\n",
      " 2.81907946e-08 1.32438285e-01 5.53627729e-01 5.20876789e-01\n",
      " 7.58665378e-01 2.75875259e-01 2.54538373e-02 3.76299993e-01\n",
      " 1.73641053e+00 4.45809494e-07 4.36797611e-10 1.29602820e+00\n",
      " 5.72131244e-05 3.66442669e-01 8.97917192e-04 9.63619115e-02\n",
      " 5.54815512e-01 1.48075578e+00 1.46078337e+00 1.00688089e-03\n",
      " 4.94356934e+00 1.84637378e-02 7.27342996e-01 1.18708125e+00\n",
      " 5.60488789e-01 1.37034271e-03 1.13740933e+00 7.75523119e-01\n",
      " 6.83007919e-04 6.19271081e-02 1.67406104e-02 1.50752579e-08\n",
      " 4.42106051e-01 7.44208440e-03 9.27311236e-08 6.77408724e-10\n",
      " 1.67260010e-03 7.71654455e-01 1.80987358e-01 1.02527607e+00\n",
      " 4.12950637e-01 2.64036493e+00 5.70694687e-03 8.34993849e-01\n",
      " 1.80103527e-02 2.35552291e-02 5.44064011e-03 7.16841138e-07\n",
      " 1.27509831e+00 2.06414326e-01 1.87387030e+00 2.10790045e+00\n",
      " 6.82060086e-02 2.36740442e-01 2.09800954e-10 6.53550251e-02\n",
      " 5.33769540e-01 4.94451591e-03 7.96278375e-01 4.89086133e-01\n",
      " 6.11641261e-04 4.69437921e-09 4.00049122e-01 2.54592238e-04\n",
      " 4.40218131e-01 5.41405350e-10 6.56270168e-01 1.21219177e-01]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1324, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.2687, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.80759902991107\n",
      "test accuracy:  98.8772272394435\n",
      "31\n",
      "[3.76003522e-01 3.41550862e-08 7.66587816e-01 4.14883944e-01\n",
      " 8.52270623e-02 2.45994352e-05 4.39409959e-02 7.25672439e-01\n",
      " 4.43652160e-01 2.43409577e-01 7.89502459e-01 1.16804759e-06\n",
      " 3.03579734e-01 9.44705006e-01 1.77422368e-07 6.36000098e-01\n",
      " 8.82540856e-01 3.24020392e-01 1.08456924e+00 3.41213179e-03\n",
      " 4.51222008e-02 1.01556153e+00 5.64805208e-01 1.06885543e+00\n",
      " 5.14040694e-01 1.47255474e+00 4.76704547e-01 7.46325879e-01\n",
      " 7.08906562e-03 3.32843407e-01 5.90123221e-01 4.48079194e-01\n",
      " 2.81375757e-08 1.12974612e-01 5.95485967e-01 5.14986043e-01\n",
      " 8.04585523e-01 2.58525100e-01 2.75482544e-02 6.50580654e-01\n",
      " 1.54068512e+00 3.69048158e-07 6.17946208e-10 1.17837091e+00\n",
      " 4.84784614e-05 3.57802055e-01 1.48485033e-03 1.24933692e-01\n",
      " 7.87661119e-01 1.49155243e+00 1.31264202e+00 8.16055923e-04\n",
      " 1.67779638e+00 2.12512280e-02 7.35143874e-01 1.13692922e+00\n",
      " 5.17818744e-01 6.63823088e-04 1.04291424e+00 8.36448070e-01\n",
      " 8.47945923e-04 6.82787481e-02 1.29479687e-02 1.33889860e-08\n",
      " 9.25157254e-01 7.26919685e-03 7.24014737e-08 5.80331295e-10\n",
      " 1.08649626e-03 7.51859081e-01 1.79246640e-01 9.91959356e-01\n",
      " 4.17520982e-01 2.47891466e+00 6.09861358e-03 7.27347407e-01\n",
      " 2.99190661e-02 3.43709536e-02 1.13635092e-02 5.80581269e-07\n",
      " 1.35570828e+00 1.11283413e-01 1.32771154e+00 1.93820319e+00\n",
      " 4.59254989e-02 2.90463730e-01 2.12968587e-10 1.02275200e-01\n",
      " 6.05871012e-01 5.38779704e-03 5.43333057e-01 4.72784636e-01\n",
      " 6.29782992e-04 4.63348217e-09 3.86903852e-01 4.07554888e-04\n",
      " 4.10650164e-01 5.25112814e-10 6.72236412e-01 1.21990273e-01]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1324, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.2995, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.80759902991107\n",
      "test accuracy:  98.92604344642422\n",
      "32\n",
      "[3.34615963e-01 4.30967677e-08 7.87097881e-01 4.16127965e-01\n",
      " 3.77005215e-02 1.61296695e-05 3.39274799e-02 7.02169461e-01\n",
      " 4.27290395e-01 4.50249581e-01 7.77329475e-01 4.40501158e-06\n",
      " 3.95963164e-01 9.42843573e-01 1.54114502e-07 6.38357367e-01\n",
      " 8.72884805e-01 3.38257373e-01 1.07048547e+00 2.80426825e-03\n",
      " 2.75731978e-02 8.39278664e-01 5.52102231e-01 1.02461450e+00\n",
      " 5.76790525e-01 1.46619445e+00 5.96411986e-01 7.41627352e-01\n",
      " 6.16448566e-03 3.09652634e-01 5.75877929e-01 4.16322571e-01\n",
      " 2.70976499e-08 1.33164658e-01 5.79004389e-01 5.72653235e-01\n",
      " 7.96212994e-01 2.62553333e-01 2.18695805e-02 4.06731717e-01\n",
      " 1.58219006e+00 2.32039444e-07 2.48241816e-10 1.46350054e+00\n",
      " 3.28963308e-05 3.55166068e-01 7.87090871e-04 9.80349228e-02\n",
      " 4.93081940e-01 1.50912671e+00 1.27967244e+00 6.12024659e-04\n",
      " 1.75668068e+00 1.85482845e-02 7.70966990e-01 1.17901340e+00\n",
      " 5.65325792e-01 6.19746728e-04 1.16119502e+00 7.42039426e-01\n",
      " 5.54663490e-04 5.23026328e-02 1.42961165e-02 1.30359426e-08\n",
      " 4.68432214e-01 6.84031628e-03 4.89937722e-08 3.80088793e-10\n",
      " 1.06135387e-03 7.54014177e-01 1.74388203e-01 9.88695921e-01\n",
      " 4.38467594e-01 2.49402802e+00 4.98171082e-03 8.70333362e-01\n",
      " 1.71134681e-02 2.06442251e-02 5.74396650e-03 3.86836004e-07\n",
      " 1.35748073e+00 2.29152057e-01 1.16264545e+00 1.90762798e+00\n",
      " 6.27601806e-02 2.39218535e-01 2.45552675e-10 5.41860742e-02\n",
      " 5.21739685e-01 4.98687126e-03 5.04536816e-01 4.95820417e-01\n",
      " 5.03428829e-04 5.76753906e-09 3.65325825e-01 1.75604399e-04\n",
      " 3.98357087e-01 5.52619699e-10 6.53276451e-01 1.36082197e-01]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1265, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.2859, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.82780921584478\n",
      "test accuracy:  99.04808396387601\n",
      "33\n",
      "[3.30962774e-01 3.98004026e-08 7.87547744e-01 3.94304937e-01\n",
      " 6.91235580e-02 1.40748608e-05 3.29002297e-02 6.91229555e-01\n",
      " 8.41551989e-01 2.55753516e-01 7.78995197e-01 1.39463521e-06\n",
      " 3.35604153e-01 2.03466721e+00 1.45058649e-07 6.26939874e-01\n",
      " 8.84389495e-01 3.81513073e-01 1.07578241e+00 2.09456727e-03\n",
      " 2.70873282e-02 1.27807579e+00 5.60304172e-01 1.02347252e+00\n",
      " 5.23547793e-01 1.47433736e+00 4.56314049e-01 7.34318849e-01\n",
      " 6.16283269e-03 2.96920951e-01 5.61342617e-01 4.06766009e-01\n",
      " 2.61183882e-08 1.34777264e-01 5.25198572e-01 5.41225907e-01\n",
      " 7.78135075e-01 2.32902133e-01 2.01290373e-02 3.74797740e-01\n",
      " 1.57185452e+00 2.30652166e-07 2.22631219e-10 1.09052213e+00\n",
      " 2.95116629e-05 3.45565881e-01 8.05449597e-04 9.48254130e-02\n",
      " 5.60369685e-01 1.51185574e+00 1.28156181e+00 5.25125233e-04\n",
      " 1.76029080e+00 1.23966994e-02 7.63091636e-01 1.35650863e+00\n",
      " 5.67382634e-01 7.18960205e-04 1.99291784e+00 7.53433871e-01\n",
      " 5.26184410e-04 5.21750051e-02 1.40483646e-02 1.17807273e-08\n",
      " 4.68163797e-01 5.79364016e-03 4.67740023e-08 3.64137650e-10\n",
      " 1.63130643e-03 7.71338369e-01 1.72503968e-01 1.00200152e+00\n",
      " 4.34225862e-01 2.48896411e+00 4.85079241e-03 7.84554432e-01\n",
      " 1.66996873e-02 1.94201985e-02 8.30156928e-03 3.73975713e-07\n",
      " 1.28903081e+00 1.40511353e-01 1.15550185e+00 1.91981237e+00\n",
      " 6.14665738e-02 2.83818335e-01 3.08912812e-10 8.45952124e-02\n",
      " 5.18471209e-01 3.70520091e-03 4.89869400e-01 4.95551269e-01\n",
      " 5.78513101e-04 5.34708602e-09 3.60118563e-01 2.44056075e-04\n",
      " 4.06278194e-01 1.43908074e-09 6.58171300e-01 1.33109203e-01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1352, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.2621, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.92886014551334\n",
      "test accuracy:  99.09690017085673\n",
      "98.82780921584478\n",
      "accuracy is best accuracy\n",
      "[100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 95.23809523809523, 100.0, 100.0, 97.5609756097561, 100.0, 97.5609756097561, 97.5609756097561, 100.0, 97.5609756097561, 100.0, 100.0, 97.61904761904762, 97.5609756097561, 100.0, 100.0, 95.1219512195122, 97.61904761904762, 100.0, 97.5609756097561, 97.61904761904762, 100.0, 100.0, 100.0, 97.61904761904762, 97.61904761904762, 100.0, 97.5609756097561, 100.0, 100.0, 97.5609756097561, 95.23809523809523, 100.0, 100.0, 92.85714285714286, 100.0, 100.0, 95.23809523809523, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 97.61904761904762, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 97.5609756097561, 100.0, 97.5609756097561, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 97.5609756097561, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 97.5609756097561, 100.0, 100.0, 100.0, 100.0, 97.61904761904762, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 90.2439024390244, 100.0, 92.6829268292683, 100.0]\n",
      "34\n",
      "[3.60990571e-01 2.36254557e-08 7.51640941e-01 9.72118649e-01\n",
      " 8.99784779e-02 2.19581730e-05 4.44927961e-02 7.27790449e-01\n",
      " 8.19346473e-01 2.37407245e-01 8.12711032e-01 7.82392226e-07\n",
      " 3.05845812e-01 1.04031784e+00 6.09060145e-07 6.51744944e-01\n",
      " 8.55971445e-01 3.25178671e-01 1.08032461e+00 2.84478572e-03\n",
      " 3.22505499e-02 7.95981548e-01 5.43680015e-01 1.02711310e+00\n",
      " 5.04266208e-01 1.45756680e+00 4.58312843e-01 7.65249574e-01\n",
      " 5.40003811e-03 2.97977338e-01 5.62487928e-01 4.03474820e-01\n",
      " 2.55602450e-08 1.33895379e-01 6.07339750e-01 5.20126629e-01\n",
      " 7.53165189e-01 2.31529844e-01 1.45385480e-02 4.15393419e-01\n",
      " 1.51002815e+00 3.50271612e-07 2.90713300e-10 1.08360888e+00\n",
      " 4.19688462e-05 7.02940269e-01 1.05422082e-03 1.81438206e-01\n",
      " 4.83129424e-01 1.53218462e+00 1.35281146e+00 6.49463164e-04\n",
      " 1.74253520e+00 2.91380416e-02 6.94630935e-01 1.05157797e+00\n",
      " 5.55616596e-01 5.15750321e-04 1.14440312e+00 7.36519100e-01\n",
      " 5.58277627e-04 5.99696612e-02 1.26096606e-02 1.04636549e-08\n",
      " 4.41596050e-01 6.71997094e-03 7.20743938e-08 4.92083192e-10\n",
      " 7.91320273e-04 7.76435255e-01 1.72138193e-01 9.82639918e-01\n",
      " 3.97114018e-01 2.56250948e+00 4.72771354e-03 7.21618375e-01\n",
      " 2.45540563e-02 2.23033712e-02 1.17284310e-02 6.32262561e-07\n",
      " 1.38832805e+00 9.59587595e-02 1.13867699e+00 1.91160690e+00\n",
      " 5.45942699e-02 3.20073537e-01 1.68928635e-10 1.03644518e-01\n",
      " 6.19810154e-01 4.76466698e-03 4.97841056e-01 4.93622166e-01\n",
      " 6.88302549e-04 2.81979459e-09 3.66283206e-01 6.15800982e-04\n",
      " 3.97770496e-01 3.73036782e-10 6.39569750e-01 9.98864702e-02]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1292, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.2678, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.90864995957963\n",
      "test accuracy:  99.14571637783745\n",
      "35\n",
      "[3.38821689e-01 2.76024742e-08 7.66697820e-01 4.24883921e-01\n",
      " 1.04154062e-01 1.55934085e-05 3.56817741e-02 6.66825430e-01\n",
      " 4.17774975e-01 2.39164189e-01 8.03184058e-01 9.52325954e-07\n",
      " 7.44674860e-01 9.30862469e-01 1.49999012e-07 6.51171466e-01\n",
      " 1.56224664e+00 3.43320785e-01 1.03598267e+00 1.99788853e-03\n",
      " 2.69353607e-02 8.66598817e-01 5.40688981e-01 9.81855439e-01\n",
      " 5.59493098e-01 1.45141570e+00 4.71885439e-01 1.58038662e+00\n",
      " 4.57800303e-03 3.03886387e-01 5.52650354e-01 4.35128947e-01\n",
      " 6.41959851e-08 1.36346392e-01 5.88460534e-01 5.31504184e-01\n",
      " 7.52440122e-01 2.23360684e-01 1.54644353e-02 4.03186429e-01\n",
      " 1.46709284e+00 2.91936028e-07 2.21503898e-10 1.17927849e+00\n",
      " 3.01883243e-05 3.18294931e-01 7.59637121e-04 9.71301546e-02\n",
      " 5.59599551e-01 1.55307613e+00 1.24518401e+00 4.62643990e-04\n",
      " 1.77361352e+00 1.51119304e-02 7.13133057e-01 1.06217770e+00\n",
      " 5.70807640e-01 4.83392893e-04 1.17991557e+00 7.49270467e-01\n",
      " 4.56127680e-04 4.47222285e-02 1.31018784e-02 9.45244242e-09\n",
      " 4.45554945e-01 5.45213518e-03 5.46598846e-08 4.11536832e-10\n",
      " 8.04681205e-04 7.39562357e-01 1.68471478e-01 9.86328736e-01\n",
      " 4.05400149e-01 2.46671175e+00 4.51467001e-03 6.37786042e-01\n",
      " 1.84852811e-02 1.79254854e-02 5.45782875e-03 4.91268381e-07\n",
      " 2.34660311e+00 1.73988610e-01 1.17801832e+00 1.88876414e+00\n",
      " 5.86198230e-02 2.77733354e-01 2.29290406e-10 6.70607390e-02\n",
      " 5.03994293e-01 4.22744528e-03 4.56319344e-01 4.99737033e-01\n",
      " 4.04891762e-04 3.52956644e-09 3.46379068e-01 1.46190436e-04\n",
      " 3.91906966e-01 4.07972072e-10 6.51959970e-01 1.09771947e-01]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1229, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.2760, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.86822958771221\n",
      "test accuracy:  99.07249206736637\n",
      "36\n",
      "[9.50374869e-01 3.74781228e-08 7.90580052e-01 4.09281359e-01\n",
      " 4.70156408e-02 9.70844551e-06 2.65054503e-02 6.65402618e-01\n",
      " 3.91484951e-01 2.42833980e-01 7.98301556e-01 1.34624404e-06\n",
      " 3.86829818e-01 9.07552981e-01 1.27859184e-07 6.41497011e-01\n",
      " 8.09038875e-01 3.71291529e-01 1.01907180e+00 1.62078464e-03\n",
      " 2.37785682e-02 9.77075665e-01 5.64149364e-01 9.68851934e-01\n",
      " 5.62106242e-01 1.39939401e+00 4.68785201e-01 1.56430995e+00\n",
      " 4.55989006e-03 2.93217348e-01 5.49428971e-01 3.94617090e-01\n",
      " 5.27878364e-08 1.37218048e-01 5.51969879e-01 5.43737742e-01\n",
      " 7.20723752e-01 2.27842176e-01 1.61173390e-02 3.73851027e-01\n",
      " 1.53595768e+00 1.96808495e-07 1.81694201e-10 1.06023485e+00\n",
      " 2.07252363e-05 3.16962581e-01 5.83762252e-04 9.22360875e-02\n",
      " 4.53865571e-01 1.54544693e+00 1.23557353e+00 3.58970984e-04\n",
      " 1.79977760e+00 1.06142348e-02 7.53125870e-01 1.02949867e+00\n",
      " 5.83407392e-01 8.88280570e-04 1.19840468e+00 7.00432526e-01\n",
      " 4.11682510e-04 4.27820090e-02 1.33401922e-02 1.00486328e-08\n",
      " 4.29191913e-01 4.84449024e-03 3.78817266e-08 3.13020276e-10\n",
      " 1.03781024e-03 7.15760238e-01 1.61915623e-01 9.30953445e-01\n",
      " 4.27335158e-01 2.47397817e+00 4.70041949e-03 6.86670603e-01\n",
      " 1.51116993e-02 1.64695467e-02 5.94912862e-03 3.19135937e-07\n",
      " 1.33767553e+00 1.84852800e-01 1.19936626e+00 1.98522469e+00\n",
      " 6.11082463e-02 2.68412636e-01 2.63508479e-10 1.18694845e-01\n",
      " 4.76527966e-01 3.22933772e-03 4.93439210e-01 5.05720318e-01\n",
      " 3.70021299e-04 4.95850250e-09 3.47793061e-01 1.32717630e-04\n",
      " 3.89734706e-01 5.02061280e-10 6.16487615e-01 1.28140910e-01]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1256, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.2784, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.92886014551334\n",
      "test accuracy:  99.12130827434709\n",
      "37\n",
      "[3.66205483e-01 2.85054238e-08 7.73097629e-01 4.27328594e-01\n",
      " 6.29561913e-02 1.18213784e-05 3.04252357e-02 6.54630174e-01\n",
      " 3.87691480e-01 2.27317042e-01 8.25915885e-01 9.77372936e-07\n",
      " 3.57375125e-01 9.60292671e-01 1.35036921e-07 6.56021963e-01\n",
      " 8.00664325e-01 3.33851778e-01 1.01677660e+00 2.64730378e-03\n",
      " 4.27712886e-02 8.15012261e-01 5.06703755e-01 9.60232403e-01\n",
      " 1.07246782e+00 1.41253724e+00 4.30756518e-01 7.17013775e-01\n",
      " 4.63119861e-03 4.44036481e-01 5.53695234e-01 3.78191445e-01\n",
      " 2.58761939e-08 1.02472220e-01 6.04864256e-01 5.16475171e-01\n",
      " 7.16147273e-01 2.21701242e-01 1.34950802e-02 6.52513045e-01\n",
      " 1.47994568e+00 2.37016479e-07 2.41770243e-10 1.25920269e+00\n",
      " 2.36594693e-05 3.06667327e-01 1.09988820e-03 8.57636795e-02\n",
      " 4.45370904e-01 1.54047870e+00 1.22396036e+00 3.65428135e-04\n",
      " 1.72314356e+00 1.30044331e-02 7.18459923e-01 1.01462082e+00\n",
      " 5.20392259e-01 4.32820090e-04 1.04916631e+00 6.90870231e-01\n",
      " 6.22853788e-04 4.70078296e-02 8.68445820e-03 1.86367143e-08\n",
      " 4.20102431e-01 4.38749455e-03 4.49518620e-08 3.43760617e-10\n",
      " 6.37058513e-04 7.15654145e-01 1.61189979e-01 9.27832074e-01\n",
      " 4.08599042e-01 2.47988357e+00 3.92658474e-03 6.10687952e-01\n",
      " 2.46226121e-02 3.25195184e-02 6.50929781e-03 4.09195494e-07\n",
      " 1.40382701e+00 1.91830970e-01 1.10252410e+00 1.86666272e+00\n",
      " 3.41685198e-02 2.85986252e-01 2.18992227e-10 8.04464612e-02\n",
      " 5.19443671e-01 3.54431519e-03 4.36477752e-01 4.77447795e-01\n",
      " 3.66400414e-04 3.53649986e-09 3.45429524e-01 1.56581756e-04\n",
      " 3.55213744e-01 4.00542737e-10 6.15692957e-01 1.10918624e-01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1295, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.2893, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.90864995957963\n",
      "test accuracy:  99.14571637783745\n",
      "38\n",
      "[3.23830008e-01 2.95785042e-08 7.77983304e-01 3.96020407e-01\n",
      " 4.78081232e-02 1.03822877e-05 2.68375479e-02 6.48377053e-01\n",
      " 3.77027245e-01 3.00383095e-01 8.08575787e-01 9.93041949e-07\n",
      " 3.84666536e-01 8.82765504e-01 1.25332834e-07 6.37595515e-01\n",
      " 7.84882666e-01 3.86730283e-01 1.00199885e+00 1.61726155e-03\n",
      " 2.46517400e-02 1.01453322e+00 4.91484420e-01 1.15131773e+00\n",
      " 5.59469374e-01 1.51899321e+00 4.27145034e-01 7.41129215e-01\n",
      " 5.13860624e-03 3.81416357e-01 8.60503892e-01 3.71836219e-01\n",
      " 2.57767159e-08 1.31416510e-01 5.29522514e-01 5.37968287e-01\n",
      " 7.03556332e-01 2.26719967e-01 1.49590217e-02 3.58606879e-01\n",
      " 1.42503502e+00 1.99861696e-07 1.56589977e-10 1.23004656e+00\n",
      " 2.04814627e-05 3.02254783e-01 1.60378708e-03 7.89857684e-02\n",
      " 4.36362015e-01 1.55604204e+00 1.34424255e+00 3.27805793e-04\n",
      " 1.81201055e+00 7.46636255e-03 7.27359758e-01 1.01014139e+00\n",
      " 5.78689762e-01 5.82399693e-04 1.19812220e+00 6.92924766e-01\n",
      " 3.74083261e-04 4.41197382e-02 1.15281208e-02 8.24252789e-09\n",
      " 4.26028505e-01 4.05434222e-03 3.78500647e-08 2.92835839e-10\n",
      " 1.23877851e-03 7.01294646e-01 1.55448302e-01 9.07015622e-01\n",
      " 4.13680597e-01 2.50145240e+00 3.85365628e-03 5.95674949e-01\n",
      " 1.40262880e-02 1.66980205e-02 4.48874945e-03 3.53721396e-07\n",
      " 1.80730413e+00 1.75927487e-01 1.08989514e+00 1.85651215e+00\n",
      " 5.69003038e-02 2.63961885e-01 2.29423674e-10 6.46204665e-02\n",
      " 4.58931338e-01 2.39208406e-03 5.04051769e-01 5.02429438e-01\n",
      " 2.95655828e-04 3.55852385e-09 3.41375422e-01 1.16794602e-04\n",
      " 3.50466837e-01 4.01072938e-10 6.03385087e-01 1.12884331e-01]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1227, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.2838, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.90864995957963\n",
      "test accuracy:  99.07249206736637\n",
      "39\n",
      "[3.13437084e-01 2.57141289e-08 7.74259950e-01 4.23237027e-01\n",
      " 6.33184666e-02 9.78475206e-06 2.73757531e-02 6.46890834e-01\n",
      " 3.78968638e-01 2.20374264e-01 8.10945706e-01 8.93927072e-07\n",
      " 3.53920393e-01 9.33399998e-01 1.25173559e-07 6.59446189e-01\n",
      " 8.49223686e-01 3.35373645e-01 9.88306522e-01 1.75176881e-03\n",
      " 2.03313583e-02 8.14645278e-01 4.97310614e-01 9.18662928e-01\n",
      " 5.40329779e-01 1.55650251e+00 4.27989348e-01 7.00477662e-01\n",
      " 4.43229441e-03 3.72430903e-01 5.37410699e-01 3.54146318e-01\n",
      " 2.61931524e-08 1.41326747e-01 6.03607074e-01 5.20893172e-01\n",
      " 7.76204283e-01 2.17886310e-01 1.26376210e-02 3.98631358e-01\n",
      " 1.42867817e+00 2.07352709e-07 1.49919216e-10 1.00565558e+00\n",
      " 2.02352537e-05 2.95718256e-01 3.98334535e-04 7.59879110e-02\n",
      " 4.21784917e-01 1.58087496e+00 1.17992051e+00 3.06744761e-04\n",
      " 1.85334030e+00 9.95099668e-03 7.13884471e-01 9.82701990e-01\n",
      " 6.04176508e-01 4.08187940e-04 1.25170496e+00 6.78854896e-01\n",
      " 3.16270376e-04 5.00464658e-02 1.26201157e-02 7.55419923e-09\n",
      " 4.20731874e-01 4.03519927e-03 3.87434604e-08 3.20840840e-10\n",
      " 6.24509444e-04 7.02205972e-01 1.54496827e-01 8.96933955e-01\n",
      " 4.07407982e-01 2.44439249e+00 4.01319647e-03 7.11442495e-01\n",
      " 1.17190965e-02 1.38599635e-02 6.22875458e-03 3.64866478e-07\n",
      " 1.39863778e+00 1.28687712e-01 1.08992664e+00 1.82660099e+00\n",
      " 6.45586132e-02 2.86544878e-01 2.14436427e-10 8.18588787e-02\n",
      " 4.66738036e-01 2.76623988e-03 4.28675835e-01 5.16241285e-01\n",
      " 3.00668072e-04 3.12551738e-09 3.53318720e-01 1.42651496e-04\n",
      " 3.41149431e-01 3.63850421e-10 8.74627329e-01 1.07522902e-01]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1230, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.2812, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.98949070331447\n",
      "test accuracy:  99.14571637783745\n",
      "98.92886014551334\n",
      "accuracy is best accuracy\n",
      "[100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 95.23809523809523, 100.0, 100.0, 100.0, 100.0, 100.0, 97.5609756097561, 100.0, 100.0, 97.5609756097561, 100.0, 100.0, 97.5609756097561, 100.0, 100.0, 95.1219512195122, 97.61904761904762, 100.0, 97.5609756097561, 97.61904761904762, 100.0, 100.0, 100.0, 97.61904761904762, 100.0, 100.0, 97.5609756097561, 100.0, 100.0, 97.5609756097561, 95.23809523809523, 100.0, 100.0, 95.23809523809523, 100.0, 100.0, 95.23809523809523, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 97.61904761904762, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 97.5609756097561, 100.0, 97.5609756097561, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 97.5609756097561, 100.0, 97.5609756097561, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 97.5609756097561, 100.0, 100.0, 100.0, 100.0, 97.61904761904762, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 95.1219512195122, 90.2439024390244, 100.0, 92.6829268292683, 100.0]\n",
      "40\n",
      "[3.28820056e-01 1.77080107e-08 7.54020511e-01 4.07168314e-01\n",
      " 5.74041045e-02 1.18178534e-05 3.54756880e-02 6.32229325e-01\n",
      " 4.21191938e-01 4.10428579e-01 8.26712787e-01 6.27708596e-07\n",
      " 3.70263070e-01 9.19970434e-01 1.50024992e-07 6.46351530e-01\n",
      " 7.68412513e-01 3.46386368e-01 1.00840571e+00 1.46375331e-03\n",
      " 2.69107144e-02 8.79454003e-01 4.91178867e-01 8.98930028e-01\n",
      " 5.46453801e-01 1.35584387e+00 4.10975695e-01 8.08059450e-01\n",
      " 4.16072012e-03 2.87534822e-01 5.38865830e-01 3.53620877e-01\n",
      " 2.60064944e-08 1.25262736e-01 5.74493737e-01 5.11958104e-01\n",
      " 7.07925421e-01 2.17670059e-01 1.98813257e-02 3.74157295e-01\n",
      " 1.37686476e+00 3.00112666e-07 2.14342280e-10 9.97841087e-01\n",
      " 2.27944694e-05 2.81043453e-01 4.99256129e-04 7.42851505e-02\n",
      " 4.82077371e-01 1.59008090e+00 1.36550486e+00 3.11024351e-04\n",
      " 1.84241327e+00 7.91312068e-03 6.65128901e-01 9.71580620e-01\n",
      " 5.78248070e-01 4.36915225e-04 1.19646663e+00 6.70572598e-01\n",
      " 3.31199173e-04 4.65442443e-02 9.79518257e-03 6.57844476e-09\n",
      " 4.13465700e-01 3.63837359e-03 5.31964669e-08 4.04821481e-10\n",
      " 7.44981310e-04 6.90673525e-01 1.55401185e-01 1.11236888e+00\n",
      " 3.79448773e-01 2.44285178e+00 3.20392505e-03 6.63149374e-01\n",
      " 1.47550499e-02 1.94130379e-02 5.25378710e-03 5.48739581e-07\n",
      " 1.35301358e+00 1.45212089e-01 1.08146416e+00 1.90868388e+00\n",
      " 4.91130396e-02 2.73981119e-01 1.72929504e-10 7.59325696e-02\n",
      " 7.20350762e-01 2.38413573e-03 3.97341594e-01 5.08746636e-01\n",
      " 2.71658368e-04 2.12974257e-09 3.29428315e-01 1.17684457e-04\n",
      " 3.33291814e-01 2.80062806e-10 5.91840821e-01 8.99798799e-02]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1299, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.2696, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.92886014551334\n",
      "test accuracy:  99.1701244813278\n",
      "41\n",
      "[3.12232637e-01 1.53662727e-08 7.44228222e-01 3.93249068e-01\n",
      " 8.30409189e-02 1.27013351e-05 3.83215448e-02 6.09344132e-01\n",
      " 3.57283591e-01 2.22307540e-01 8.24415054e-01 5.36770605e-07\n",
      " 4.05910017e-01 8.78902749e-01 1.65191601e-07 6.37456946e-01\n",
      " 8.25658802e-01 3.66489694e-01 9.65359552e-01 1.39746043e-03\n",
      " 2.14608377e-02 9.54704932e-01 4.81203815e-01 8.69011914e-01\n",
      " 5.68159928e-01 1.33203212e+00 4.11139728e-01 7.11363186e-01\n",
      " 8.05226218e-03 2.98934783e-01 8.74923994e-01 3.46118049e-01\n",
      " 2.70870969e-08 1.37673888e-01 5.43975524e-01 5.16415220e-01\n",
      " 7.29291724e-01 2.22015943e-01 1.31916614e-02 3.57813286e-01\n",
      " 1.32094984e+00 3.32322409e-07 2.15151175e-10 9.86885403e-01\n",
      " 2.38438388e-05 2.72471561e-01 3.88103813e-04 6.97731839e-02\n",
      " 4.13961370e-01 1.59923989e+00 1.14475206e+00 3.09987729e-04\n",
      " 1.87777950e+00 5.90939873e-03 6.44731560e-01 9.43521881e-01\n",
      " 6.03133331e-01 5.24583967e-04 1.25108229e+00 6.51112084e-01\n",
      " 2.81483139e-04 4.32083200e-02 1.12165744e-02 6.34650083e-09\n",
      " 4.23188340e-01 3.42411075e-03 5.88136951e-08 4.50401021e-10\n",
      " 1.56834313e-03 6.99849012e-01 1.41525602e-01 8.54772877e-01\n",
      " 3.67379793e-01 2.40052159e+00 3.42905395e-03 5.37655261e-01\n",
      " 1.19914393e-02 1.54030376e-02 3.48358382e-03 6.37743778e-07\n",
      " 1.30989487e+00 2.52847501e-01 1.07424632e+00 2.76656693e+00\n",
      " 5.73933111e-02 2.42772987e-01 1.59863442e-10 5.84735239e-02\n",
      " 4.40424164e-01 1.95554770e-03 3.83095627e-01 5.20409701e-01\n",
      " 2.39171233e-04 1.79255686e-09 3.62526294e-01 8.42605006e-05\n",
      " 3.18263905e-01 2.53237722e-10 5.72418068e-01 8.12769911e-02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1524, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.2861, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.90864995957963\n",
      "test accuracy:  99.14571637783745\n",
      "42\n",
      "[3.00118561e-01 1.43142348e-08 7.40565273e-01 4.02097358e-01\n",
      " 8.09475973e-02 1.24617576e-05 3.76664525e-02 6.05845568e-01\n",
      " 3.81822316e-01 2.12460186e-01 8.20657934e-01 4.99202179e-07\n",
      " 3.38937752e-01 8.91231595e-01 1.69671152e-07 6.46040939e-01\n",
      " 7.77443457e-01 3.48032246e-01 9.86419999e-01 1.36244254e-03\n",
      " 1.63110290e-02 8.79190019e-01 4.94715564e-01 8.55615736e-01\n",
      " 5.23036341e-01 1.34811393e+00 3.96527597e-01 6.91222046e-01\n",
      " 4.27459397e-03 2.80133559e-01 5.39719493e-01 3.75136274e-01\n",
      " 2.78785060e-08 1.50811506e-01 5.72657695e-01 5.03398047e-01\n",
      " 7.23706167e-01 2.13172057e-01 1.08372566e-02 3.66911051e-01\n",
      " 1.32004397e+00 3.34626343e-07 1.87513227e-10 9.52315984e-01\n",
      " 2.34594466e-05 2.65469929e-01 2.65527922e-04 6.59467477e-02\n",
      " 4.09506726e-01 1.61213645e+00 1.18820943e+00 2.93686201e-04\n",
      " 1.92909937e+00 6.22317867e-03 6.36737190e-01 9.41063122e-01\n",
      " 6.33984621e-01 4.33519381e-04 1.31833239e+00 6.48024203e-01\n",
      " 2.43387842e-04 5.14347212e-02 1.34090283e-02 6.07653390e-09\n",
      " 4.32599604e-01 3.68695393e-03 5.82814058e-08 4.48512434e-10\n",
      " 7.53400870e-04 7.12574871e-01 1.49229501e-01 8.63851242e-01\n",
      " 3.62219970e-01 2.57314409e+00 4.03223239e-03 5.26312531e-01\n",
      " 8.75660929e-03 1.14340899e-02 7.12290052e-03 6.53778323e-07\n",
      " 1.33974864e+00 1.06512577e-01 1.08121592e+00 1.76687786e+00\n",
      " 6.93942808e-02 2.97694282e-01 1.57361513e-10 9.80602862e-02\n",
      " 4.39291114e-01 1.88149017e-03 3.62125375e-01 5.35278422e-01\n",
      " 3.05880789e-04 1.65205707e-09 3.23651858e-01 1.56979633e-04\n",
      " 3.20808659e-01 2.44091649e-10 1.12550048e+00 7.72318251e-02]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1242, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.2731, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.94907033144705\n",
      "test accuracy:  99.1701244813278\n",
      "43\n",
      "[3.18019956e-01 1.20877084e-08 7.25215362e-01 4.13909473e-01\n",
      " 6.48879759e-02 1.45222633e-05 4.04373162e-02 6.04022787e-01\n",
      " 3.54894427e-01 2.82408663e-01 8.37110758e-01 4.08823806e-07\n",
      " 3.64880877e-01 8.75093599e-01 2.02776080e-07 6.60181842e-01\n",
      " 7.50409329e-01 3.51648088e-01 9.58751751e-01 1.20739392e-03\n",
      " 2.79322884e-02 8.83919288e-01 4.90617775e-01 8.53644593e-01\n",
      " 5.34803970e-01 1.33071559e+00 3.92345758e-01 6.88262642e-01\n",
      " 3.57924380e-03 3.10633212e-01 7.09872378e-01 3.29884866e-01\n",
      " 5.46989931e-08 1.16698322e-01 5.87484792e-01 5.13644084e-01\n",
      " 6.90940761e-01 2.11190777e-01 1.03322139e-02 3.76093691e-01\n",
      " 1.31590545e+00 3.97255137e-07 2.15577833e-10 1.11388415e+00\n",
      " 2.66372024e-05 2.57309871e-01 4.62516327e-04 6.92181416e-02\n",
      " 4.59987687e-01 1.60931107e+00 1.94970052e+00 3.09027607e-04\n",
      " 1.85705494e+00 6.63163121e-03 6.11188565e-01 9.42170039e-01\n",
      " 5.72579572e-01 3.75799117e-04 1.17469858e+00 9.38801808e-01\n",
      " 3.02644058e-04 4.53583662e-02 8.37824925e-03 6.19227907e-09\n",
      " 4.34091446e-01 3.13980918e-03 6.66769977e-08 5.26370557e-10\n",
      " 6.22107935e-04 6.76429713e-01 1.42891981e-01 8.54128418e-01\n",
      " 3.46493650e-01 2.44535879e+00 2.76203677e-03 5.22897307e-01\n",
      " 1.38576473e-02 2.28359175e-02 5.32557821e-03 8.01647651e-07\n",
      " 1.60892988e+00 1.32016722e-01 1.06849886e+00 1.92535064e+00\n",
      " 3.97739211e-02 2.79092436e-01 1.43373799e-10 8.38935110e-02\n",
      " 4.30647050e-01 1.93429870e-03 3.66809333e-01 5.28860363e-01\n",
      " 2.44177137e-04 1.33656011e-09 3.21632886e-01 1.11119269e-04\n",
      " 3.48760058e-01 2.42096704e-10 5.73702181e-01 6.63735190e-02]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1355, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.2710, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.96928051738077\n",
      "test accuracy:  99.19453258481816\n",
      "44\n",
      "[2.93594495e-01 1.36409172e-08 7.35990571e-01 4.19236985e-01\n",
      " 5.86282532e-02 1.16600579e-05 3.52112622e-02 6.01200487e-01\n",
      " 4.14419362e-01 2.81602474e-01 8.27489451e-01 4.67645090e-07\n",
      " 3.76598612e-01 8.87229635e-01 1.74627238e-07 6.66228852e-01\n",
      " 7.51197899e-01 3.44435948e-01 9.57977296e-01 1.48467167e-03\n",
      " 1.52684469e-02 8.83886813e-01 4.77247460e-01 8.56838026e-01\n",
      " 5.43817771e-01 2.69308479e+00 3.90786174e-01 7.05598688e-01\n",
      " 3.22926837e-03 2.79452117e-01 5.25580957e-01 3.52032923e-01\n",
      " 2.81220276e-08 1.53286040e-01 5.95926820e-01 5.13415735e-01\n",
      " 6.75836074e-01 2.08333982e-01 1.02516526e-02 4.39784058e-01\n",
      " 1.33118909e+00 3.22656767e-07 1.77062628e-10 1.01365171e+00\n",
      " 2.21672131e-05 2.55131668e-01 2.25243029e-04 6.98117707e-02\n",
      " 4.10870269e-01 1.63778154e+00 1.14109143e+00 2.63287312e-04\n",
      " 1.96461222e+00 6.71459398e-03 6.28573721e-01 9.45241535e-01\n",
      " 6.46194260e-01 3.47475008e-04 1.34249199e+00 7.34132196e-01\n",
      " 2.08984124e-04 4.65381032e-02 1.30991969e-02 5.72615665e-09\n",
      " 4.16104181e-01 2.77722192e-03 5.42202197e-08 4.73751717e-10\n",
      " 5.32753793e-04 6.71564602e-01 1.43453740e-01 8.42684969e-01\n",
      " 3.57524617e-01 2.43637594e+00 3.65568543e-03 5.17167534e-01\n",
      " 7.75318124e-03 1.12263055e-02 4.47478598e-03 6.44591078e-07\n",
      " 1.39264781e+00 1.45642692e-01 1.11893716e+00 1.79490906e+00\n",
      " 6.71274544e-02 2.61604031e-01 1.59290872e-10 7.80606728e-02\n",
      " 4.23319170e-01 1.97164946e-03 5.54109430e-01 5.44841565e-01\n",
      " 2.13048010e-04 1.53948155e-09 3.18786329e-01 9.36470363e-05\n",
      " 3.13040368e-01 2.37351069e-10 5.66804933e-01 7.27659639e-02]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1354, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.2555, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  99.02991107518189\n",
      "test accuracy:  99.1701244813278\n",
      "98.98949070331447\n",
      "accuracy is best accuracy\n",
      "[100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 95.23809523809523, 100.0, 100.0, 100.0, 100.0, 100.0, 97.5609756097561, 100.0, 100.0, 100.0, 100.0, 100.0, 97.5609756097561, 100.0, 100.0, 95.1219512195122, 97.61904761904762, 97.5609756097561, 97.5609756097561, 97.61904761904762, 100.0, 100.0, 100.0, 97.61904761904762, 97.61904761904762, 100.0, 97.5609756097561, 100.0, 100.0, 97.5609756097561, 95.23809523809523, 100.0, 100.0, 95.23809523809523, 100.0, 100.0, 95.23809523809523, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 97.61904761904762, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 97.5609756097561, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 97.5609756097561, 100.0, 97.5609756097561, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 97.5609756097561, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 95.1219512195122, 90.2439024390244, 100.0, 92.6829268292683, 100.0]\n",
      "45\n",
      "[3.83784982e-01 1.50664094e-08 7.43844531e-01 4.23733794e-01\n",
      " 1.07196403e-01 9.71449487e-06 3.17877540e-02 6.20456945e-01\n",
      " 3.46393528e-01 2.02750127e-01 8.31773804e-01 5.24397161e-07\n",
      " 3.86602723e-01 1.11280120e+00 1.57416204e-07 6.72323729e-01\n",
      " 7.34095311e-01 3.37468551e-01 9.48296099e-01 1.14444952e-03\n",
      " 1.63179453e-02 8.53134916e-01 4.76518281e-01 1.01236673e+00\n",
      " 5.58071620e-01 1.40239149e+00 3.95946304e-01 6.80574003e-01\n",
      " 3.13463697e-03 2.81369836e-01 5.25850973e-01 3.20185111e-01\n",
      " 2.72570275e-08 1.47535954e-01 6.09754026e-01 5.20245561e-01\n",
      " 6.80010331e-01 2.09006167e-01 1.00650066e-02 6.00158605e-01\n",
      " 1.29048292e+00 1.14083020e-06 1.58279750e-10 9.33537412e-01\n",
      " 1.89995994e-05 7.19506840e-01 7.14391420e-04 6.79377847e-02\n",
      " 3.91287238e-01 1.63887671e+00 1.11804062e+00 2.27069848e-04\n",
      " 1.95673692e+00 6.85703338e-03 6.38868692e-01 9.26093824e-01\n",
      " 6.38660979e-01 3.23679032e-04 1.32620048e+00 6.33166480e-01\n",
      " 2.06274122e-04 4.59098451e-02 1.18975986e-02 5.50478165e-09\n",
      " 4.09842399e-01 2.54210328e-03 4.78493210e-08 3.78240882e-10\n",
      " 4.65926885e-04 6.68795154e-01 1.50637043e-01 8.31118457e-01\n",
      " 3.63188367e-01 2.41531731e+00 3.41812287e-03 5.01321964e-01\n",
      " 8.01004227e-03 1.15557414e-02 3.79001208e-03 5.63210979e-07\n",
      " 1.41122401e+00 2.13752444e-01 1.39687983e+00 1.76033498e+00\n",
      " 6.22280049e-02 2.57874718e-01 1.77746109e-10 7.25273757e-02\n",
      " 4.19306712e-01 1.94856389e-03 3.47344709e-01 5.41321472e-01\n",
      " 1.92331238e-04 1.73336458e-09 3.18555957e-01 7.92841506e-05\n",
      " 3.05322563e-01 2.53573204e-10 5.64864210e-01 7.74269913e-02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1358, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.2883, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  99.05012126111559\n",
      "test accuracy:  99.21894068830852\n",
      "99.02991107518189\n",
      "accuracy is best accuracy\n",
      "[100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 95.23809523809523, 100.0, 100.0, 97.5609756097561, 100.0, 100.0, 97.5609756097561, 100.0, 100.0, 100.0, 100.0, 100.0, 97.5609756097561, 100.0, 100.0, 95.1219512195122, 97.61904761904762, 100.0, 97.5609756097561, 97.61904761904762, 100.0, 100.0, 100.0, 97.61904761904762, 97.61904761904762, 100.0, 97.5609756097561, 100.0, 100.0, 100.0, 95.23809523809523, 100.0, 100.0, 95.23809523809523, 100.0, 100.0, 95.23809523809523, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 97.61904761904762, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 97.5609756097561, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 97.5609756097561, 100.0, 97.5609756097561, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 97.5609756097561, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 97.5609756097561, 90.2439024390244, 100.0, 92.6829268292683, 100.0]\n",
      "46\n",
      "[3.67791687e-01 1.58702828e-08 7.46935572e-01 4.12023495e-01\n",
      " 7.08346114e-02 8.89033862e-06 2.97623550e-02 6.02589824e-01\n",
      " 5.22758155e-01 2.04905731e-01 8.43432094e-01 5.46948512e-07\n",
      " 3.56132925e-01 8.33365424e-01 1.49492680e-07 6.62710101e-01\n",
      " 7.53656674e-01 3.51819012e-01 9.53427475e-01 1.06466788e-03\n",
      " 2.08880245e-02 9.10897175e-01 4.79955459e-01 8.51301192e-01\n",
      " 5.42741846e-01 1.26167642e+00 3.96657878e-01 6.86664413e-01\n",
      " 3.13799608e-03 2.89551437e-01 5.30693198e-01 3.36061820e-01\n",
      " 2.61292081e-08 1.29828507e-01 5.82878169e-01 5.12949032e-01\n",
      " 8.56366814e-01 2.15381326e-01 9.46320400e-03 3.72665154e-01\n",
      " 1.32394586e+00 2.54107548e-07 1.44510737e-10 1.00115162e+00\n",
      " 1.72684486e-05 2.46669613e-01 2.97592981e-04 6.57831353e-02\n",
      " 4.75479599e-01 1.64418644e+00 1.14672249e+00 2.09649049e-04\n",
      " 1.93479600e+00 5.59803391e-03 6.44243073e-01 9.42961057e-01\n",
      " 6.10608951e-01 4.19721537e-04 1.26761530e+00 6.13554840e-01\n",
      " 2.15544784e-04 5.08842555e-02 9.04879710e-03 1.35764569e-08\n",
      " 3.84778169e-01 3.43502577e-03 4.37722298e-08 4.11271128e-10\n",
      " 5.68290937e-04 6.42942568e-01 1.37897473e-01 7.95511197e-01\n",
      " 3.66547356e-01 2.44608587e+00 2.71795439e-03 5.10744054e-01\n",
      " 9.96056713e-03 1.55764228e-02 5.15626420e-03 5.17989344e-07\n",
      " 1.37439503e+00 1.21236160e-01 1.03353437e+00 2.20408296e+00\n",
      " 4.76529794e-02 5.55727433e-01 6.95943330e-10 8.91124265e-02\n",
      " 4.48138451e-01 1.67651427e-03 3.78996228e-01 5.30425281e-01\n",
      " 2.12894886e-04 1.79274680e-09 3.21215545e-01 1.06629714e-04\n",
      " 3.01307712e-01 2.60021407e-10 7.07374441e-01 7.85543955e-02]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1344, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.2631, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.94907033144705\n",
      "test accuracy:  99.1701244813278\n",
      "47\n",
      "[3.01444240e-01 1.87462841e-08 7.61285580e-01 4.16076108e-01\n",
      " 6.99150356e-02 6.87659825e-06 2.52785158e-02 5.82496785e-01\n",
      " 3.34316206e-01 2.03556936e-01 8.46068517e-01 6.61578297e-07\n",
      " 3.59164802e-01 8.75180235e-01 1.26116189e-07 6.68560575e-01\n",
      " 7.15712595e-01 3.53188527e-01 9.43863661e-01 9.95738408e-04\n",
      " 2.17647865e-02 9.15038911e-01 4.63117934e-01 8.34652182e-01\n",
      " 5.39037418e-01 1.36454016e+00 3.84561494e-01 6.78714188e-01\n",
      " 3.03669676e-03 2.82625115e-01 5.27306223e-01 3.15523813e-01\n",
      " 2.56417820e-08 1.24900129e-01 5.86187082e-01 5.15416665e-01\n",
      " 6.71780701e-01 2.10463339e-01 8.92721938e-03 5.68511440e-01\n",
      " 1.26990705e+00 2.07840017e-07 1.23519805e-10 1.04648229e+00\n",
      " 1.37575520e-05 2.44604760e-01 2.90472801e-04 6.41874944e-02\n",
      " 3.84662643e-01 1.64883532e+00 1.90951995e+00 1.78735559e-04\n",
      " 1.94215351e+00 8.64015492e-03 6.64706805e-01 9.73126217e-01\n",
      " 6.08156565e-01 3.27401298e-04 1.25815562e+00 6.14345340e-01\n",
      " 2.12770999e-04 5.18244443e-02 8.50344712e-03 5.36075906e-09\n",
      " 4.01914575e-01 2.66228293e-03 3.53839720e-08 2.84005999e-10\n",
      " 5.42486065e-04 6.57155906e-01 1.37128309e-01 1.13433998e+00\n",
      " 3.78185545e-01 2.43498950e+00 2.55101491e-03 4.88879593e-01\n",
      " 9.44650870e-03 1.64966489e-02 5.14324383e-03 4.10490604e-07\n",
      " 1.38288196e+00 1.61855402e-01 1.05079900e+00 1.88898034e+00\n",
      " 4.48317199e-02 2.70286289e-01 2.20181109e-10 8.87735709e-02\n",
      " 4.06547395e-01 1.55500422e-03 3.82981594e-01 5.30015478e-01\n",
      " 2.39669821e-04 2.17788649e-09 3.16284374e-01 1.04015162e-04\n",
      " 2.93680976e-01 2.96480673e-10 5.45482267e-01 8.78128782e-02]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1531, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.2823, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.98949070331447\n",
      "test accuracy:  99.24334879179888\n",
      "48\n",
      "[8.13712012e-01 1.55663680e-08 7.48995566e-01 4.18659023e-01\n",
      " 1.02885772e-01 7.80607498e-06 2.75775639e-02 5.69964557e-01\n",
      " 3.97625232e-01 2.03822336e-01 8.49878210e-01 5.44380270e-07\n",
      " 3.92247601e-01 8.92371270e-01 1.42990334e-07 6.71132852e-01\n",
      " 7.22884196e-01 3.54198660e-01 9.29212575e-01 9.56036196e-04\n",
      " 2.55313696e-02 9.12847674e-01 7.38003434e-01 7.95007401e-01\n",
      " 5.66288850e-01 1.40181727e+00 3.75740311e-01 1.47643057e+00\n",
      " 2.96271911e-03 2.82525917e-01 5.27484967e-01 3.08611785e-01\n",
      " 2.65796973e-08 1.12114184e-01 5.88513429e-01 5.15742003e-01\n",
      " 6.76246508e-01 2.08163606e-01 9.10099010e-03 3.74412372e-01\n",
      " 1.24263877e+00 2.43429910e-07 1.35967029e-10 9.04439625e-01\n",
      " 1.51261604e-05 2.37673264e-01 3.41427654e-04 6.25345677e-02\n",
      " 3.77896621e-01 1.64409318e+00 1.88162017e+00 1.96386973e-04\n",
      " 1.91461043e+00 5.30061895e-03 6.41949587e-01 8.98153777e-01\n",
      " 5.85363750e-01 3.14051718e-04 1.20874395e+00 6.41940351e-01\n",
      " 2.34131325e-04 4.84569712e-02 7.15161976e-03 5.00844731e-09\n",
      " 4.15888373e-01 2.26179312e-03 4.01986709e-08 3.25612439e-10\n",
      " 5.23552271e-04 6.83818326e-01 1.35745774e-01 8.12522210e-01\n",
      " 3.64333822e-01 2.48274696e+00 2.31815321e-03 4.72183762e-01\n",
      " 1.04781485e-02 1.98486645e-02 3.39235823e-03 4.89135366e-07\n",
      " 1.38962522e+00 1.60699234e-01 1.73126656e+00 1.72601461e+00\n",
      " 3.60993866e-02 2.49145966e-01 7.32846130e-10 7.01179832e-02\n",
      " 4.09164682e-01 1.47940020e-03 3.33905526e-01 5.18964118e-01\n",
      " 1.59554650e-04 1.76214661e-09 3.15834547e-01 6.94704700e-05\n",
      " 2.91939452e-01 2.61013474e-10 5.51232040e-01 7.77521063e-02]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1414, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.2862, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.98949070331447\n",
      "test accuracy:  99.21894068830852\n",
      "49\n",
      "[2.94232135e-01 1.47996936e-08 7.43488539e-01 4.30769861e-01\n",
      " 1.39735619e-01 8.15102714e-06 2.80049334e-02 7.84654722e-01\n",
      " 3.31331692e-01 2.00498509e-01 8.47379331e-01 5.02216848e-07\n",
      " 3.61117188e-01 1.38974914e+00 1.50808263e-07 6.82362047e-01\n",
      " 7.13721589e-01 3.43230533e-01 9.26052659e-01 9.47965381e-04\n",
      " 2.07721865e-02 1.07938039e+00 4.66920851e-01 9.03319588e-01\n",
      " 5.52947950e-01 1.26698026e+00 3.74900764e-01 6.74086830e-01\n",
      " 2.82933831e-03 2.86979621e-01 5.28182730e-01 3.13211304e-01\n",
      " 2.61782925e-08 1.23470818e-01 6.12261061e-01 5.10577425e-01\n",
      " 8.91309264e-01 3.20880737e-01 8.43370185e-03 3.92442951e-01\n",
      " 1.23817254e+00 2.39322006e-07 1.35178771e-10 8.92483205e-01\n",
      " 1.54681567e-05 2.34709459e-01 2.60386471e-04 6.25018758e-02\n",
      " 3.76764238e-01 1.65539276e+00 1.23209203e+00 1.96663081e-04\n",
      " 1.95413032e+00 5.92187778e-03 6.34635873e-01 8.94943939e-01\n",
      " 6.09077045e-01 2.79674983e-04 1.26532260e+00 6.01483223e-01\n",
      " 1.98378679e-04 5.15632098e-02 8.17424650e-03 4.94576118e-09\n",
      " 4.21812363e-01 2.41117657e-03 4.14762642e-08 3.21878024e-10\n",
      " 4.17538866e-04 6.53478157e-01 1.38597075e-01 7.95034222e-01\n",
      " 3.60320223e-01 3.39539521e+00 2.40545981e-03 4.66742445e-01\n",
      " 8.38651577e-03 1.56447311e-02 4.57382430e-03 5.03514815e-07\n",
      " 1.43066553e+00 1.20927996e-01 1.26401490e+00 1.72211306e+00\n",
      " 4.26301530e-02 2.74957569e-01 1.89139898e-10 8.96160110e-02\n",
      " 4.02592042e-01 1.57161158e-03 3.27946516e-01 5.31067654e-01\n",
      " 1.79714057e-04 1.59964161e-09 3.21494568e-01 9.24426202e-05\n",
      " 2.89592019e-01 2.46680412e-10 5.60944409e-01 7.34028482e-02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1444, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.2762, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.98949070331447\n",
      "test accuracy:  99.21894068830852\n",
      "50\n",
      "[2.88928416e-01 1.56085204e-08 7.49046765e-01 4.28614577e-01\n",
      " 7.11244496e-02 7.17229691e-06 2.56168191e-02 5.70513136e-01\n",
      " 3.28831175e-01 2.04550399e-01 8.49471204e-01 5.65067535e-07\n",
      " 3.58529923e-01 8.43929068e-01 1.39937145e-07 6.82489420e-01\n",
      " 7.21752076e-01 3.53579963e-01 9.27114675e-01 8.88513467e-04\n",
      " 1.89818569e-02 8.92898036e-01 4.62722578e-01 7.90975652e-01\n",
      " 5.49303373e-01 1.26803173e+00 3.76290219e-01 6.68927660e-01\n",
      " 2.80582123e-03 3.27684543e-01 6.52191538e-01 3.68601072e-01\n",
      " 2.58825417e-08 1.29290601e-01 6.01104254e-01 5.13391842e-01\n",
      " 6.74020481e-01 2.05731150e-01 8.00768876e-03 4.45578877e-01\n",
      " 1.24096569e+00 2.16057638e-07 1.23927979e-10 1.02052519e+00\n",
      " 1.39792280e-05 2.32558272e-01 2.27223695e-04 6.04857048e-02\n",
      " 3.83708363e-01 5.89911919e+00 1.07712069e+00 1.67799843e-04\n",
      " 1.97965021e+00 5.26818434e-03 6.42464843e-01 9.52613803e-01\n",
      " 6.22437105e-01 3.82530110e-04 1.29382164e+00 6.05073606e-01\n",
      " 1.79734075e-04 4.98585755e-02 6.09291803e-02 4.85736869e-09\n",
      " 3.97664130e-01 2.41294507e-03 3.72156029e-08 2.90984195e-10\n",
      " 4.54417254e-04 6.72446049e-01 1.34716631e-01 7.94055647e-01\n",
      " 3.65304176e-01 3.17173394e+00 2.43808971e-03 4.81827853e-01\n",
      " 7.70060071e-03 1.41809196e-02 5.07731620e-03 4.53101353e-07\n",
      " 1.42425386e+00 1.17311715e-01 1.04148575e+00 1.72950657e+00\n",
      " 4.54487159e-02 2.87185490e-01 2.02544037e-10 9.23098829e-02\n",
      " 3.98332085e-01 1.43356522e-03 3.31250162e-01 5.45621862e-01\n",
      " 1.82238029e-04 1.70942699e-09 3.09542127e-01 9.10909237e-05\n",
      " 2.83407559e-01 2.58228758e-10 5.38610499e-01 7.66334563e-02]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1442, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.2855, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.98949070331447\n",
      "test accuracy:  99.19453258481816\n",
      "51\n",
      "[2.83180071e-01 1.54992418e-08 7.48535076e-01 4.29135076e-01\n",
      " 5.25118389e-02 6.98102928e-06 2.49736609e-02 5.71831189e-01\n",
      " 3.31027673e-01 2.04276764e-01 8.52435171e-01 5.30384003e-07\n",
      " 3.88976665e-01 8.53780962e-01 1.39156343e-07 6.84489741e-01\n",
      " 6.99952237e-01 3.56792139e-01 9.27189792e-01 8.45124596e-04\n",
      " 1.59492084e-02 9.07389366e-01 4.50872039e-01 7.96450152e-01\n",
      " 5.72637995e-01 1.26494367e+00 3.72129580e-01 7.65600356e-01\n",
      " 2.68478136e-03 4.33814261e-01 5.26679463e-01 3.03310770e-01\n",
      " 2.58532341e-08 1.39394608e-01 6.00149583e-01 8.59904994e-01\n",
      " 6.48779363e-01 2.00650300e-01 7.89296461e-03 3.85438933e-01\n",
      " 1.58075736e+00 2.08877359e-07 1.20944241e-10 9.14254426e-01\n",
      " 1.35985591e-05 2.28659605e-01 1.89707460e-04 5.99783206e-02\n",
      " 3.70002706e-01 1.68184375e+00 1.08691203e+00 1.60766920e-04\n",
      " 2.00875882e+00 5.02273866e-03 6.40827629e-01 9.02146937e-01\n",
      " 1.29306978e+00 2.66986292e-04 1.33680046e+00 8.98978624e-01\n",
      " 1.59650661e-04 4.46906839e-02 9.69854952e-03 4.76056753e-09\n",
      " 9.52970241e-01 2.06996506e-03 3.64393650e-08 2.89612417e-10\n",
      " 4.44160851e-04 6.53518577e-01 1.34178223e-01 7.94328689e-01\n",
      " 3.63885489e-01 2.41543798e+00 2.62048616e-03 4.68224866e-01\n",
      " 6.84352809e-03 1.18433576e-02 3.26830508e-03 4.45408801e-07\n",
      " 1.42485218e+00 1.97187605e-01 1.04247017e+00 1.77332543e+00\n",
      " 5.06676771e-02 2.70785400e-01 2.05232109e-10 7.40309281e-02\n",
      " 4.48802837e-01 1.37217092e-03 3.16107713e-01 5.50785933e-01\n",
      " 1.49307228e-04 1.67977630e-09 3.05278987e-01 6.00575287e-05\n",
      " 2.84609271e-01 2.57906279e-10 1.07645322e+00 7.53466980e-02]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1477, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.2840, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.98949070331447\n",
      "test accuracy:  99.24334879179888\n",
      "52\n",
      "[2.91865823e-01 1.88840008e-08 7.66255239e-01 4.41187200e-01\n",
      " 6.07048788e-02 5.23704280e-06 2.05813438e-02 7.78223377e-01\n",
      " 3.32336692e-01 1.99984634e-01 8.58158222e-01 6.71212999e-07\n",
      " 3.75405556e-01 8.98618120e-01 1.13612378e-07 6.95364984e-01\n",
      " 7.53714212e-01 3.42331367e-01 9.30199801e-01 8.44917718e-04\n",
      " 2.07248931e-02 8.56993719e-01 4.54527302e-01 7.94105992e-01\n",
      " 5.66661102e-01 1.27649497e+00 3.73017863e-01 6.72569932e-01\n",
      " 2.57776445e-03 2.95366621e-01 5.18760688e-01 3.02321726e-01\n",
      " 2.55695775e-08 1.22074087e-01 6.23435647e-01 5.11595208e-01\n",
      " 6.42618825e-01 1.97495043e-01 1.21066567e-02 4.03380877e-01\n",
      " 1.24723651e+00 1.61832028e-07 1.00753239e-10 9.12335663e-01\n",
      " 1.04810559e-05 2.28629189e-01 2.49223675e-04 6.06691687e-02\n",
      " 3.68702412e-01 1.67565218e+00 1.08660525e+00 1.39695431e-04\n",
      " 1.97163777e+00 5.82816060e-03 6.67515766e-01 9.28793301e-01\n",
      " 6.11819461e-01 2.41729600e-04 1.27001807e+00 7.45804882e-01\n",
      " 1.78098619e-04 4.63836093e-02 7.68364272e-03 4.93748591e-09\n",
      " 4.02780248e-01 2.47214507e-03 2.82892562e-08 2.26143937e-10\n",
      " 3.52429873e-04 6.60090189e-01 1.43859909e-01 9.96323702e-01\n",
      " 3.79479811e-01 2.43244696e+00 3.95821833e-03 4.64366355e-01\n",
      " 8.55674294e-03 1.57175501e-02 3.64597689e-03 3.34892012e-07\n",
      " 1.45947981e+00 1.37114936e-01 1.04146996e+00 1.74895984e+00\n",
      " 3.88778648e-02 2.81010486e-01 2.42759423e-10 8.32086904e-02\n",
      " 6.67396104e-01 1.58603129e-03 3.53303373e-01 5.36406069e-01\n",
      " 1.54682160e-04 2.13418020e-09 3.89908762e-01 1.24114668e-04\n",
      " 2.85914773e-01 3.05694053e-10 5.44049175e-01 8.68376678e-02]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1535, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.2959, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  99.02991107518189\n",
      "test accuracy:  99.21894068830852\n",
      "53\n",
      "[2.85696582e-01 1.58341064e-08 7.54353201e-01 4.36165854e-01\n",
      " 5.44647268e-02 5.91047514e-06 2.28269701e-02 5.90681917e-01\n",
      " 3.22829775e-01 2.03382025e-01 8.55522368e-01 5.55772096e-07\n",
      " 3.86083402e-01 8.35937934e-01 1.27699498e-07 6.91434718e-01\n",
      " 6.88291140e-01 3.52431815e-01 9.33451232e-01 8.47042927e-04\n",
      " 1.93137787e-02 8.91085153e-01 4.43188190e-01 7.94647799e-01\n",
      " 5.79819078e-01 1.24406378e+00 3.74573679e-01 6.66039546e-01\n",
      " 2.55728316e-03 2.79405507e-01 5.27631382e-01 3.61967967e-01\n",
      " 2.56915365e-08 1.23924786e-01 6.09816006e-01 5.13740723e-01\n",
      " 6.40622901e-01 2.01636837e-01 7.53999034e-03 3.93809722e-01\n",
      " 1.23816076e+00 1.90115628e-07 1.09899145e-10 8.99126335e-01\n",
      " 1.16661190e-05 2.23238166e-01 2.17038367e-04 5.90523590e-02\n",
      " 3.69122740e-01 1.67832697e+00 1.08340805e+00 1.40448665e-04\n",
      " 1.98537597e+00 5.17928160e-03 6.45400089e-01 9.77136130e-01\n",
      " 6.20019212e-01 2.38776736e-04 1.28649845e+00 5.98909433e-01\n",
      " 1.74366152e-04 4.59275111e-02 7.79766564e-03 4.52920058e-09\n",
      " 3.92853837e-01 1.92121653e-03 3.25928147e-08 2.56963089e-10\n",
      " 4.02820388e-04 6.54708929e-01 1.34329833e-01 1.06323581e+00\n",
      " 3.66833578e-01 2.40923266e+00 2.21874418e-03 5.07534549e-01\n",
      " 7.43667454e-03 1.43978495e-02 4.32986409e-03 4.56297410e-07\n",
      " 1.44262251e+00 1.51755295e-01 1.02765540e+00 1.74068664e+00\n",
      " 4.08936719e-02 2.72346017e-01 2.17462478e-10 7.63213615e-02\n",
      " 3.85021471e-01 1.31871600e-03 3.16090134e-01 5.45453870e-01\n",
      " 2.34277347e-04 1.74204883e-09 4.61992941e-01 5.72588676e-05\n",
      " 2.76886017e-01 2.66640598e-10 5.29173329e-01 7.76711468e-02]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1447, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.2951, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.98949070331447\n",
      "test accuracy:  99.21894068830852\n",
      "54\n",
      "[2.93862377e-01 1.33357514e-08 7.41645771e-01 4.33203785e-01\n",
      " 6.12259209e-02 6.91634758e-06 2.53805060e-02 5.61691057e-01\n",
      " 3.26418311e-01 2.02921787e-01 8.63911282e-01 4.53607322e-07\n",
      " 3.74419535e-01 8.37329858e-01 1.47517194e-07 6.90854009e-01\n",
      " 6.95736933e-01 3.58106682e-01 9.17203728e-01 7.70209443e-04\n",
      " 2.34766757e-02 9.06588533e-01 5.29765526e-01 7.67901713e-01\n",
      " 5.73204517e-01 1.26298222e+00 3.65324961e-01 6.77326744e-01\n",
      " 2.58593347e-03 2.80024778e-01 5.21078139e-01 3.00535853e-01\n",
      " 2.60963139e-08 1.10562919e-01 6.03471142e-01 5.14908971e-01\n",
      " 7.45305346e-01 2.56390788e-01 7.18105499e-03 3.89319457e-01\n",
      " 1.66510312e+00 2.20130100e-07 1.21151741e-10 8.75719435e-01\n",
      " 1.31349540e-05 2.19321463e-01 2.68069065e-04 9.81315558e-02\n",
      " 3.61902959e-01 1.67982570e+00 1.05888170e+00 1.46713023e-04\n",
      " 1.97207892e+00 4.64065250e-03 6.24295738e-01 9.33414829e-01\n",
      " 5.95872545e-01 2.38990773e-04 1.22967202e+00 6.09311198e-01\n",
      " 1.85646572e-04 4.73422272e-02 6.42075003e-03 1.10062325e-08\n",
      " 4.09147449e-01 1.98521627e-03 3.72850977e-08 2.92595350e-10\n",
      " 4.14065857e-04 6.51707536e-01 1.31623287e-01 7.79038445e-01\n",
      " 3.54040333e-01 2.38942029e+00 1.96871983e-03 4.47555042e-01\n",
      " 8.86315892e-03 1.88084020e-02 3.45528055e-03 4.82537538e-07\n",
      " 1.43518665e+00 1.35395052e-01 1.73039878e+00 1.71318191e+00\n",
      " 3.19185843e-02 2.82480033e-01 1.92082586e-10 8.55418036e-02\n",
      " 4.11118202e-01 1.19932976e-03 3.09935568e-01 5.29022353e-01\n",
      " 1.46604600e-04 1.40878677e-09 3.02578996e-01 6.34876977e-05\n",
      " 2.76569651e-01 2.30244629e-10 1.06955010e+00 6.83881175e-02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1569, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.2973, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.94907033144705\n",
      "test accuracy:  99.21894068830852\n",
      "55\n",
      "[3.34228208e-01 1.55987745e-08 7.45285327e-01 4.45847880e-01\n",
      " 5.34679584e-02 6.35658990e-06 2.40711605e-02 5.62859137e-01\n",
      " 5.60530298e-01 1.96216895e-01 8.60993579e-01 4.75274424e-07\n",
      " 3.90133824e-01 8.31672819e-01 1.40243153e-07 7.01950625e-01\n",
      " 6.98585947e-01 3.42043146e-01 1.14936940e+00 1.00587553e-03\n",
      " 1.92779297e-02 8.49300729e-01 4.39902820e-01 7.69856984e-01\n",
      " 5.86361844e-01 1.24323388e+00 3.67170361e-01 6.62032835e-01\n",
      " 2.48809578e-03 2.78843602e-01 5.21074821e-01 3.27672053e-01\n",
      " 2.56354752e-08 1.21686657e-01 6.30017684e-01 5.09237553e-01\n",
      " 6.42370718e-01 2.01344285e-01 7.40232946e-03 4.06198741e-01\n",
      " 1.21532820e+00 2.02878141e-07 1.13154319e-10 8.76925671e-01\n",
      " 1.21735735e-05 2.16936988e-01 2.09722488e-04 5.69277630e-02\n",
      " 3.59556780e-01 1.69158276e+00 1.15904881e+00 1.38143406e-04\n",
      " 1.99688113e+00 5.59930462e-03 6.29232713e-01 8.77818361e-01\n",
      " 6.18187239e-01 2.18315264e-04 1.28302175e+00 6.08497163e-01\n",
      " 1.59241374e-04 4.74748910e-02 7.36848152e-03 4.26056357e-09\n",
      " 3.96180500e-01 1.74789170e-03 3.48980280e-08 2.73001607e-10\n",
      " 3.19108740e-04 9.72934078e-01 1.39537699e-01 7.73058841e-01\n",
      " 3.56855668e-01 2.42039099e+00 2.07166089e-03 4.43330078e-01\n",
      " 7.17904439e-03 1.47063390e-02 2.74858462e-03 4.50559283e-07\n",
      " 1.47158172e+00 1.56408146e-01 1.02763168e+00 1.71995004e+00\n",
      " 3.76952498e-02 5.22705935e-01 2.02031794e-10 1.48520435e-01\n",
      " 3.83659119e-01 1.30251610e-03 3.16275152e-01 5.40645774e-01\n",
      " 1.23323581e-04 1.47694637e-09 3.01306557e-01 5.30651558e-05\n",
      " 2.77661961e-01 2.38315187e-10 5.39218483e-01 7.01657190e-02]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1672, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.2884, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.98949070331447\n",
      "test accuracy:  99.21894068830852\n",
      "56\n",
      "[2.81095733e-01 1.20490542e-08 7.33477201e-01 4.35872953e-01\n",
      " 6.39162106e-02 7.28965767e-06 2.67436099e-02 5.60895770e-01\n",
      " 3.17900084e-01 2.03982099e-01 8.64335687e-01 3.98297765e-07\n",
      " 3.72126322e-01 8.33243069e-01 1.61741422e-07 6.94740359e-01\n",
      " 7.13460343e-01 3.66611014e-01 1.14669474e+00 6.92812564e-04\n",
      " 1.93387686e-02 9.40568627e-01 4.37713178e-01 7.76341369e-01\n",
      " 5.73252199e-01 1.22706039e+00 3.67817367e-01 6.60346960e-01\n",
      " 2.41196470e-03 2.82751290e-01 5.23898227e-01 3.01194899e-01\n",
      " 2.67984658e-08 1.20057416e-01 5.97191458e-01 5.20625360e-01\n",
      " 6.33463767e-01 2.02396401e-01 6.94433725e-03 3.83957635e-01\n",
      " 1.21420457e+00 2.36975915e-07 1.26809813e-10 8.78384029e-01\n",
      " 1.35662150e-05 2.12026630e-01 2.14985274e-04 5.52539100e-02\n",
      " 3.61123705e-01 1.69785654e+00 1.27309058e+00 1.43745079e-04\n",
      " 2.00388554e+00 6.83974960e-03 6.09123837e-01 9.16848989e-01\n",
      " 6.17621372e-01 2.30639892e-04 1.28184414e+00 6.01380405e-01\n",
      " 1.55469631e-04 7.63062252e-02 7.12734548e-03 4.12975336e-09\n",
      " 3.87349589e-01 1.88131965e-03 4.00420179e-08 3.17303169e-10\n",
      " 4.12306944e-04 6.42088338e-01 1.29301958e-01 7.72408274e-01\n",
      " 3.44152755e-01 2.39938355e+00 1.99142082e-03 4.42846241e-01\n",
      " 7.25492175e-03 1.49336069e-02 3.39490840e-03 2.10556324e-06\n",
      " 1.43327358e+00 1.31431602e-01 1.01989301e+00 1.72337337e+00\n",
      " 3.64561628e-02 2.80190307e-01 1.83693976e-10 8.74260306e-02\n",
      " 4.14337952e-01 1.08988341e-03 3.15890472e-01 5.44064891e-01\n",
      " 1.41585338e-04 1.22328430e-09 3.02930499e-01 6.34412923e-05\n",
      " 2.69572999e-01 2.11750450e-10 5.17483022e-01 6.20273833e-02]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1487, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.2951, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.98949070331447\n",
      "test accuracy:  99.19453258481816\n",
      "57\n",
      "[7.70189059e-01 1.34060541e-08 7.41829554e-01 9.87172025e-01\n",
      " 6.11252526e-02 6.21463352e-06 2.37371576e-02 5.60633163e-01\n",
      " 3.21643764e-01 2.05331704e-01 8.67825573e-01 1.35112583e-06\n",
      " 3.79257405e-01 8.24185295e-01 1.44597085e-07 6.95958884e-01\n",
      " 6.77657429e-01 3.74215893e-01 9.17134353e-01 6.68912880e-04\n",
      " 2.17778980e-02 9.72617740e-01 4.34465893e-01 7.70100878e-01\n",
      " 5.77706853e-01 1.23472867e+00 3.67745038e-01 6.57879357e-01\n",
      " 2.35251397e-03 2.76738110e-01 5.22200989e-01 2.97789938e-01\n",
      " 2.61675566e-08 1.11723982e-01 5.88461092e-01 5.25942860e-01\n",
      " 6.32142607e-01 1.99063550e-01 6.86483794e-03 3.75469253e-01\n",
      " 1.69661516e+00 2.05646090e-07 1.12899287e-10 8.87342354e-01\n",
      " 1.18164487e-05 2.10187702e-01 2.26428224e-04 5.49919710e-02\n",
      " 3.61421138e-01 1.69573318e+00 1.06879717e+00 4.37087524e-04\n",
      " 1.98772561e+00 3.91565839e-03 6.21906093e-01 8.80046217e-01\n",
      " 6.04710836e-01 2.32944016e-04 1.93888480e+00 5.99918870e-01\n",
      " 1.64855026e-04 4.68911536e-02 6.26873361e-03 4.10660983e-09\n",
      " 3.94230560e-01 1.74289771e-03 3.45641615e-08 2.75431177e-10\n",
      " 4.37398025e-04 6.42323754e-01 1.27312031e-01 7.68152984e-01\n",
      " 3.52336513e-01 3.56948102e+00 3.45077441e-03 4.41943158e-01\n",
      " 7.67754506e-03 1.75165748e-02 3.07283242e-03 4.65633612e-07\n",
      " 1.42474020e+00 1.40420205e-01 1.11967136e+00 2.77244143e+00\n",
      " 3.17065315e-02 2.74768012e-01 2.04116751e-10 8.43998424e-02\n",
      " 3.80944932e-01 1.03199106e-03 3.04989037e-01 5.34209798e-01\n",
      " 1.30495777e-04 1.39189905e-09 2.97899731e-01 5.71761264e-05\n",
      " 2.80102093e-01 2.33002922e-10 5.25094926e-01 6.69558470e-02]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1568, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.2934, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.98949070331447\n",
      "test accuracy:  99.19453258481816\n",
      "58\n",
      "[2.73678994e-01 1.35615550e-08 7.43915274e-01 4.36359465e-01\n",
      " 1.17091630e-01 5.82220644e-06 2.29662624e-02 5.66114227e-01\n",
      " 3.16665056e-01 2.02514056e-01 8.67387412e-01 4.58322970e-07\n",
      " 3.85485889e-01 8.36760204e-01 1.39969531e-07 6.96463255e-01\n",
      " 6.73477097e-01 3.69403131e-01 9.12474535e-01 6.78235485e-04\n",
      " 1.71404985e-02 9.56186954e-01 5.50801825e-01 8.85370562e-01\n",
      " 5.80010320e-01 1.22493705e+00 3.65293027e-01 6.72784114e-01\n",
      " 2.26568382e-03 3.66257273e-01 5.14596178e-01 2.96844795e-01\n",
      " 2.60277414e-08 1.26099099e-01 5.96169417e-01 5.23113209e-01\n",
      " 6.10013426e-01 1.97033291e-01 6.67724662e-03 3.79420382e-01\n",
      " 1.22734509e+00 1.97458194e-07 1.08839979e-10 9.46215555e-01\n",
      " 1.11589412e-05 2.07805029e-01 1.75873030e-04 5.45940254e-02\n",
      " 3.54267516e-01 1.70920879e+00 1.07946038e+00 1.22922142e-04\n",
      " 2.02695702e+00 4.02087432e-03 6.23472051e-01 8.82792187e-01\n",
      " 6.33956812e-01 2.20691913e-04 1.31050557e+00 6.02842738e-01\n",
      " 1.37846895e-04 4.59015260e-02 7.54567345e-03 4.00853888e-09\n",
      " 3.87463604e-01 1.67677473e-03 3.33410684e-08 2.65363300e-10\n",
      " 3.97333871e-04 6.34905601e-01 1.27720681e-01 7.62947007e-01\n",
      " 3.53240491e-01 2.56861687e+00 2.04943413e-03 4.44319317e-01\n",
      " 6.34576589e-03 1.36419378e-02 2.89702148e-03 4.47221653e-07\n",
      " 1.43434179e+00 1.45803649e-01 1.01002639e+00 2.78886697e+00\n",
      " 3.92172625e-02 2.71995618e-01 2.09475811e-10 8.06703836e-02\n",
      " 3.82699350e-01 1.03025710e-03 3.09075546e-01 5.49296495e-01\n",
      " 1.25054166e-04 1.40538343e-09 2.96751433e-01 5.24047387e-05\n",
      " 2.73120636e-01 2.35603259e-10 5.21690715e-01 6.74725695e-02]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1505, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.3068, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  99.00970088924818\n",
      "test accuracy:  99.14571637783745\n",
      "59\n",
      "[2.65833969e-01 1.34192469e-08 7.43928967e-01 1.01059910e+00\n",
      " 4.37363583e-02 5.68952559e-06 2.26723600e-02 5.58098812e-01\n",
      " 3.45511446e-01 1.97838496e-01 8.65654527e-01 4.51357379e-07\n",
      " 4.13122129e-01 8.17099456e-01 1.38993431e-07 7.01980790e-01\n",
      " 6.75130100e-01 3.60506473e-01 9.05789487e-01 9.27076409e-04\n",
      " 1.57157614e-02 9.78195536e-01 4.43702183e-01 7.54424450e-01\n",
      " 5.99484499e-01 1.30142469e+00 3.60719442e-01 6.52672824e-01\n",
      " 2.18812549e-03 3.02607344e-01 5.14916990e-01 2.94395286e-01\n",
      " 2.58490083e-08 1.28943840e-01 6.08856865e-01 5.17308747e-01\n",
      " 6.29546227e-01 1.97363527e-01 7.10118751e-03 3.90175273e-01\n",
      " 1.20560745e+00 1.93503665e-07 1.06600867e-10 9.11008100e-01\n",
      " 1.09498736e-05 2.06508223e-01 1.57611564e-04 5.42219139e-02\n",
      " 3.50044726e-01 1.71159982e+00 1.15333580e+00 1.20036895e-04\n",
      " 2.03784193e+00 4.33866068e-03 6.22816526e-01 8.67967195e-01\n",
      " 6.41486349e-01 2.05198658e-04 1.32319609e+00 6.01610925e-01\n",
      " 1.31294761e-04 4.49608073e-02 7.86954642e-03 3.93987366e-09\n",
      " 3.94432693e-01 1.54387064e-03 3.28403297e-08 2.60134178e-10\n",
      " 3.42229022e-04 6.39940769e-01 1.28433803e-01 7.73928808e-01\n",
      " 3.52957674e-01 2.39722449e+00 2.10665598e-03 4.33247188e-01\n",
      " 5.77249642e-03 1.31166930e-02 3.33388123e-03 4.40728186e-07\n",
      " 1.45380439e+00 1.82066887e-01 1.00954358e+00 1.72030169e+00\n",
      " 4.04918763e-02 2.50439961e-01 2.09050041e-10 6.53108302e-02\n",
      " 4.15104535e-01 1.06213113e-03 2.93299206e-01 5.52852378e-01\n",
      " 1.03434915e-04 1.37549354e-09 3.68809318e-01 3.88644697e-05\n",
      " 2.93419890e-01 2.32638172e-10 5.35023372e-01 6.69182547e-02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1736, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.2939, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  99.02991107518189\n",
      "test accuracy:  99.1701244813278\n",
      "60\n",
      "[2.65070613e-01 1.13697484e-08 7.31529174e-01 1.03164112e+00\n",
      " 4.29313552e-02 6.58590110e-06 2.55977694e-02 5.56879465e-01\n",
      " 3.29599016e-01 1.93043598e-01 8.70223179e-01 3.75935250e-07\n",
      " 6.25142961e-01 8.26397155e-01 1.62231163e-07 7.10026456e-01\n",
      " 6.72920743e-01 7.91265004e-01 9.05152024e-01 7.07967537e-04\n",
      " 1.52285742e-02 8.73840687e-01 1.41728780e+00 7.52806273e-01\n",
      " 5.99442159e-01 1.22061172e+00 3.59498078e-01 6.51037418e-01\n",
      " 2.13890688e-03 2.76618984e-01 5.14254422e-01 2.90813193e-01\n",
      " 2.73477527e-08 1.31566025e-01 6.29821506e-01 5.11530646e-01\n",
      " 6.23565459e-01 1.96936550e-01 6.91139552e-03 4.05464670e-01\n",
      " 1.21821230e+00 2.32909127e-07 1.22723567e-10 8.71360156e-01\n",
      " 1.21715860e-05 2.00878035e-01 1.56796180e-04 5.51098182e-02\n",
      " 3.50298737e-01 1.71733575e+00 1.05457725e+00 1.23384063e-04\n",
      " 2.04364059e+00 4.95024630e-03 6.00846817e-01 9.46690189e-01\n",
      " 6.45095020e-01 3.01581454e-04 1.33041497e+00 5.97040187e-01\n",
      " 2.25012164e-04 4.36090708e-02 8.09589094e-03 3.79295907e-09\n",
      " 3.89043824e-01 1.72659906e-03 3.84774365e-08 3.11800293e-10\n",
      " 2.80138199e-04 6.34198041e-01 1.32976319e-01 7.57755070e-01\n",
      " 3.38771803e-01 2.39476632e+00 2.11088658e-03 4.84664953e-01\n",
      " 5.86046561e-03 1.28079834e-02 2.09371184e-03 2.11286883e-06\n",
      " 1.48492829e+00 1.81877121e-01 1.00539320e+00 1.71534079e+00\n",
      " 4.08819244e-02 2.52362881e-01 1.89471091e-10 6.65486024e-02\n",
      " 3.73858746e-01 1.15841619e-03 2.92980430e-01 5.55079174e-01\n",
      " 1.00375763e-04 1.13741258e-09 2.95385349e-01 3.77244190e-05\n",
      " 2.77084817e-01 2.06056394e-10 1.12016505e+00 5.87321014e-02]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 2 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1567, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.3145, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.96928051738077\n",
      "test accuracy:  99.14571637783745\n",
      "61\n",
      "[2.74173391e-01 1.23052418e-08 7.38949329e-01 4.46369001e-01\n",
      " 4.03975794e-02 5.77379976e-06 2.35210451e-02 5.46436055e-01\n",
      " 3.15072337e-01 1.96661824e-01 8.76473403e-01 4.14345850e-07\n",
      " 4.21315860e-01 8.27209847e-01 1.46672658e-07 7.08708078e-01\n",
      " 1.00293101e+00 3.61224469e-01 8.98567793e-01 6.14330774e-04\n",
      " 1.94412103e-02 9.28429994e-01 4.62201367e-01 7.25221565e-01\n",
      " 6.01282754e-01 1.23262094e+00 3.57088584e-01 7.45894498e-01\n",
      " 2.06278581e-03 2.83831976e-01 5.21616578e-01 2.83039947e-01\n",
      " 2.66936233e-08 1.15588630e-01 6.11857579e-01 5.19053890e-01\n",
      " 6.45138106e-01 2.01171655e-01 6.84864468e-03 3.92243770e-01\n",
      " 1.15524018e+00 2.08827169e-07 1.11714540e-10 8.36617048e-01\n",
      " 1.07891315e-05 1.99376732e-01 2.02344626e-04 5.42897062e-02\n",
      " 3.49924383e-01 1.71187108e+00 1.02805088e+00 1.13308148e-04\n",
      " 2.00617144e+00 4.31155543e-03 6.10963519e-01 8.59720244e-01\n",
      " 6.15923812e-01 1.92522394e-04 1.26297378e+00 5.79340878e-01\n",
      " 1.41496901e-04 4.27391995e-02 6.43780313e-03 3.73148010e-09\n",
      " 3.96485359e-01 1.48157183e-03 3.44053209e-08 2.80645604e-10\n",
      " 5.44385346e-04 6.43472904e-01 1.28469490e-01 7.53555592e-01\n",
      " 3.45054147e-01 2.36093884e+00 1.79592020e-03 4.31942149e-01\n",
      " 7.27163444e-03 1.75611838e-02 1.95637667e-03 4.82727442e-07\n",
      " 1.46344579e+00 1.88946384e-01 1.01812156e+00 1.66912678e+00\n",
      " 3.15629817e-02 2.48874086e-01 2.05497452e-10 6.31899285e-02\n",
      " 3.94995174e-01 1.04659322e-03 2.83972296e-01 5.41150911e-01\n",
      " 9.64931719e-05 1.25246523e-09 3.01876123e-01 3.48006873e-05\n",
      " 3.02975082e-01 2.20636842e-10 5.16351252e-01 6.34948777e-02]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1542, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.3017, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.98949070331447\n",
      "test accuracy:  99.1701244813278\n",
      "62\n",
      "[2.68849151e-01 1.39570035e-08 7.49480642e-01 4.43463141e-01\n",
      " 5.04134296e-02 4.78090392e-06 2.08346139e-02 5.50997156e-01\n",
      " 3.49083049e-01 1.97500149e-01 8.74484484e-01 4.82961466e-07\n",
      " 4.00066138e-01 8.24611687e-01 1.32152276e-07 7.04581774e-01\n",
      " 6.92594066e-01 3.69081137e-01 9.00057096e-01 7.50234479e-04\n",
      " 1.82109397e-02 9.52959897e-01 4.33817020e-01 7.47238914e-01\n",
      " 5.85645352e-01 1.27331994e+00 3.68870278e-01 6.47746507e-01\n",
      " 2.03632332e-03 2.80402595e-01 5.22452154e-01 2.86143604e-01\n",
      " 2.64531543e-08 1.18267912e-01 6.03090466e-01 5.20902520e-01\n",
      " 6.31156851e-01 1.99773016e-01 6.07928508e-03 3.86807533e-01\n",
      " 1.16897303e+00 1.80892297e-07 1.00048608e-10 9.79301001e-01\n",
      " 9.21747733e-06 1.98150563e-01 1.83616389e-04 5.39327819e-02\n",
      " 3.51825726e-01 1.71530569e+00 1.03547333e+00 1.02711079e-04\n",
      " 2.01804676e+00 4.01918340e-03 6.25779273e-01 8.47949428e-01\n",
      " 6.23599942e-01 1.96662461e-04 1.27824706e+00 5.80447056e-01\n",
      " 3.64393037e-04 4.15969175e-02 6.67786485e-03 3.77205378e-09\n",
      " 3.90196883e-01 1.56494321e-03 2.97077553e-08 2.45386697e-10\n",
      " 3.34116102e-04 6.31398638e-01 1.25653994e-01 7.47495226e-01\n",
      " 3.54197735e-01 2.37337822e+00 1.80788389e-03 4.21867170e-01\n",
      " 6.69231218e-03 1.66852169e-02 2.54541543e-03 4.09367588e-07\n",
      " 1.45435432e+00 1.55424533e-01 1.00965112e+00 1.98080278e+00\n",
      " 3.29959836e-02 2.66752953e-01 2.32736538e-10 7.50177336e-02\n",
      " 3.70715114e-01 9.98865762e-04 2.98602418e-01 5.44983242e-01\n",
      " 1.89874516e-04 1.46336631e-09 2.99704998e-01 4.23469782e-05\n",
      " 2.62248070e-01 2.48943227e-10 5.20874659e-01 6.89210499e-02]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1550, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.3165, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.94907033144705\n",
      "test accuracy:  99.12130827434709\n",
      "63\n",
      "[2.80242389e-01 1.34760725e-08 7.46709512e-01 4.49364062e-01\n",
      " 3.98552780e-02 4.81502629e-06 2.09997695e-02 5.48539687e-01\n",
      " 3.12510874e-01 1.94228988e-01 8.80428825e-01 4.64246168e-07\n",
      " 4.23464754e-01 8.22149465e-01 4.98779305e-07 7.10499404e-01\n",
      " 6.84776516e-01 3.59882114e-01 8.99029022e-01 5.94267994e-04\n",
      " 2.23801651e-02 9.22750247e-01 4.28527171e-01 7.31853376e-01\n",
      " 6.01200472e-01 1.22134983e+00 3.71200639e-01 6.46213698e-01\n",
      " 2.00604599e-03 2.80859119e-01 8.78000241e-01 2.89648669e-01\n",
      " 2.67291705e-08 1.05479706e-01 6.14930785e-01 5.17203848e-01\n",
      " 6.35979532e-01 1.98740267e-01 6.38643753e-03 4.49104216e-01\n",
      " 1.49050677e+00 1.85007755e-07 1.03084277e-10 8.60194866e-01\n",
      " 9.28176395e-06 1.95287195e-01 2.27223786e-04 5.36930724e-02\n",
      " 3.48078975e-01 1.71029057e+00 1.03059979e+00 1.00924301e-04\n",
      " 1.98998180e+00 4.23359487e-03 6.20502561e-01 8.46014763e-01\n",
      " 4.31737111e+00 1.83768753e-04 1.22410616e+00 5.80310515e-01\n",
      " 1.53233666e-04 6.52315156e-02 5.64199555e-03 3.69840355e-09\n",
      " 3.93937436e-01 1.44664732e-03 3.03911927e-08 2.51072593e-10\n",
      " 2.95945917e-04 9.49219093e-01 1.27023002e-01 7.60918677e-01\n",
      " 3.50876951e-01 2.36778733e+00 1.64026408e-03 5.02950869e-01\n",
      " 8.00578878e-03 1.68195505e-01 1.93677559e-03 4.23128593e-07\n",
      " 1.47305402e+00 1.87818303e-01 1.01150461e+00 1.80909657e+00\n",
      " 2.68464562e-02 2.50025017e-01 2.30204605e-10 6.28391141e-02\n",
      " 3.73115643e-01 1.02680193e-03 2.80196418e-01 5.33490238e-01\n",
      " 9.54533840e-05 1.40025406e-09 3.01416317e-01 3.32370685e-05\n",
      " 2.69501874e-01 2.43024003e-10 5.14812415e-01 6.68062658e-02]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1612, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.3022, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.98949070331447\n",
      "test accuracy:  99.1701244813278\n",
      "64\n",
      "[2.56682042e-01 1.21481485e-08 7.38502143e-01 4.47639798e-01\n",
      " 3.94366390e-02 5.33958760e-06 2.26048890e-02 5.54863595e-01\n",
      " 3.05594226e-01 1.95686811e-01 8.70984635e-01 4.09694668e-07\n",
      " 4.23576339e-01 1.67956539e+00 1.45521453e-07 7.09206920e-01\n",
      " 6.63612587e-01 3.65245923e-01 9.01363006e-01 6.02579673e-04\n",
      " 1.47945459e-02 9.43977089e-01 4.23806427e-01 7.41460576e-01\n",
      " 5.99012228e-01 1.35386453e+00 3.55407907e-01 6.62285343e-01\n",
      " 1.98500728e-03 2.78320636e-01 5.19605975e-01 2.84000483e-01\n",
      " 2.74646012e-08 1.28195931e-01 6.08019543e-01 5.19508842e-01\n",
      " 6.18613929e-01 1.97881077e-01 6.20623342e-03 3.91744195e-01\n",
      " 1.26097687e+00 2.02102450e-07 1.27223301e-10 1.31868179e+00\n",
      " 1.00258935e-05 1.93162683e-01 1.42409050e-04 5.33098617e-02\n",
      " 4.32825764e-01 6.08678333e+00 1.04768054e+00 1.03841832e-04\n",
      " 2.04928352e+00 3.98080471e-03 6.07846912e-01 8.58519344e-01\n",
      " 6.44829314e-01 1.84627533e-04 1.32427668e+00 5.88674715e-01\n",
      " 1.18044359e-04 3.95904940e-02 7.72600935e-03 3.59590109e-09\n",
      " 4.03988108e-01 1.44804817e-03 3.33753485e-08 2.72945013e-10\n",
      " 3.10283129e-04 6.27720318e-01 1.25067052e-01 7.61236595e-01\n",
      " 3.42938720e-01 2.38505749e+00 1.97913303e-03 4.78774373e-01\n",
      " 5.32362085e-03 1.34793967e-02 1.97132956e-03 4.74058487e-07\n",
      " 1.46400589e+00 1.85496477e-01 1.70587537e+00 1.69899376e+00\n",
      " 3.85757715e-02 2.51344388e-01 2.12471387e-10 6.31168570e-02\n",
      " 3.67428396e-01 1.95712601e-03 2.89714124e-01 5.54463882e-01\n",
      " 9.60725933e-05 1.22706684e-09 3.05909256e-01 3.26303211e-05\n",
      " 4.37137687e-01 2.21170235e-10 1.11101276e+00 6.14452643e-02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1702, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.3054, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  99.02991107518189\n",
      "test accuracy:  99.14571637783745\n",
      "65\n",
      "[2.60787714e-01 1.36643329e-08 7.48506597e-01 4.39979374e-01\n",
      " 4.55815232e-02 4.51103201e-06 2.02509126e-02 5.77412651e-01\n",
      " 2.98907047e-01 2.64264158e-01 8.76010431e-01 4.73040513e-07\n",
      " 4.10377740e-01 7.97553067e-01 1.28008979e-07 7.03355474e-01\n",
      " 1.39498971e+00 3.78991770e-01 9.04516075e-01 5.47636619e-04\n",
      " 1.63990393e-02 1.00092790e+00 4.12646653e-01 8.18208816e-01\n",
      " 5.88396705e-01 1.17599449e+00 3.56842799e-01 6.51189712e-01\n",
      " 2.00549830e-03 2.84932082e-01 5.22967966e-01 2.92165502e-01\n",
      " 2.65200192e-08 1.22634183e-01 5.88039538e-01 5.26697021e-01\n",
      " 6.13372933e-01 2.03388101e-01 5.77131805e-03 3.77899463e-01\n",
      " 1.17683480e+00 1.76006733e-07 9.73550545e-11 8.57919612e-01\n",
      " 8.71315488e-06 1.92962100e-01 1.62166000e-04 7.67536277e-02\n",
      " 3.52937708e-01 1.72116823e+00 1.04554748e+00 9.64907395e-05\n",
      " 2.03385937e+00 3.47460578e-03 6.22166428e-01 9.11046670e-01\n",
      " 6.33053836e-01 1.97490879e-04 1.29780092e+00 5.65612735e-01\n",
      " 1.22057508e-04 3.85707267e-02 7.04329172e-03 3.65344402e-09\n",
      " 3.76523476e-01 1.47218231e-03 1.14211628e-07 2.40233555e-10\n",
      " 3.83658908e-04 6.26945574e-01 2.29119405e-01 7.26086739e-01\n",
      " 3.51838663e-01 2.38574481e+00 1.84316519e-03 4.24749114e-01\n",
      " 6.06753240e-03 1.53126111e-02 2.29043330e-03 4.41526563e-07\n",
      " 1.43754618e+00 1.65423865e-01 9.89756642e-01 1.70188156e+00\n",
      " 3.45785264e-02 2.64982815e-01 2.39916114e-10 6.98007812e-02\n",
      " 6.07570076e-01 8.73982452e-04 2.99393551e-01 5.49654073e-01\n",
      " 1.05300465e-04 1.41953959e-09 3.01918927e-01 3.58975354e-05\n",
      " 2.55838978e-01 2.46711706e-10 5.11465512e-01 6.74757295e-02]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1652, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.3099, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  99.00970088924818\n",
      "test accuracy:  99.24334879179888\n",
      "66\n",
      "[2.60073352e-01 1.33175641e-08 7.46989880e-01 4.44721646e-01\n",
      " 5.10080944e-02 4.58394249e-06 2.05549950e-02 7.69745388e-01\n",
      " 2.99764958e-01 1.96531541e-01 8.79334195e-01 4.77198515e-07\n",
      " 3.98938754e-01 8.04638575e-01 1.29757029e-07 7.08192630e-01\n",
      " 6.55459174e-01 3.69892838e-01 9.06173528e-01 5.75213555e-04\n",
      " 1.88732869e-02 9.56851224e-01 4.30583447e-01 8.73446469e-01\n",
      " 5.78861080e-01 1.25431339e+00 3.53583288e-01 6.62763982e-01\n",
      " 2.00591521e-03 3.72149042e-01 5.21344771e-01 2.85697664e-01\n",
      " 2.63539185e-08 1.24345730e-01 6.02466970e-01 5.26362071e-01\n",
      " 6.19981994e-01 2.02598762e-01 5.55214502e-03 3.86308036e-01\n",
      " 1.18184087e+00 1.78154947e-07 9.74614833e-11 8.45205997e-01\n",
      " 8.72148453e-06 1.91521518e-01 1.62083432e-04 5.06973009e-02\n",
      " 3.52517634e-01 1.72618531e+00 1.03390232e+00 3.01201862e-04\n",
      " 2.03744797e+00 3.61528010e-03 6.19303512e-01 8.60209326e-01\n",
      " 6.35642893e-01 1.84188035e-04 1.30247207e+00 5.77811911e-01\n",
      " 3.47305941e-04 3.94879587e-02 7.15329046e-03 3.58470594e-09\n",
      " 4.02718475e-01 1.78987504e-03 3.11296905e-08 2.42779407e-10\n",
      " 3.35282812e-04 6.20839166e-01 1.22259176e-01 7.97303296e-01\n",
      " 3.49931351e-01 2.37622327e+00 1.83549938e-03 4.17482385e-01\n",
      " 6.18700939e-03 1.51999456e-02 3.45161763e-03 4.13205475e-07\n",
      " 1.45557177e+00 1.48942590e-01 1.14597938e+00 1.70374467e+00\n",
      " 3.47203944e-02 2.71660850e-01 2.32024413e-10 7.61815879e-02\n",
      " 3.58648682e-01 8.73903906e-04 2.98870018e-01 5.51662857e-01\n",
      " 1.12795147e-04 1.36922365e-09 3.00286502e-01 4.00644762e-05\n",
      " 2.53355001e-01 2.95560728e-10 6.47438750e-01 6.60064144e-02]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1661, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.3076, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  99.02991107518189\n",
      "test accuracy:  99.12130827434709\n",
      "67\n",
      "[2.61255443e-01 1.33264599e-08 7.47828929e-01 4.49795754e-01\n",
      " 4.67352020e-02 4.44043699e-06 2.02966993e-02 5.51060603e-01\n",
      " 2.96908814e-01 1.94092086e-01 8.80594048e-01 4.61688898e-07\n",
      " 4.06761722e-01 8.02726746e-01 1.28233651e-07 7.13383780e-01\n",
      " 6.50813602e-01 3.63544365e-01 8.98328679e-01 6.09753514e-04\n",
      " 1.70056932e-02 9.30291928e-01 4.15870513e-01 7.24802376e-01\n",
      " 5.87162659e-01 1.17795808e+00 3.61823358e-01 6.50803266e-01\n",
      " 3.16650137e-03 2.84920877e-01 5.32326482e-01 2.84436646e-01\n",
      " 2.64137334e-08 1.20073209e-01 6.14285476e-01 5.15707301e-01\n",
      " 6.17027730e-01 2.01590442e-01 5.60462890e-03 3.93822269e-01\n",
      " 1.48326466e+00 1.76245033e-07 1.23928867e-10 8.49328865e-01\n",
      " 8.45634710e-06 1.89849160e-01 1.67954348e-04 5.00364677e-02\n",
      " 3.49282025e-01 1.72547467e+00 1.03934848e+00 9.28439587e-05\n",
      " 2.03268422e+00 3.82910948e-03 6.19280328e-01 8.46670232e-01\n",
      " 6.28875141e-01 1.99844666e-04 1.28797754e+00 5.70907858e-01\n",
      " 1.20984660e-04 3.90238126e-02 6.78126935e-03 3.54758911e-09\n",
      " 3.99173455e-01 1.43823871e-03 2.90977109e-08 2.40931122e-10\n",
      " 3.02231616e-04 6.18135988e-01 1.24082782e-01 7.24598695e-01\n",
      " 3.49777789e-01 2.38003761e+00 1.74639805e-03 4.32303335e-01\n",
      " 6.80277277e-03 1.61575785e-02 2.30028029e-03 4.09531807e-07\n",
      " 1.47202086e+00 1.61177452e-01 9.93806754e-01 1.69028604e+00\n",
      " 3.25426413e-02 2.65653165e-01 2.35822680e-10 1.41646672e-01\n",
      " 3.59519656e-01 8.88252357e-04 2.82055274e-01 5.48634511e-01\n",
      " 1.02020391e-04 1.37821596e-09 3.02589972e-01 3.62909662e-05\n",
      " 2.52300593e-01 2.42108555e-10 4.92641681e-01 6.62755700e-02]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1714, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.3101, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.96928051738077\n",
      "test accuracy:  99.21894068830852\n",
      "68\n",
      "[2.57240596e-01 1.22274533e-08 7.41197422e-01 4.54102010e-01\n",
      " 4.74768817e-02 4.78638085e-06 2.15765149e-02 9.80724734e-01\n",
      " 2.93616549e-01 1.92244921e-01 8.79448702e-01 4.18937929e-07\n",
      " 4.05655725e-01 8.19304037e-01 1.38953280e-07 7.17787511e-01\n",
      " 6.46846048e-01 3.60020004e-01 9.02545909e-01 5.43809069e-04\n",
      " 1.58539286e-02 9.16478126e-01 4.11413164e-01 7.24270927e-01\n",
      " 5.84888494e-01 1.17062311e+00 3.53385195e-01 6.53815352e-01\n",
      " 1.93143351e-03 2.91248278e-01 8.82555340e-01 2.86115164e-01\n",
      " 2.68511700e-08 1.23851914e-01 6.22012332e-01 5.14966150e-01\n",
      " 6.15365751e-01 2.02926972e-01 5.52680840e-03 3.99212271e-01\n",
      " 1.18415625e+00 1.92790775e-07 1.02897871e-10 8.49055871e-01\n",
      " 8.92017441e-06 1.86911201e-01 1.53146194e-04 5.08878580e-02\n",
      " 3.53425271e-01 1.72944344e+00 1.06730253e+00 9.31305726e-05\n",
      " 2.04464785e+00 3.99697530e-03 6.07975157e-01 1.05359975e+00\n",
      " 6.38188580e-01 1.67317480e-04 1.30632189e+00 5.60938191e-01\n",
      " 1.18449258e-04 3.88802312e-02 7.16508265e-03 3.43707931e-09\n",
      " 3.76823754e-01 1.59839331e-03 3.15520529e-08 2.61766095e-10\n",
      " 2.86818159e-04 6.15134672e-01 1.25520798e-01 7.20752154e-01\n",
      " 1.36999558e+00 2.40589741e+00 1.79224397e-03 4.16443669e-01\n",
      " 5.95274132e-03 1.48947168e-02 2.33394364e-03 4.54486045e-07\n",
      " 1.48587524e+00 1.58639520e-01 9.87754233e-01 2.09416972e+00\n",
      " 3.54281227e-02 2.66171129e-01 2.23453949e-10 7.24156592e-02\n",
      " 3.50175337e-01 9.10317087e-04 2.90024840e-01 5.52798879e-01\n",
      " 1.01520736e-04 1.24637267e-09 3.02675224e-01 4.01629915e-05\n",
      " 2.52700662e-01 2.26139523e-10 4.91211121e-01 6.18606176e-02]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1721, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.3096, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  99.00970088924818\n",
      "test accuracy:  99.1701244813278\n",
      "69\n",
      "[2.66784725e-01 1.11647879e-08 7.34618372e-01 4.47026231e-01\n",
      " 5.80381989e-02 5.16165880e-06 2.29811311e-02 6.01477320e-01\n",
      " 2.90622522e-01 3.68813948e-01 8.83990883e-01 3.78547232e-07\n",
      " 3.84715406e-01 1.39678670e+00 1.50896050e-07 7.11724460e-01\n",
      " 6.49645367e-01 3.72180896e-01 9.09202457e-01 5.21091344e-04\n",
      " 1.93063338e-02 1.39752514e+00 5.44154909e-01 7.22225375e-01\n",
      " 5.70843084e-01 1.16196380e+00 3.54113239e-01 6.47685888e-01\n",
      " 1.92870598e-03 2.87464785e-01 5.29449989e-01 2.86912064e-01\n",
      " 2.74417916e-08 1.11359918e-01 6.02731364e-01 5.20406538e-01\n",
      " 6.25921341e-01 2.04534693e-01 5.44439560e-03 3.86844386e-01\n",
      " 1.16017954e+00 2.10858411e-07 1.10119760e-10 1.32457250e+00\n",
      " 9.47861310e-06 1.84841546e-01 1.86087830e-04 4.97979016e-02\n",
      " 4.32572721e-01 1.72543752e+00 1.03847303e+00 9.44469517e-05\n",
      " 2.01832520e+00 3.50242820e-03 5.96921992e-01 8.88755122e-01\n",
      " 6.14905961e-01 1.75057670e-04 1.25599073e+00 5.53259726e-01\n",
      " 1.27820408e-04 4.10825507e-02 5.99492222e-03 3.35324998e-09\n",
      " 3.80349374e-01 1.60246486e-03 3.42982670e-08 2.85436813e-10\n",
      " 3.20679808e-04 6.10654541e-01 1.24969653e-01 7.70912236e-01\n",
      " 3.35501599e-01 2.37767516e+00 1.60464509e-03 4.18805301e-01\n",
      " 6.98983709e-03 1.86480443e-02 2.96931152e-03 5.04193166e-07\n",
      " 1.46104421e+00 1.72363687e-01 1.00221777e+00 1.69067453e+00\n",
      " 2.82615766e-02 2.82186104e-01 2.10048104e-10 8.45231461e-02\n",
      " 3.53840264e-01 8.33086806e-04 3.00988211e-01 5.42199787e-01\n",
      " 1.16756753e-04 1.11849880e-09 3.03905000e-01 4.56200453e-05\n",
      " 2.44677026e-01 2.20852794e-10 5.98420438e-01 5.76483347e-02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1662, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.3099, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  99.02991107518189\n",
      "test accuracy:  99.14571637783745\n",
      "70\n",
      "[2.61975310e-01 1.03506905e-08 7.28816206e-01 4.51027106e-01\n",
      " 5.83383025e-02 5.49928876e-06 2.42916936e-02 5.50570285e-01\n",
      " 2.94333870e-01 1.94081145e-01 8.82432464e-01 3.47835951e-07\n",
      " 3.84641209e-01 7.96964442e-01 1.62528777e-07 7.14950111e-01\n",
      " 9.33705770e-01 3.67480746e-01 8.96709199e-01 5.50827930e-04\n",
      " 1.79440764e-02 9.49497859e-01 4.11650157e-01 7.19185488e-01\n",
      " 5.71555651e-01 1.45087508e+00 3.52578292e-01 6.57982609e-01\n",
      " 1.87716524e-03 2.84864363e-01 6.60546843e-01 2.84273690e-01\n",
      " 2.82097641e-08 1.14767921e-01 6.09883511e-01 5.18072644e-01\n",
      " 6.60011090e-01 2.02744414e-01 5.36766552e-03 3.92188400e-01\n",
      " 1.15664616e+00 2.28784432e-07 1.17599014e-10 1.00473226e+00\n",
      " 9.99239754e-06 1.83031333e-01 1.66919263e-04 4.99416785e-02\n",
      " 4.39601942e-01 1.72933221e+00 1.03506327e+00 9.58031330e-05\n",
      " 2.03319388e+00 3.68151715e-03 5.87291924e-01 1.03831332e+00\n",
      " 6.24033347e-01 1.68070451e-04 1.27498505e+00 5.56861888e-01\n",
      " 1.19856337e-04 4.12514366e-02 6.25466527e-03 3.30668431e-09\n",
      " 3.77290330e-01 2.10578122e-03 3.69369661e-08 3.09214875e-10\n",
      " 2.96527126e-04 6.11369007e-01 1.22283782e-01 7.15166152e-01\n",
      " 3.29217338e-01 2.37815124e+00 1.63174924e-03 4.18630833e-01\n",
      " 6.35363969e-03 1.72282787e-02 2.95917515e-03 5.52957722e-07\n",
      " 1.47262302e+00 1.29452301e-01 9.84423557e-01 1.69671410e+00\n",
      " 3.02374066e-02 2.82858902e-01 1.99144020e-10 8.50594767e-02\n",
      " 3.99476434e-01 8.54589985e-04 2.90803503e-01 5.46635887e-01\n",
      " 1.16621512e-04 1.02121485e-09 3.53138381e-01 4.52126849e-05\n",
      " 2.46689266e-01 1.96921174e-10 4.86777302e-01 5.41256586e-02]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1688, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.3077, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  99.00970088924818\n",
      "test accuracy:  99.1701244813278\n",
      "71\n",
      "[2.56453647e-01 1.01752013e-08 7.27528568e-01 4.55079461e-01\n",
      " 6.04050744e-02 5.47103143e-06 2.42008304e-02 5.49399705e-01\n",
      " 2.94963739e-01 1.91491869e-01 8.79608917e-01 3.42690228e-07\n",
      " 3.80332259e-01 8.06632084e-01 1.64848660e-07 7.18716791e-01\n",
      " 6.46574057e-01 3.60424098e-01 9.78934812e-01 7.33767952e-04\n",
      " 1.65622955e-02 9.77638429e-01 4.22494141e-01 7.13187175e-01\n",
      " 5.68170407e-01 1.18363630e+00 3.49218322e-01 6.43572120e-01\n",
      " 1.84478967e-03 3.73110494e-01 5.42651395e-01 2.83252855e-01\n",
      " 2.87423050e-08 1.18274381e-01 6.20857754e-01 5.14761612e-01\n",
      " 6.14388844e-01 2.02936609e-01 5.32491188e-03 3.99050790e-01\n",
      " 1.17535428e+00 2.32451524e-07 1.19471891e-10 9.74647652e-01\n",
      " 9.93075978e-06 1.80936134e-01 1.48304001e-04 5.02942065e-02\n",
      " 3.45373306e-01 1.73175928e+00 1.03449618e+00 9.39575789e-05\n",
      " 2.04688836e+00 3.92920592e-03 5.84604529e-01 8.36740206e-01\n",
      " 6.32537426e-01 1.61072278e-04 1.29361178e+00 8.27763707e-01\n",
      " 1.14432276e-04 4.16801949e-02 6.59135341e-03 3.27351221e-09\n",
      " 3.84406137e-01 1.64119902e-03 3.79494405e-08 3.25773422e-10\n",
      " 2.79397076e-04 6.15039255e-01 1.28810253e-01 7.34533294e-01\n",
      " 3.27169695e-01 2.37133500e+00 9.95580597e-03 4.50134875e-01\n",
      " 5.71877245e-03 1.58234295e-02 3.10612556e-03 5.63604842e-07\n",
      " 1.48708023e+00 1.55675004e-01 9.90509093e-01 1.80736907e+00\n",
      " 3.23007565e-02 2.87805271e-01 2.00344033e-10 8.77670210e-02\n",
      " 3.50452166e-01 1.79860716e-03 3.01943257e-01 5.50620101e-01\n",
      " 1.20823153e-04 1.00551305e-09 3.26472141e-01 4.77301643e-05\n",
      " 2.49641653e-01 6.67994867e-10 5.06019950e-01 5.32173198e-02]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1775, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.3139, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.98949070331447\n",
      "test accuracy:  99.1701244813278\n",
      "72\n",
      "[2.50931331e-01 1.05654925e-08 7.31733909e-01 4.56833372e-01\n",
      " 6.07145422e-02 5.07422180e-06 2.31446000e-02 5.47246496e-01\n",
      " 2.94853541e-01 1.90797057e-01 8.79130885e-01 3.60784539e-07\n",
      " 3.80632088e-01 8.06942346e-01 1.59313495e-07 7.20292578e-01\n",
      " 8.07487110e-01 8.18797357e-01 8.92488488e-01 5.40751599e-04\n",
      " 1.47631357e-02 9.19848098e-01 4.09965203e-01 8.55134575e-01\n",
      " 5.69299092e-01 1.17118593e+00 3.65822852e-01 6.46733076e-01\n",
      " 1.82323928e-03 2.83478133e-01 7.29598959e-01 2.88047062e-01\n",
      " 2.82983294e-08 1.25096929e-01 9.38508095e-01 5.22396589e-01\n",
      " 6.17703978e-01 2.01229287e-01 5.27829619e-03 4.01519644e-01\n",
      " 1.46916648e+00 2.20400886e-07 1.73051462e-10 8.49711146e-01\n",
      " 9.32212660e-06 1.80227811e-01 1.29636342e-04 5.02331429e-02\n",
      " 3.43835320e-01 1.73947658e+00 1.04880843e+00 8.98657696e-05\n",
      " 2.06813701e+00 3.98834315e-03 5.89651220e-01 8.31349473e-01\n",
      " 6.47027626e-01 1.57722885e-04 1.32453170e+00 5.59100665e-01\n",
      " 1.05366860e-04 4.13840415e-02 7.22285489e-03 3.23484470e-09\n",
      " 3.81795723e-01 1.60578131e-03 3.51034969e-08 2.97701571e-10\n",
      " 2.58522507e-04 6.28593104e-01 1.28365642e-01 7.18662053e-01\n",
      " 3.30301926e-01 2.39215918e+00 1.83078448e-03 4.07261008e-01\n",
      " 5.14121298e-03 1.40449492e-02 3.05592376e-03 5.29311501e-07\n",
      " 1.49220776e+00 1.24469823e-01 9.94687082e-01 1.78540520e+00\n",
      " 3.54964936e-02 2.87159283e-01 2.08185358e-10 8.81689892e-02\n",
      " 4.04722250e-01 8.92364072e-04 2.74388261e-01 5.58283975e-01\n",
      " 1.19201206e-04 1.06054369e-09 3.01664695e-01 4.65006827e-05\n",
      " 2.47529527e-01 2.04252837e-10 4.91170437e-01 2.20303141e-01]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1807, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.3209, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.96928051738077\n",
      "test accuracy:  99.1701244813278\n",
      "73\n",
      "[2.47503717e-01 1.07036337e-08 7.32682571e-01 4.63086550e-01\n",
      " 4.85730691e-02 4.95969309e-06 2.26784237e-02 5.61808808e-01\n",
      " 3.26253066e-01 2.68172080e-01 8.78218991e-01 3.65510198e-07\n",
      " 4.04330726e-01 8.21856526e-01 1.53000902e-07 7.26050540e-01\n",
      " 6.44096133e-01 3.52207183e-01 8.93076232e-01 5.21026157e-04\n",
      " 1.35337852e-02 8.94192713e-01 5.09708733e-01 1.17051004e+00\n",
      " 5.84655343e-01 1.30927171e+00 3.54423248e-01 7.34353468e-01\n",
      " 1.77412919e-03 2.83187698e-01 5.24045253e-01 3.26922734e-01\n",
      " 2.81128828e-08 1.30212904e-01 6.36045183e-01 5.12564568e-01\n",
      " 6.10010648e-01 1.99889762e-01 5.23299715e-03 4.10299876e-01\n",
      " 1.37775916e+00 2.13959934e-07 1.12444790e-10 8.35299189e-01\n",
      " 9.10577469e-06 1.79324685e-01 1.18364448e-04 5.15203132e-02\n",
      " 3.42389610e-01 1.74216289e+00 1.02612395e+00 8.82711229e-05\n",
      " 2.08093609e+00 4.36451593e-03 5.91014689e-01 9.70486119e-01\n",
      " 6.56673129e-01 1.51861347e-04 1.34615916e+00 5.59370981e-01\n",
      " 9.98342141e-05 3.82278386e-02 7.87511972e-03 3.22033128e-09\n",
      " 3.79710112e-01 1.37500772e-03 3.42640809e-08 2.95694357e-10\n",
      " 2.27017483e-04 6.18905711e-01 1.28539429e-01 7.24896054e-01\n",
      " 3.31138732e-01 2.37300114e+00 1.86894946e-03 4.09716288e-01\n",
      " 4.76832439e-03 1.27016801e-02 2.32445189e-03 2.01739526e-06\n",
      " 1.51355470e+00 1.53695339e-01 9.95521249e-01 1.67680404e+00\n",
      " 3.85689472e-02 2.68627530e-01 2.13716419e-10 7.48213190e-02\n",
      " 6.23207040e-01 9.60386259e-04 2.75160755e-01 5.62660080e-01\n",
      " 9.69627052e-05 1.07295377e-09 2.99828842e-01 3.51849871e-05\n",
      " 2.47693718e-01 2.06863262e-10 4.90391013e-01 5.55716377e-02]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1754, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.3168, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.98949070331447\n",
      "test accuracy:  99.19453258481816\n",
      "74\n",
      "[2.46853809e-01 1.09101521e-08 7.34006665e-01 4.58339058e-01\n",
      " 5.12624482e-02 4.82855280e-06 2.21017006e-02 5.53572344e-01\n",
      " 2.97345495e-01 1.89691868e-01 8.79439615e-01 3.70621879e-07\n",
      " 3.98809647e-01 8.09967033e-01 1.52673461e-07 1.64885934e+00\n",
      " 9.06395101e-01 8.17724989e-01 8.89825970e-01 5.03161107e-04\n",
      " 1.34524586e-02 9.20016132e-01 4.12556545e-01 7.00113146e-01\n",
      " 1.16237939e+00 1.17275712e+00 3.79070885e-01 6.40439312e-01\n",
      " 1.76875037e-03 2.83716287e-01 5.27137936e-01 2.78190159e-01\n",
      " 2.78331121e-08 1.30415160e-01 6.25547557e-01 8.04321103e-01\n",
      " 6.44361900e-01 2.02686577e-01 5.16860439e-03 6.52712156e-01\n",
      " 1.43656802e+00 2.06846256e-07 1.24962748e-10 8.18655269e-01\n",
      " 8.90943672e-06 1.79096409e-01 1.17358833e-04 5.03599013e-02\n",
      " 3.41054735e-01 1.74410598e+00 1.01761930e+00 8.69710100e-05\n",
      " 2.08349814e+00 4.01246973e-03 5.93140060e-01 8.23586545e-01\n",
      " 6.57514106e-01 1.53529265e-04 1.34733684e+00 5.56775243e-01\n",
      " 1.01164184e-04 3.86203753e-02 7.79327285e-03 3.20677561e-09\n",
      " 3.89272726e-01 1.40213843e-03 3.32678271e-08 2.79347406e-10\n",
      " 2.53975293e-04 6.52112314e-01 1.25055090e-01 7.18999510e-01\n",
      " 3.32540595e-01 3.39316524e+00 1.85964500e-03 4.00765953e-01\n",
      " 4.74307659e-03 1.26763245e-02 3.08145972e-03 5.01439861e-07\n",
      " 1.49845309e+00 1.45384167e-01 1.31887497e+00 1.69174670e+00\n",
      " 3.80978513e-02 2.72189989e-01 2.14535486e-10 7.82834597e-02\n",
      " 3.51899678e-01 8.89866974e-04 2.67900014e-01 5.63250320e-01\n",
      " 9.89579689e-05 1.08632709e-09 3.04973057e-01 3.72004600e-05\n",
      " 2.46906025e-01 2.09325446e-10 5.06465255e-01 5.61505722e-02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1794, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.3136, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.98949070331447\n",
      "test accuracy:  99.1701244813278\n",
      "75\n",
      "[2.45175511e-01 1.17086491e-08 7.40546349e-01 4.60775790e-01\n",
      " 4.80792061e-02 4.29488048e-06 2.04029514e-02 6.68241139e-01\n",
      " 2.95003556e-01 1.88338078e-01 8.80133588e-01 4.09300753e-07\n",
      " 4.04903657e-01 8.16279115e-01 1.37514365e-07 7.24973170e-01\n",
      " 6.46527321e-01 3.56685431e-01 8.92598403e-01 4.98153055e-04\n",
      " 1.31417270e-02 9.19743559e-01 4.11217530e-01 7.06197953e-01\n",
      " 5.88647162e-01 1.52650113e+00 3.45405650e-01 6.57728930e-01\n",
      " 1.72476635e-03 2.87611795e-01 5.27181130e-01 2.74965303e-01\n",
      " 2.75630387e-08 1.31147563e-01 6.29266742e-01 8.22181379e-01\n",
      " 6.27129470e-01 2.00102750e-01 5.16386336e-03 4.04225233e-01\n",
      " 1.13434205e+00 1.89307281e-07 1.00872741e-10 8.33264844e-01\n",
      " 8.09558689e-06 1.78203345e-01 1.14639650e-04 5.07582068e-02\n",
      " 3.40379200e-01 1.74550633e+00 1.01627746e+00 1.35540738e-04\n",
      " 2.08666412e+00 4.10733025e-03 6.01956019e-01 8.59509515e-01\n",
      " 6.60357407e-01 1.50714549e-04 1.35040393e+00 5.56204567e-01\n",
      " 9.66564134e-05 3.84231330e-02 7.86167345e-03 3.22835966e-09\n",
      " 3.85572036e-01 1.48820050e-03 3.03800343e-08 2.59063923e-10\n",
      " 2.47740661e-04 6.19677944e-01 1.26091680e-01 7.17380616e-01\n",
      " 3.38093009e-01 2.36330284e+00 1.87955352e-03 4.03323882e-01\n",
      " 4.67571829e-03 1.26049645e-02 2.81499496e-03 4.49303941e-07\n",
      " 1.76211815e+00 1.54280329e-01 1.08249124e+00 1.66349520e+00\n",
      " 3.84834410e-02 2.67403602e-01 2.31845320e-10 7.50082327e-02\n",
      " 3.50202030e-01 9.06514491e-04 2.68894837e-01 5.64699332e-01\n",
      " 9.29827778e-05 1.20232059e-09 3.00257071e-01 3.49512437e-05\n",
      " 2.47053338e-01 2.27424565e-10 4.90648276e-01 6.05809166e-02]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1979, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.3197, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.96928051738077\n",
      "test accuracy:  99.1701244813278\n",
      "76\n",
      "[2.47182980e-01 1.15027665e-08 7.39502233e-01 4.63142621e-01\n",
      " 4.73275352e-02 4.30008861e-06 2.04842338e-02 5.56006106e-01\n",
      " 2.95751196e-01 1.87841986e-01 8.82164388e-01 4.02767006e-07\n",
      " 4.06739359e-01 8.04494248e-01 1.39062243e-07 7.27288281e-01\n",
      " 6.45800787e-01 3.55455544e-01 8.91041323e-01 4.95633389e-04\n",
      " 1.39334911e-02 9.08780280e-01 4.13194814e-01 7.93589068e-01\n",
      " 5.89178565e-01 1.17368446e+00 3.70002592e-01 8.52223241e-01\n",
      " 1.69647773e-03 2.82101249e-01 5.38156840e-01 2.83321853e-01\n",
      " 2.77354870e-08 1.27808051e-01 6.33663679e-01 5.14921138e-01\n",
      " 6.13971848e-01 1.97834041e-01 5.10229768e-03 4.06570962e-01\n",
      " 1.15615506e+00 1.91671302e-07 3.01832323e-10 8.32863088e-01\n",
      " 8.09640309e-06 1.76926163e-01 1.21684374e-04 5.08617267e-02\n",
      " 3.38585077e-01 1.74530112e+00 1.02146100e+00 1.33352223e-04\n",
      " 2.07973425e+00 4.15924002e-03 5.99797310e-01 8.28757461e-01\n",
      " 6.54537081e-01 1.47437620e-04 1.33742298e+00 5.64290474e-01\n",
      " 9.80973835e-05 3.80827181e-02 7.45368678e-03 3.19411318e-09\n",
      " 3.83971516e-01 1.31929912e-03 3.06130271e-08 9.12797378e-10\n",
      " 2.28112722e-04 6.16915863e-01 1.27170055e-01 7.21727375e-01\n",
      " 3.36589366e-01 2.37049797e+00 1.79280702e-03 4.09013440e-01\n",
      " 4.92629909e-03 1.35251567e-02 2.38281173e-03 4.55152653e-07\n",
      " 1.51083246e+00 1.55936495e-01 9.86290604e-01 1.67354342e+00\n",
      " 3.64534563e-02 2.66777035e-01 2.30577279e-10 7.56545853e-02\n",
      " 3.80680895e-01 9.14529468e-04 2.74554588e-01 5.62190276e-01\n",
      " 9.15935489e-05 1.18156102e-09 3.48516008e-01 3.39087071e-05\n",
      " 2.48154373e-01 2.25307384e-10 4.96477187e-01 5.89760389e-02]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1754, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.3165, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.98949070331447\n",
      "test accuracy:  99.19453258481816\n",
      "77\n",
      "[2.49688116e-01 1.14789462e-08 7.39592598e-01 4.60974573e-01\n",
      " 4.86780033e-02 4.23125265e-06 2.02940633e-02 5.42706981e-01\n",
      " 3.17805342e-01 1.89105977e-01 8.84277943e-01 4.03427182e-07\n",
      " 4.04247185e-01 8.06671625e-01 1.38735350e-07 7.25403421e-01\n",
      " 6.48054948e-01 3.61864021e-01 8.90504717e-01 4.73538279e-04\n",
      " 1.52715031e-02 1.21389622e+00 4.12654323e-01 7.04365776e-01\n",
      " 5.89128344e-01 1.17338420e+00 3.44119955e-01 6.49138787e-01\n",
      " 1.64686247e-03 2.81726735e-01 5.21148132e-01 2.75516631e-01\n",
      " 2.78372268e-08 1.21904063e-01 6.26696875e-01 5.18390164e-01\n",
      " 6.10936835e-01 1.97313121e-01 5.09437296e-03 4.00576933e-01\n",
      " 1.13301878e+00 1.91099619e-07 1.01627484e-10 8.43931790e-01\n",
      " 8.00988486e-06 1.76016160e-01 1.48311696e-04 9.18644325e-02\n",
      " 3.39863892e-01 1.74346532e+00 1.01756837e+00 8.01145866e-05\n",
      " 2.06733478e+00 3.96855886e-03 5.99416465e-01 8.41506920e-01\n",
      " 6.43985180e-01 1.47285445e-04 1.31297815e+00 5.61348674e-01\n",
      " 1.02389050e-04 3.83594581e-02 6.80396961e-03 3.17808631e-09\n",
      " 3.86548132e-01 1.32066068e-03 3.04120241e-08 2.58523938e-10\n",
      " 2.38862535e-04 6.17207464e-01 1.32187374e-01 7.23376436e-01\n",
      " 3.36234752e-01 2.85458317e+00 1.67339653e-03 4.43186481e-01\n",
      " 5.27976019e-03 1.51716651e-02 2.24609653e-03 4.52817565e-07\n",
      " 1.50235158e+00 1.53963459e-01 9.87381623e-01 2.70551457e+00\n",
      " 3.32119075e-02 2.68583161e-01 2.33929473e-10 7.57738130e-02\n",
      " 5.99554481e-01 8.90553996e-04 2.66086827e-01 5.57376697e-01\n",
      " 9.14177509e-05 1.18189010e-09 2.98277855e-01 3.42170825e-05\n",
      " 2.49440254e-01 2.26354116e-10 4.94351629e-01 5.88317472e-02]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1761, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.3202, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.98949070331447\n",
      "test accuracy:  99.1701244813278\n",
      "78\n",
      "[2.47735954e-01 1.09429047e-08 7.35416014e-01 4.61194046e-01\n",
      " 5.17886955e-02 4.45055089e-06 2.10886336e-02 5.48009743e-01\n",
      " 2.96590905e-01 1.89112579e-01 8.83986836e-01 3.79940958e-07\n",
      " 3.98524759e-01 8.14247811e-01 1.46242306e-07 7.25768177e-01\n",
      " 6.46395681e-01 3.61100208e-01 8.94459281e-01 4.65751220e-04\n",
      " 1.48812822e-02 9.35525728e-01 1.07284083e+00 7.11308776e-01\n",
      " 5.85086506e-01 1.46742304e+00 3.42927368e-01 6.40326217e-01\n",
      " 1.62874115e-03 2.79433009e-01 7.24908270e-01 2.76608755e-01\n",
      " 2.82321338e-08 1.22810843e-01 6.26271336e-01 5.26633492e-01\n",
      " 7.42248846e-01 1.96939070e-01 5.04818764e-03 4.00065942e-01\n",
      " 1.13012814e+00 2.01486416e-07 1.06112036e-10 8.21290754e-01\n",
      " 8.29541884e-06 1.74461283e-01 3.98366980e-04 5.10454016e-02\n",
      " 3.37276831e-01 1.74562034e+00 1.01493550e+00 8.06045561e-05\n",
      " 2.07326486e+00 3.93530183e-03 5.92774974e-01 8.93850514e-01\n",
      " 6.47160520e-01 1.45754468e-04 1.31919166e+00 5.60711917e-01\n",
      " 1.00203533e-04 3.87413538e-02 6.88809277e-03 3.13277904e-09\n",
      " 3.85622685e-01 1.34691341e-03 3.19823759e-08 2.72609448e-10\n",
      " 2.37998844e-04 6.18149188e-01 1.25505800e-01 7.20538259e-01\n",
      " 3.31770418e-01 2.36493775e+00 1.68278816e-03 3.98807668e-01\n",
      " 5.07999855e-03 1.48820599e-02 2.38294719e-03 4.83860390e-07\n",
      " 1.50261242e+00 1.44727062e-01 9.88835953e-01 1.69705208e+00\n",
      " 3.35331305e-02 2.72867880e-01 2.24803134e-10 8.06435776e-02\n",
      " 3.50271546e-01 8.84597146e-04 2.67035718e-01 5.59076314e-01\n",
      " 9.50244101e-05 1.11017873e-09 2.97895420e-01 3.61017299e-05\n",
      " 2.46853285e-01 2.16954621e-10 4.97557681e-01 5.62558919e-02]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1793, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.3233, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.96928051738077\n",
      "test accuracy:  99.19453258481816\n",
      "79\n",
      "[2.98369216e-01 1.08890928e-08 7.35499061e-01 4.65679273e-01\n",
      " 4.73990767e-02 4.42936031e-06 8.39954919e-02 6.49200399e-01\n",
      " 7.07858676e-01 1.88161468e-01 8.84308533e-01 3.79207928e-07\n",
      " 4.07933383e-01 8.06002164e-01 1.45889209e-07 7.30066893e-01\n",
      " 6.54620287e-01 3.56637826e-01 8.90132762e-01 4.68613385e-04\n",
      " 1.46954344e-02 9.16673937e-01 4.14740346e-01 7.02543040e-01\n",
      " 5.91401421e-01 1.18535157e+00 3.41798569e-01 6.43061787e-01\n",
      " 1.62277977e-03 2.77794321e-01 5.22662853e-01 2.82563507e-01\n",
      " 2.81985856e-08 1.23547519e-01 6.35112262e-01 5.16940686e-01\n",
      " 6.13890182e-01 1.95994997e-01 5.10051730e-03 4.06927571e-01\n",
      " 1.12675730e+00 2.00548681e-07 1.05463180e-10 8.19802300e-01\n",
      " 8.21539819e-06 1.73965993e-01 1.26225069e-04 5.14262587e-02\n",
      " 3.39745832e-01 1.74644582e+00 1.01567657e+00 7.98336801e-05\n",
      " 2.07488941e+00 4.15626283e-03 5.92540107e-01 9.11258085e-01\n",
      " 6.48164446e-01 2.22276729e-04 1.32147906e+00 5.65127605e-01\n",
      " 9.90117204e-05 3.79560896e-02 6.94918603e-03 3.11490549e-09\n",
      " 3.86755176e-01 1.27616956e-03 3.18227267e-08 2.71157346e-10\n",
      " 2.20557876e-04 6.19285077e-01 1.27552670e-01 7.23480978e-01\n",
      " 3.31721740e-01 2.36450223e+00 1.68291260e-03 3.97589527e-01\n",
      " 5.03877650e-03 1.47554494e-02 2.13172201e-03 4.81270371e-07\n",
      " 1.51670115e+00 1.56892582e-01 9.99508736e-01 2.05674166e+00\n",
      " 3.37211794e-02 2.64686021e-01 2.24979424e-10 7.42245236e-02\n",
      " 3.66436148e-01 9.12831324e-04 2.62607169e-01 5.59746084e-01\n",
      " 8.66644067e-05 1.10497242e-09 3.54614803e-01 3.24948996e-05\n",
      " 4.22017062e-01 2.16361373e-10 4.96338232e-01 5.61825630e-02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1827, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.3285, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.92886014551334\n",
      "test accuracy:  99.07249206736637\n",
      "80\n",
      "[2.44719227e-01 1.22532354e-08 7.31283744e-01 4.64412305e-01\n",
      " 4.58407207e-02 4.68012241e-06 2.19490584e-02 5.43025287e-01\n",
      " 2.95405253e-01 1.88219582e-01 8.83270309e-01 1.06889858e-06\n",
      " 4.11164868e-01 8.13727215e-01 1.54164702e-07 7.29569516e-01\n",
      " 6.79239218e-01 3.58802372e-01 9.57704332e-01 4.60609942e-04\n",
      " 1.42308794e-02 9.23923570e-01 4.17441373e-01 7.00480036e-01\n",
      " 1.18579741e+00 1.17177883e+00 3.45257851e-01 6.47965129e-01\n",
      " 1.60814011e-03 3.46287032e-01 5.20744048e-01 2.69855162e-01\n",
      " 2.85877214e-08 1.24563355e-01 6.32704346e-01 8.24797857e-01\n",
      " 6.17086551e-01 1.97050671e-01 5.08817606e-03 4.04052039e-01\n",
      " 1.12734582e+00 2.12566729e-07 1.10894127e-10 8.52680056e-01\n",
      " 8.59391173e-06 1.73132676e-01 1.20252089e-04 5.08244824e-02\n",
      " 3.84261167e-01 1.74637582e+00 1.32037998e+00 8.16482371e-05\n",
      " 2.08004535e+00 4.00823915e-03 5.85818691e-01 8.27367534e-01\n",
      " 6.51275041e-01 1.41500572e-04 1.32817659e+00 5.62641218e-01\n",
      " 9.75833743e-05 3.76686290e-02 7.05114028e-03 3.07319888e-09\n",
      " 3.88444443e-01 1.24571326e-03 3.35882770e-08 2.86861326e-10\n",
      " 2.25828882e-04 6.16701766e-01 1.26478851e-01 7.23304197e-01\n",
      " 3.27272813e-01 2.36316424e+00 1.71517004e-03 3.98380064e-01\n",
      " 4.79492680e-03 1.42500343e-02 2.04286137e-03 5.14740664e-07\n",
      " 2.10138898e+00 1.61303084e-01 9.88036251e-01 1.66040820e+00\n",
      " 3.43565752e-02 2.63079598e-01 2.15505016e-10 7.22872210e-02\n",
      " 3.88406258e-01 8.82082304e-04 2.63614328e-01 5.60971109e-01\n",
      " 8.41199864e-05 1.03023571e-09 2.96692830e-01 3.10081998e-05\n",
      " 2.46137364e-01 2.05407052e-10 4.98796314e-01 5.36247498e-02]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1823, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.3321, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.92886014551334\n",
      "test accuracy:  99.09690017085673\n",
      "81\n",
      "[2.45067041e-01 9.67296224e-09 7.26361809e-01 4.63828389e-01\n",
      " 4.59513046e-02 4.94327080e-06 2.30699801e-02 5.44253987e-01\n",
      " 3.02065026e-01 1.88608424e-01 8.84543945e-01 3.29710692e-07\n",
      " 4.12122558e-01 8.12387061e-01 1.64479527e-07 7.29446916e-01\n",
      " 6.44694679e-01 3.61167562e-01 8.89281433e-01 4.54296951e-04\n",
      " 1.43308764e-02 9.32839275e-01 1.07494141e+00 7.00077160e-01\n",
      " 5.93147466e-01 1.17361530e+00 3.41770614e-01 6.36333644e-01\n",
      " 2.70771396e-03 2.82502730e-01 5.28549365e-01 2.69678905e-01\n",
      " 2.93654785e-08 1.24098472e-01 6.29802820e-01 5.18270563e-01\n",
      " 6.07663716e-01 1.95554998e-01 5.01308952e-03 4.02626429e-01\n",
      " 1.12665790e+00 2.28282915e-07 1.72466319e-10 9.59522503e-01\n",
      " 9.01218633e-06 1.71865925e-01 1.21041879e-04 5.05237651e-02\n",
      " 3.36651281e-01 1.74774289e+00 1.01428386e+00 8.23567757e-05\n",
      " 2.08126338e+00 3.94301930e-03 5.77987947e-01 8.75807812e-01\n",
      " 1.27400923e+00 1.40608689e-04 1.32733329e+00 5.63879546e-01\n",
      " 9.68854561e-05 3.75543886e-02 6.98301457e-03 3.04180692e-09\n",
      " 3.85638756e-01 1.50034953e-03 3.58090776e-08 3.08342865e-10\n",
      " 2.29517263e-04 6.20911664e-01 1.25379076e-01 9.65881670e-01\n",
      " 3.21932027e-01 2.36738268e+00 1.68964440e-03 5.33910346e-01\n",
      " 4.89734972e-03 1.44178614e-02 2.02952274e-03 5.58769559e-07\n",
      " 1.51021543e+00 1.62043447e-01 9.88304078e-01 1.66529732e+00\n",
      " 3.40661793e-02 2.62494298e-01 2.07128384e-10 7.25295632e-02\n",
      " 4.07936258e-01 8.65019719e-04 2.62818106e-01 5.61175565e-01\n",
      " 8.44478232e-05 9.54125272e-10 2.95834273e-01 3.05763583e-05\n",
      " 2.47475269e-01 1.94721974e-10 4.93752086e-01 5.08032365e-02]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1846, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.3284, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.98949070331447\n",
      "test accuracy:  99.14571637783745\n",
      "82\n",
      "[2.43745310e-01 9.58883314e-09 7.25614026e-01 4.65081761e-01\n",
      " 4.54423778e-02 4.95800865e-06 2.30442737e-02 5.42834109e-01\n",
      " 2.96398914e-01 1.87465821e-01 8.84192012e-01 3.25994769e-07\n",
      " 4.13224731e-01 7.99652208e-01 1.65761827e-07 7.30470981e-01\n",
      " 6.41754097e-01 3.59113319e-01 8.89740980e-01 4.60927730e-04\n",
      " 1.61569121e-02 9.24416434e-01 4.11798791e-01 7.07022340e-01\n",
      " 5.94707633e-01 1.17104609e+00 3.41488505e-01 6.36014117e-01\n",
      " 2.44333974e-03 2.79004773e-01 5.32333327e-01 2.69563958e-01\n",
      " 2.94784640e-08 1.24682670e-01 6.32790747e-01 5.17084090e-01\n",
      " 6.08368956e-01 1.95773982e-01 6.76975866e-03 4.05330034e-01\n",
      " 1.12729608e+00 2.29204135e-07 1.17752100e-10 8.22972178e-01\n",
      " 9.03017588e-06 1.71160662e-01 1.20110921e-04 5.04363694e-02\n",
      " 3.34438504e-01 1.74836495e+00 1.01428217e+00 8.18987852e-05\n",
      " 2.08381320e+00 3.99068905e-03 5.76853063e-01 8.21937205e-01\n",
      " 6.52849434e-01 1.39146502e-04 1.33011771e+00 5.62338671e-01\n",
      " 9.58149219e-05 3.76262431e-02 7.03011231e-03 3.02696778e-09\n",
      " 3.84390959e-01 1.20716785e-03 3.58496283e-08 3.08703576e-10\n",
      " 2.22267975e-04 6.37006777e-01 1.25817004e-01 7.18097621e-01\n",
      " 3.21137100e-01 2.39143301e+00 1.69778608e-03 4.24927361e-01\n",
      " 4.73766607e-03 1.42047565e-02 2.81297619e-03 5.62592914e-07\n",
      " 1.51461436e+00 1.64040580e-01 9.83902276e-01 1.95881049e+00\n",
      " 3.42831865e-02 2.61116928e-01 2.04712455e-10 7.18158529e-02\n",
      " 3.46548917e-01 8.73562888e-04 2.88962863e-01 5.61944873e-01\n",
      " 8.31378734e-05 9.42114865e-10 2.98776659e-01 3.00257908e-05\n",
      " 2.46826023e-01 1.94588373e-10 4.92717762e-01 5.02507525e-02]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1882, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.3287, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.94907033144705\n",
      "test accuracy:  99.14571637783745\n",
      "83\n",
      "[2.45682546e-01 9.21624540e-09 7.22384843e-01 4.61392928e-01\n",
      " 4.45758069e-02 5.17714900e-06 2.37214828e-02 5.57362929e-01\n",
      " 2.90393096e-01 1.89057621e-01 9.00248221e-01 3.10193267e-07\n",
      " 4.15325678e-01 8.03117156e-01 1.72871333e-07 7.26765999e-01\n",
      " 6.34336431e-01 3.65702334e-01 8.98145819e-01 5.40971325e-04\n",
      " 1.48125343e-02 9.56792935e-01 4.08323999e-01 7.03768018e-01\n",
      " 5.95770211e-01 1.18767073e+00 3.45149496e-01 6.40417113e-01\n",
      " 1.60273640e-03 2.81333646e-01 5.18476395e-01 2.75144191e-01\n",
      " 2.98438373e-08 1.21766401e-01 6.22545912e-01 5.19376798e-01\n",
      " 9.25621932e-01 1.96528096e-01 6.70777251e-03 3.97745491e-01\n",
      " 1.13681173e+00 2.37558075e-07 1.35841796e-10 8.32028215e-01\n",
      " 9.34393798e-06 1.70859793e-01 1.27138497e-04 4.94811324e-02\n",
      " 3.33976114e-01 1.74830172e+00 1.02260307e+00 8.32777945e-05\n",
      " 2.07684543e+00 3.67041457e-03 5.72289263e-01 8.26594536e-01\n",
      " 6.47413818e-01 1.41579578e-04 1.31768598e+00 5.60501268e-01\n",
      " 9.77542876e-05 3.76316018e-02 6.85060342e-03 3.03777190e-09\n",
      " 3.79155999e-01 1.18893040e-03 3.72202976e-08 3.19820197e-10\n",
      " 2.43336853e-04 6.09349127e-01 1.22743819e-01 7.11349456e-01\n",
      " 3.18044471e-01 2.37715818e+00 1.63387720e-03 3.98614080e-01\n",
      " 4.99051883e-03 1.50813419e-02 3.01713652e-03 5.89094572e-07\n",
      " 1.49954888e+00 1.66737286e-01 1.01880174e+00 1.68007902e+00\n",
      " 3.25140606e-02 2.58588084e-01 1.97605279e-10 7.10458752e-02\n",
      " 3.43123594e-01 8.17132935e-04 2.65007792e-01 5.59761888e-01\n",
      " 8.08190600e-05 8.93082338e-10 3.19061866e-01 2.92499326e-05\n",
      " 2.59253566e-01 1.86524851e-10 4.87334842e-01 4.84502241e-02]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1841, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.3267, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.96928051738077\n",
      "test accuracy:  99.12130827434709\n",
      "84\n",
      "[2.43937457e-01 9.32965904e-09 7.23677434e-01 4.58317800e-01\n",
      " 4.10097874e-02 5.03634040e-06 2.33336913e-02 5.65709643e-01\n",
      " 2.90656961e-01 1.89443379e-01 8.86155015e-01 3.15726161e-07\n",
      " 4.23322817e-01 1.05059448e+00 1.69658873e-07 7.26990439e-01\n",
      " 6.36246743e-01 3.67837423e-01 9.22757319e-01 6.33642722e-04\n",
      " 1.43072097e-02 9.58570987e-01 4.06973438e-01 7.08436837e-01\n",
      " 6.02234785e-01 1.16454222e+00 4.48194533e-01 6.41552428e-01\n",
      " 1.61803694e-03 2.81762529e-01 5.20954176e-01 2.74982315e-01\n",
      " 2.96883577e-08 1.23537644e-01 6.18007666e-01 5.19070912e-01\n",
      " 5.98003247e-01 1.96520098e-01 5.13492949e-03 3.94753600e-01\n",
      " 1.13686200e+00 2.33381401e-07 1.19395688e-10 8.34893093e-01\n",
      " 9.09998247e-06 1.70415933e-01 1.20169303e-04 4.87065859e-02\n",
      " 3.34308969e-01 1.74961857e+00 1.02470322e+00 8.15944345e-05\n",
      " 2.08219095e+00 3.52795815e-03 5.73707279e-01 8.27121003e-01\n",
      " 6.51119256e-01 1.43184767e-04 1.32577881e+00 5.59727039e-01\n",
      " 9.61835868e-05 3.75077147e-02 6.86686878e-03 2.99899427e-09\n",
      " 3.84358171e-01 1.15293598e-03 3.64611363e-08 3.13925718e-10\n",
      " 2.54351405e-04 6.16078829e-01 1.21160433e-01 7.10751036e-01\n",
      " 3.18938976e-01 2.39353715e+00 1.66309311e-03 3.98592895e-01\n",
      " 4.82131818e-03 1.45378971e-02 1.74241381e-03 5.78115068e-07\n",
      " 1.49245514e+00 1.77881941e-01 1.07005063e+00 1.68014256e+00\n",
      " 3.33827464e-02 2.52128736e-01 2.00860953e-10 6.69106042e-02\n",
      " 3.42044638e-01 7.89435433e-04 2.64736797e-01 5.61494054e-01\n",
      " 9.39353414e-05 9.08931562e-10 3.03324960e-01 2.68512201e-05\n",
      " 2.70490350e-01 1.89212618e-10 4.85602068e-01 4.90441826e-02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1909, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.3292, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.94907033144705\n",
      "test accuracy:  99.19453258481816\n",
      "85\n",
      "[3.09053811e-01 9.39939386e-09 7.24322936e-01 4.58540023e-01\n",
      " 4.14380478e-02 4.93868947e-06 2.29523333e-02 5.61373583e-01\n",
      " 2.89905955e-01 1.89443187e-01 8.85990639e-01 3.19203541e-07\n",
      " 4.22438036e-01 1.91040271e+00 1.67890086e-07 7.25357853e-01\n",
      " 6.29867106e-01 3.67916669e-01 8.92940161e-01 4.38902973e-04\n",
      " 1.41053737e-02 1.01400099e+00 4.04983033e-01 1.16348114e+00\n",
      " 5.99620136e-01 1.15532351e+00 6.61101132e-01 6.38384536e-01\n",
      " 1.62466221e-03 3.37896912e-01 5.20136681e-01 3.17239484e-01\n",
      " 2.96059900e-08 1.24273087e-01 6.18396540e-01 5.18337895e-01\n",
      " 6.69288283e-01 1.95882847e-01 5.04589366e-03 3.95431191e-01\n",
      " 1.14235881e+00 2.30058458e-07 1.17864107e-10 8.38084022e-01\n",
      " 8.95805437e-06 1.69475837e-01 3.51984640e-04 4.83082745e-02\n",
      " 3.36200917e-01 1.75028919e+00 1.02844552e+00 8.03656869e-05\n",
      " 5.78707674e+00 3.50402758e-03 5.74552497e-01 8.30502214e-01\n",
      " 6.53602499e-01 1.42389443e-04 1.32960622e+00 6.25048009e-01\n",
      " 9.47901013e-05 3.71765759e-02 6.93285667e-03 2.98853161e-09\n",
      " 3.90810745e-01 1.15389005e-03 3.58424838e-08 3.08669117e-10\n",
      " 2.53484378e-04 6.06560845e-01 1.20838813e-01 7.07245640e-01\n",
      " 3.19453239e-01 2.44127150e+00 1.67341687e-03 4.00176671e-01\n",
      " 4.72454832e-03 1.43123673e-02 1.77392793e-03 5.69098190e-07\n",
      " 1.49290568e+00 2.21887157e-01 9.73491666e-01 1.68505312e+00\n",
      " 3.37433723e-02 2.54147350e-01 2.03337333e-10 6.74783565e-02\n",
      " 4.02559890e-01 7.76169478e-04 2.67255662e-01 5.62230441e-01\n",
      " 7.69925261e-05 9.18844771e-10 2.96458776e-01 2.68326542e-05\n",
      " 2.42311390e-01 6.60149032e-10 4.91768977e-01 4.93012378e-02]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1847, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.3308, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.94907033144705\n",
      "test accuracy:  99.1701244813278\n",
      "86\n",
      "[2.41914582e-01 9.83350733e-09 7.24833657e-01 4.57994737e-01\n",
      " 4.43293646e-02 4.88522097e-06 2.28267422e-02 5.60824023e-01\n",
      " 2.90758397e-01 1.89717651e-01 8.86537792e-01 3.20596356e-07\n",
      " 4.19294322e-01 7.98425942e-01 1.66582707e-07 7.25535643e-01\n",
      " 7.81768423e-01 8.41314950e-01 1.38553160e+00 4.34755487e-04\n",
      " 1.37735776e-02 9.61165311e-01 4.07857734e-01 6.99809442e-01\n",
      " 5.94630896e-01 1.16335072e+00 3.42561683e-01 6.37894862e-01\n",
      " 1.61744001e-03 2.84867733e-01 5.21777061e-01 2.79562786e-01\n",
      " 2.93926465e-08 1.25761891e-01 6.16921367e-01 5.18651032e-01\n",
      " 6.00231427e-01 1.95625228e-01 4.88768691e-03 3.93918777e-01\n",
      " 1.13301354e+00 2.27109507e-07 3.63293381e-10 8.25654725e-01\n",
      " 8.86876792e-06 1.69290558e-01 1.14784312e-04 4.81305022e-02\n",
      " 3.35188229e-01 1.75266894e+00 1.01965917e+00 7.98121541e-05\n",
      " 2.08974277e+00 3.44416145e-03 5.75191639e-01 8.22569012e-01\n",
      " 6.55896965e-01 2.36301557e-04 1.33595276e+00 5.66625171e-01\n",
      " 9.24996427e-05 3.66663141e-02 7.06480178e-03 2.97694224e-09\n",
      " 3.81740476e-01 1.17475775e-03 3.55461921e-08 3.05182309e-10\n",
      " 2.56281681e-04 6.11304643e-01 1.20356871e-01 7.12184094e-01\n",
      " 3.19887718e-01 2.37411880e+00 1.69198691e-03 3.96355370e-01\n",
      " 4.64106555e-03 1.39538766e-02 1.91149548e-03 5.63198812e-07\n",
      " 1.49117895e+00 1.66357125e-01 9.78414148e-01 2.05230780e+00\n",
      " 3.46221730e-02 2.60818815e-01 2.04229036e-10 7.10210109e-02\n",
      " 3.48099699e-01 7.64754787e-04 2.63098978e-01 5.63981951e-01\n",
      " 8.09024288e-05 9.21415895e-10 2.97001250e-01 2.82228086e-05\n",
      " 2.46085123e-01 1.91876390e-10 4.88027904e-01 4.94568899e-02]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1896, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.3316, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.94907033144705\n",
      "test accuracy:  99.1701244813278\n",
      "87\n",
      "[2.42152630e-01 9.53548802e-09 7.26027658e-01 4.55912867e-01\n",
      " 4.27967875e-02 4.76616567e-06 2.24847323e-02 5.44685755e-01\n",
      " 2.90848127e-01 1.90768679e-01 8.87515782e-01 3.25435553e-07\n",
      " 4.19750743e-01 7.93229818e-01 1.63752565e-07 7.23460724e-01\n",
      " 6.36439415e-01 3.73285933e-01 8.89083204e-01 5.39945860e-04\n",
      " 1.38734384e-02 9.78584598e-01 4.17913337e-01 7.05529387e-01\n",
      " 5.97359222e-01 1.18856854e+00 3.41806408e-01 6.36093345e-01\n",
      " 1.61407650e-03 4.37960051e-01 5.36972908e-01 2.69142887e-01\n",
      " 2.92485264e-08 1.25267175e-01 6.10990301e-01 5.21145785e-01\n",
      " 7.74474865e-01 1.95000975e-01 4.92249819e-03 3.89677603e-01\n",
      " 1.13655581e+00 2.23099960e-07 1.14510304e-10 8.29812458e-01\n",
      " 8.67962503e-06 1.68790514e-01 1.15110052e-04 5.32047588e-02\n",
      " 3.34171988e-01 1.75440881e+00 1.02105056e+00 7.85283729e-05\n",
      " 2.09096993e+00 3.29961330e-03 5.76569128e-01 8.24345176e-01\n",
      " 6.55546253e-01 1.45050958e-04 1.33533990e+00 5.61974978e-01\n",
      " 9.27227072e-05 3.65322892e-02 6.99680403e-03 2.96215939e-09\n",
      " 3.79979103e-01 1.15664645e-03 3.56726752e-08 2.99912039e-10\n",
      " 2.68742600e-04 6.11206838e-01 1.19216693e-01 7.12773790e-01\n",
      " 3.20805042e-01 3.40379055e+00 1.67083420e-03 3.96999111e-01\n",
      " 4.66270028e-03 1.41251726e-02 1.83032574e-03 5.51898622e-07\n",
      " 1.48338187e+00 1.70716066e-01 9.75746445e-01 1.79172935e+00\n",
      " 3.39340926e-02 2.58696072e-01 2.07043368e-10 6.91834867e-02\n",
      " 3.43208811e-01 7.41985761e-04 2.62629544e-01 5.64147068e-01\n",
      " 7.84105728e-05 9.35335372e-10 3.91527638e-01 2.70450790e-05\n",
      " 2.43842744e-01 2.03107517e-10 4.88547773e-01 4.99681302e-02]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.2005, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.3343, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.96928051738077\n",
      "test accuracy:  99.1701244813278\n",
      "88\n",
      "[2.40168714e-01 9.52340946e-09 7.25945985e-01 4.57339260e-01\n",
      " 4.19425877e-02 4.73909110e-06 2.23551394e-02 5.47496361e-01\n",
      " 2.86969344e-01 1.90186765e-01 8.86897379e-01 3.25109467e-07\n",
      " 4.21356716e-01 7.87976141e-01 1.63678170e-07 7.25132098e-01\n",
      " 6.29321369e-01 3.71894778e-01 8.89982347e-01 4.30828220e-04\n",
      " 1.32272728e-02 9.73166628e-01 4.03269746e-01 7.05724266e-01\n",
      " 5.98141140e-01 1.17754581e+00 3.42644405e-01 6.38792209e-01\n",
      " 1.59682832e-03 2.78067729e-01 5.22572340e-01 2.89507957e-01\n",
      " 2.95715280e-08 1.27954427e-01 6.13833866e-01 5.20734564e-01\n",
      " 6.21686923e-01 1.96969424e-01 4.89701090e-03 3.91756052e-01\n",
      " 1.13756138e+00 2.22106485e-07 1.14018420e-10 8.39320966e-01\n",
      " 8.63179282e-06 1.68354649e-01 1.09460449e-04 4.79593425e-02\n",
      " 4.28817508e-01 6.21902408e+00 1.02578370e+00 7.79807409e-05\n",
      " 2.09767297e+00 3.35633676e-03 5.76407247e-01 8.56940532e-01\n",
      " 6.60542000e-01 1.41650903e-04 1.34679964e+00 5.58070245e-01\n",
      " 8.96802870e-05 3.61948966e-02 7.28620779e-03 2.95516017e-09\n",
      " 3.75501951e-01 1.15350763e-03 3.46772785e-08 2.97985844e-10\n",
      " 2.61883022e-04 6.04488575e-01 1.19516898e-01 7.93633814e-01\n",
      " 3.20746386e-01 3.40803475e+00 1.71952058e-03 4.67941566e-01\n",
      " 4.46198784e-03 1.34366325e-02 2.37604180e-03 5.49905345e-07\n",
      " 1.48759681e+00 1.72346474e-01 1.11193806e+00 1.68098525e+00\n",
      " 3.52830975e-02 2.59007255e-01 2.07201062e-10 6.83023969e-02\n",
      " 3.97517294e-01 7.45618375e-04 2.64407562e-01 5.66297797e-01\n",
      " 1.32235470e-04 9.34311260e-10 2.97229495e-01 2.64218784e-05\n",
      " 2.41394587e-01 1.94281563e-10 4.82255066e-01 4.98562396e-02]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1895, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.3343, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.94907033144705\n",
      "test accuracy:  99.14571637783745\n",
      "89\n",
      "[2.41433977e-01 9.71262484e-09 7.27832130e-01 4.57825941e-01\n",
      " 4.21097628e-02 4.56898498e-06 2.18026718e-02 5.47466564e-01\n",
      " 2.85708698e-01 1.90188507e-01 8.88067412e-01 3.34184453e-07\n",
      " 4.20905102e-01 7.87682801e-01 1.59440158e-07 7.25522332e-01\n",
      " 6.44100288e-01 3.71842968e-01 8.89748997e-01 5.13799776e-04\n",
      " 1.36210638e-02 9.72590166e-01 4.17007334e-01 7.06495265e-01\n",
      " 5.98396653e-01 1.15279311e+00 3.42434384e-01 6.38501151e-01\n",
      " 1.59382078e-03 2.79141754e-01 5.24703310e-01 2.78431961e-01\n",
      " 2.92068218e-08 1.26260487e-01 6.14512320e-01 5.20726252e-01\n",
      " 5.94294300e-01 2.11920142e-01 4.89680133e-03 3.92328088e-01\n",
      " 1.14035644e+00 2.16617653e-07 1.11657156e-10 8.54410510e-01\n",
      " 8.39014641e-06 1.67806367e-01 1.13018372e-04 4.78857926e-02\n",
      " 3.32767887e-01 1.75510291e+00 1.02495184e+00 2.50307413e-04\n",
      " 2.09390625e+00 3.55092108e-03 5.78740871e-01 8.25495657e-01\n",
      " 6.57174149e-01 1.40817275e-04 6.80432899e+00 5.55569811e-01\n",
      " 1.11229038e-04 3.61105285e-02 7.08816673e-03 2.94325254e-09\n",
      " 3.75110592e-01 1.38880143e-03 3.37371636e-08 2.90385840e-10\n",
      " 2.59519273e-04 8.97097653e-01 1.19544645e-01 8.78625157e-01\n",
      " 3.22226172e-01 3.00002779e+00 1.67872874e-03 4.17857044e-01\n",
      " 4.58215421e-03 1.37882261e-02 1.94179692e-03 5.32940263e-07\n",
      " 1.48886237e+00 1.71408298e-01 9.70064363e-01 1.68424281e+00\n",
      " 3.49618291e-02 2.58977365e-01 2.12140818e-10 6.84670140e-02\n",
      " 3.37122516e-01 7.42625227e-04 2.64412905e-01 5.64768087e-01\n",
      " 7.71515094e-05 9.61467607e-10 2.97059818e-01 2.63959505e-05\n",
      " 3.10244064e-01 1.99020980e-10 4.80920507e-01 5.07734830e-02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1874, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.3337, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.92886014551334\n",
      "test accuracy:  99.19453258481816\n",
      "90\n",
      "[2.40509511e-01 9.93189760e-09 7.29862354e-01 4.61542010e-01\n",
      " 4.26436109e-02 4.41440182e-06 2.12091570e-02 7.61667801e-01\n",
      " 2.86541398e-01 1.88378610e-01 8.87564387e-01 3.43320043e-07\n",
      " 4.20009460e-01 7.87012444e-01 1.54870797e-07 7.28601363e-01\n",
      " 6.69554593e-01 3.66285803e-01 8.87822784e-01 4.27949780e-04\n",
      " 1.34395531e-02 9.49513038e-01 4.02194872e-01 7.01110073e-01\n",
      " 5.97202368e-01 1.15063899e+00 3.41970628e-01 7.49670085e-01\n",
      " 1.58215077e-03 4.42092041e-01 5.19846103e-01 3.16072225e-01\n",
      " 2.89176191e-08 1.26602303e-01 6.23305498e-01 5.20891594e-01\n",
      " 6.00939999e-01 1.98467428e-01 4.84746809e-03 3.98917719e-01\n",
      " 1.13347235e+00 2.09177646e-07 1.08334813e-10 8.41257021e-01\n",
      " 8.12865279e-06 1.67475358e-01 1.10244233e-04 4.78913869e-02\n",
      " 3.33549507e-01 1.75558074e+00 1.02519261e+00 7.49161612e-05\n",
      " 2.09707938e+00 3.55765927e-03 5.81629525e-01 8.27775499e-01\n",
      " 6.58941686e-01 1.38088582e-04 1.34382920e+00 5.56330262e-01\n",
      " 8.98434803e-05 3.59112619e-02 7.14840592e-03 7.58083742e-09\n",
      " 3.82414311e-01 1.14856408e-03 3.25876751e-08 2.80118220e-10\n",
      " 2.40091109e-04 6.03314166e-01 1.21316173e-01 7.17661988e-01\n",
      " 3.24112210e-01 2.37911319e+00 1.68810697e-03 4.47801912e-01\n",
      " 3.20028941e-02 1.35957377e-02 1.82362453e-03 5.13634276e-07\n",
      " 1.50136512e+00 1.69811363e-01 9.69663031e-01 1.68072584e+00\n",
      " 3.46630282e-02 2.60098079e-01 2.16952123e-10 6.92565945e-02\n",
      " 4.87387071e-01 7.66147967e-04 2.72946151e-01 5.65594747e-01\n",
      " 7.96663493e-05 9.88162544e-10 3.00024267e-01 2.65474647e-05\n",
      " 2.40634479e-01 2.03812911e-10 4.81877075e-01 5.17328586e-02]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1963, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.3401, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.94907033144705\n",
      "test accuracy:  99.12130827434709\n",
      "91\n",
      "[2.41557071e-01 1.00536089e-08 7.31016795e-01 4.61276653e-01\n",
      " 4.12365607e-02 4.33351976e-06 2.09202660e-02 5.53312749e-01\n",
      " 4.99930647e-01 1.88694899e-01 8.88364280e-01 3.48293570e-07\n",
      " 4.23236005e-01 7.85560452e-01 1.52271669e-07 7.28488733e-01\n",
      " 6.56336362e-01 3.67382567e-01 8.88217405e-01 4.33246601e-04\n",
      " 1.38809435e-02 1.00949458e+00 4.01620912e-01 7.02014091e-01\n",
      " 5.99516812e-01 1.15202050e+00 3.41865080e-01 6.36979527e-01\n",
      " 1.58149009e-03 2.78330798e-01 5.19389289e-01 2.71638747e-01\n",
      " 2.86327274e-08 1.24342473e-01 6.22185750e-01 5.17346858e-01\n",
      " 6.04827896e-01 3.28584970e-01 4.89474722e-03 3.98747994e-01\n",
      " 1.13564551e+00 2.05162088e-07 1.06490622e-10 8.36373197e-01\n",
      " 7.98878479e-06 1.67283256e-01 1.13824523e-04 4.76764517e-02\n",
      " 3.44328513e-01 1.75453698e+00 1.02512601e+00 7.41804786e-05\n",
      " 2.09255486e+00 3.51354226e-03 5.83239948e-01 8.25426094e-01\n",
      " 6.55075198e-01 1.36306067e-04 1.33453614e+00 1.23574164e+00\n",
      " 9.29757078e-05 3.59308449e-02 6.91685266e-03 2.93040121e-09\n",
      " 3.74951663e-01 1.13151850e-03 3.20051512e-08 2.74823913e-10\n",
      " 2.51330505e-04 6.07180431e-01 1.20803088e-01 7.02836231e-01\n",
      " 3.25094413e-01 2.38553551e+00 1.64480065e-03 3.96329481e-01\n",
      " 4.60799346e-03 1.41116042e-02 2.35969864e-03 5.03919201e-07\n",
      " 1.50042016e+00 1.74679352e-01 9.69327651e-01 1.68666538e+00\n",
      " 3.35630099e-02 2.57670196e-01 2.19456217e-10 6.74841768e-02\n",
      " 3.85972968e-01 7.57440233e-04 2.64442712e-01 5.63655883e-01\n",
      " 7.54460066e-05 1.00220104e-09 2.94394775e-01 2.59416341e-05\n",
      " 2.40261804e-01 2.05860911e-10 4.83608671e-01 5.22369116e-02]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.2027, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.3366, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.92886014551334\n",
      "test accuracy:  99.19453258481816\n",
      "92\n",
      "[2.42270277e-01 1.02559228e-08 7.32847936e-01 4.63779050e-01\n",
      " 4.04833756e-02 4.20222354e-06 2.04029140e-02 5.62723064e-01\n",
      " 2.84960353e-01 2.61765599e-01 8.89492948e-01 3.56951633e-07\n",
      " 4.24723849e-01 8.51775229e-01 1.48407682e-07 7.30808941e-01\n",
      " 6.33973903e-01 3.64291957e-01 8.89167854e-01 4.24803620e-04\n",
      " 1.41726771e-02 9.41219384e-01 5.37554098e-01 7.10908559e-01\n",
      " 5.99704615e-01 1.15559586e+00 3.42613085e-01 6.40186357e-01\n",
      " 1.56677298e-03 2.77941212e-01 5.19380128e-01 2.72949069e-01\n",
      " 2.84355688e-08 1.23117579e-01 6.27625911e-01 5.16498137e-01\n",
      " 5.85246417e-01 1.97753956e-01 4.88801402e-03 4.02246979e-01\n",
      " 1.47183443e+00 1.99144670e-07 1.04189504e-10 8.59636635e-01\n",
      " 7.79107311e-06 1.67207787e-01 1.16137957e-04 5.25302230e-02\n",
      " 3.33268145e-01 1.75507166e+00 1.03016250e+00 7.31877532e-05\n",
      " 2.09084309e+00 3.63762397e-03 5.85890861e-01 8.28719722e-01\n",
      " 6.53200774e-01 1.34781938e-04 1.32985720e+00 5.63042774e-01\n",
      " 9.17524301e-05 3.57368212e-02 6.77437746e-03 2.93102256e-09\n",
      " 3.72447887e-01 1.12264723e-03 3.10599568e-08 2.66407673e-10\n",
      " 2.40896156e-04 6.00872985e-01 1.22084409e-01 6.99908050e-01\n",
      " 3.26839646e-01 2.38657494e+00 1.61589235e-03 3.98337733e-01\n",
      " 4.69399203e-03 1.44731478e-02 1.71873376e-03 4.87255411e-07\n",
      " 1.50819700e+00 1.78306625e-01 1.00812188e+00 1.69119891e+00\n",
      " 3.28671435e-02 2.57068621e-01 2.23691926e-10 6.69097607e-02\n",
      " 3.33904283e-01 7.72741580e-04 2.64876500e-01 5.62974957e-01\n",
      " 7.47837776e-05 1.02753298e-09 2.99341583e-01 2.50978305e-05\n",
      " 2.39461486e-01 2.10101644e-10 1.00076581e+00 5.31614747e-02]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1876, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.3357, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.94907033144705\n",
      "test accuracy:  99.1701244813278\n",
      "93\n",
      "[2.42649618e-01 1.02517893e-08 7.32913231e-01 4.65195850e-01\n",
      " 4.12777131e-02 4.16484861e-06 2.02689494e-02 5.47557519e-01\n",
      " 2.84549754e-01 1.87193939e-01 8.90469501e-01 3.58048200e-07\n",
      " 4.22843962e-01 7.84271753e-01 1.48093151e-07 7.32037527e-01\n",
      " 6.27503127e-01 3.62387545e-01 1.38603876e+00 4.43410188e-04\n",
      " 1.44646418e-02 9.87420506e-01 4.00454258e-01 7.05091596e-01\n",
      " 5.99218475e-01 1.14828969e+00 3.41151783e-01 6.37045641e-01\n",
      " 1.56325949e-03 2.79641848e-01 5.66971447e-01 2.75112523e-01\n",
      " 2.85867488e-08 1.22842501e-01 6.30766173e-01 5.15370901e-01\n",
      " 8.78641131e-01 1.96893979e-01 4.85086509e-03 4.50870742e-01\n",
      " 1.13513178e+00 1.99265569e-07 1.04045106e-10 8.38582477e-01\n",
      " 7.73301447e-06 1.66451255e-01 1.17420678e-04 5.24165045e-02\n",
      " 4.10204555e-01 1.75649642e+00 1.02563644e+00 7.24270954e-05\n",
      " 2.09099753e+00 3.70729512e-03 7.21334747e-01 8.25214658e-01\n",
      " 6.52703708e-01 1.32640813e-04 1.32846850e+00 5.56643063e-01\n",
      " 9.16058080e-05 3.57558458e-02 6.71817391e-03 2.92090398e-09\n",
      " 3.90629194e-01 1.28679466e-03 3.09249468e-08 2.66255670e-10\n",
      " 2.24972978e-04 6.01881351e-01 1.29549457e-01 7.06234435e-01\n",
      " 3.26578436e-01 2.38197161e+00 1.60040746e-03 4.41085982e-01\n",
      " 7.09545692e-03 1.46739287e-02 1.75147757e-03 4.86548807e-07\n",
      " 1.51286080e+00 1.73793502e-01 9.72121565e-01 2.10080137e+00\n",
      " 3.25126428e-02 2.58206992e-01 2.25306842e-10 6.90963946e-02\n",
      " 3.35618626e-01 7.80155004e-04 2.98396780e-01 5.63085320e-01\n",
      " 7.51977075e-05 1.03089511e-09 2.94356964e-01 2.54767464e-05\n",
      " 6.63278945e-01 2.57778909e-10 4.80721151e-01 5.31157074e-02]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1919, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.3409, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.92886014551334\n",
      "test accuracy:  99.14571637783745\n",
      "94\n",
      "[2.43068007e-01 1.01715616e-08 7.32327688e-01 4.63766459e-01\n",
      " 4.23607643e-02 4.17598103e-06 2.03345771e-02 5.49987664e-01\n",
      " 2.83303008e-01 1.87950282e-01 8.90670722e-01 3.54735681e-07\n",
      " 4.20264217e-01 7.81731653e-01 1.49141358e-07 7.30937634e-01\n",
      " 6.24039635e-01 8.32643058e-01 8.88425586e-01 4.20124407e-04\n",
      " 1.47216858e-02 9.52187082e-01 5.36946041e-01 7.05077087e-01\n",
      " 5.96890948e-01 1.29504310e+00 3.49770669e-01 6.40219861e-01\n",
      " 1.55373548e-03 2.78031738e-01 5.18995269e-01 2.81985813e-01\n",
      " 2.86606053e-08 1.21415215e-01 6.26708578e-01 5.16842810e-01\n",
      " 5.84773484e-01 1.97152536e-01 4.79539450e-03 4.01677009e-01\n",
      " 1.14036715e+00 2.00479023e-07 1.04555003e-10 1.00351009e+00\n",
      " 7.75645229e-06 3.58932583e-01 1.18838041e-04 4.81427290e-02\n",
      " 3.31406439e-01 1.75561020e+00 1.03075641e+00 2.31096252e-04\n",
      " 2.08847847e+00 3.73896826e-03 5.84673359e-01 8.27926436e-01\n",
      " 6.50405629e-01 1.33110870e-04 1.32319964e+00 6.21645116e-01\n",
      " 9.28507260e-05 3.56451462e-02 6.57957838e-03 2.91168378e-09\n",
      " 3.72021169e-01 1.12930070e-03 3.11025565e-08 3.01332126e-10\n",
      " 2.31640092e-04 6.03147876e-01 1.21699640e-01 6.99011626e-01\n",
      " 3.25925624e-01 2.38917853e+00 1.57436863e-03 3.97611863e-01\n",
      " 4.79889834e-03 1.49714139e-02 1.79758608e-03 5.20855935e-07\n",
      " 1.50770337e+00 2.26610569e-01 9.64483730e-01 1.68952717e+00\n",
      " 3.18794551e-02 2.60512239e-01 2.24329735e-10 6.92105252e-02\n",
      " 3.73435230e-01 7.65380916e-04 2.86141739e-01 5.61892242e-01\n",
      " 7.65563917e-05 1.02020046e-09 2.93999629e-01 2.60161202e-05\n",
      " 2.38827818e-01 2.10101686e-10 4.78505311e-01 5.27538407e-02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1910, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.3403, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.94907033144705\n",
      "test accuracy:  99.12130827434709\n",
      "95\n",
      "[2.42996442e-01 1.03627881e-08 7.34127267e-01 4.63035784e-01\n",
      " 4.32036349e-02 4.04797319e-06 1.99090110e-02 5.48992336e-01\n",
      " 2.84636430e-01 1.87961746e-01 8.91568721e-01 3.63608618e-07\n",
      " 4.18104789e-01 7.88909248e-01 1.45495829e-07 7.32401098e-01\n",
      " 6.31999749e-01 3.66289899e-01 8.87665014e-01 4.17612673e-04\n",
      " 1.45039826e-02 1.48282731e+00 4.00637028e-01 7.04304518e-01\n",
      " 5.95269359e-01 1.14791906e+00 3.41227527e-01 6.37115831e-01\n",
      " 1.55464661e-03 2.77700681e-01 5.19380275e-01 2.87396180e-01\n",
      " 7.57684028e-08 1.21908477e-01 6.25234379e-01 5.17287817e-01\n",
      " 5.98935869e-01 1.96382763e-01 4.76533827e-03 4.00312402e-01\n",
      " 1.45582455e+00 1.95693570e-07 1.02422085e-10 8.38935235e-01\n",
      " 7.54614468e-06 1.65885012e-01 1.18012311e-04 4.75966973e-02\n",
      " 3.37888217e-01 1.75784996e+00 1.03035779e+00 7.12354176e-05\n",
      " 2.09110941e+00 3.55052918e-03 5.87000056e-01 8.34724912e-01\n",
      " 6.51882150e-01 1.33303738e-04 1.32631804e+00 5.58432361e-01\n",
      " 9.09797182e-05 3.56412655e-02 6.61146412e-03 2.91056431e-09\n",
      " 3.78382010e-01 1.13706647e-03 3.03001239e-08 2.61187349e-10\n",
      " 2.34375669e-04 6.00585771e-01 1.26974652e-01 9.19759816e-01\n",
      " 3.27421718e-01 2.38714216e+00 1.57456796e-03 3.97205185e-01\n",
      " 4.80754582e-03 1.48907612e-02 1.83997676e-03 4.76377678e-07\n",
      " 1.50531487e+00 1.66583916e-01 9.64306658e-01 1.68888218e+00\n",
      " 3.20598575e-02 2.62326551e-01 2.28942101e-10 7.03139965e-02\n",
      " 5.91133004e-01 7.54794396e-04 2.63217358e-01 5.63031301e-01\n",
      " 7.71130885e-05 1.04628808e-09 2.98207279e-01 2.64678410e-05\n",
      " 2.40933511e-01 2.14487122e-10 4.81821062e-01 5.36664551e-02]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1913, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.3401, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.94907033144705\n",
      "test accuracy:  99.12130827434709\n",
      "96\n",
      "[2.42894514e-01 1.02817311e-08 7.33439708e-01 4.62291554e-01\n",
      " 8.38032048e-02 4.07783062e-06 2.00053017e-02 6.62655427e-01\n",
      " 2.82880517e-01 1.88506269e-01 6.55287542e+00 3.59717847e-07\n",
      " 4.24360816e-01 7.81628049e-01 1.49016386e-07 7.30043710e-01\n",
      " 6.21126768e-01 3.68086511e-01 8.89572122e-01 4.37377386e-04\n",
      " 1.44100901e-02 9.56143361e-01 3.98874762e-01 7.03128783e-01\n",
      " 5.97479673e-01 1.42423513e+00 3.43046180e-01 6.40673279e-01\n",
      " 1.55522221e-03 2.78272713e-01 5.19241643e-01 2.71622562e-01\n",
      " 2.85759764e-08 1.22505920e-01 1.10935193e+00 5.67713254e-01\n",
      " 5.84377953e-01 1.96829498e-01 4.78026609e-03 3.98578435e-01\n",
      " 1.14926930e+00 1.96725568e-07 1.02864744e-10 8.38017558e-01\n",
      " 7.57750181e-06 1.65615095e-01 3.69157076e-04 4.74485507e-02\n",
      " 3.33176125e-01 1.75915827e+00 1.02846848e+00 7.12380797e-05\n",
      " 2.09244143e+00 3.48135243e-03 5.85978898e-01 8.30882377e-01\n",
      " 6.52854260e-01 1.33508332e-04 1.32839798e+00 5.55800009e-01\n",
      " 9.28225308e-05 3.54122826e-02 6.64415001e-03 2.90303277e-09\n",
      " 3.77450313e-01 1.11923025e-03 3.05140328e-08 2.62966884e-10\n",
      " 2.39124925e-04 5.99560164e-01 1.20589535e-01 6.98193618e-01\n",
      " 3.26745196e-01 2.38560619e+00 1.57792141e-03 3.97440556e-01\n",
      " 4.82813823e-03 1.48247492e-02 2.33219775e-03 4.80585736e-07\n",
      " 1.50372040e+00 2.37550979e-01 9.64397979e-01 2.22614327e+00\n",
      " 5.46034690e-02 2.60401607e-01 2.27600396e-10 6.86843813e-02\n",
      " 3.33088481e-01 8.53889288e-04 2.65623072e-01 5.63726492e-01\n",
      " 7.53896067e-05 1.03396367e-09 3.02397188e-01 2.55315899e-05\n",
      " 2.39815673e-01 2.12866488e-10 4.79923924e-01 5.32287711e-02]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1956, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.3405, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.92886014551334\n",
      "test accuracy:  99.14571637783745\n",
      "97\n",
      "[2.42149428e-01 1.03882622e-08 7.34320226e-01 4.63108354e-01\n",
      " 4.33492825e-02 4.03805039e-06 1.97810309e-02 5.48875686e-01\n",
      " 2.82148150e-01 1.88337184e-01 8.92246595e-01 3.64167721e-07\n",
      " 4.18303989e-01 7.81246037e-01 1.47243351e-07 7.30624009e-01\n",
      " 8.42401036e-01 3.67842167e-01 9.10785588e-01 4.11773439e-04\n",
      " 1.41788920e-02 9.55081445e-01 4.01068075e-01 7.84710620e-01\n",
      " 5.95218698e-01 1.20321368e+00 3.41385651e-01 6.37165173e-01\n",
      " 1.54381188e-03 2.80612435e-01 5.19674871e-01 2.71515798e-01\n",
      " 2.84582446e-08 1.23189056e-01 6.23828490e-01 5.18168530e-01\n",
      " 6.08836004e-01 1.96498621e-01 4.73338408e-03 3.99252962e-01\n",
      " 1.13779699e+00 1.93686285e-07 1.01490288e-10 9.96320232e-01\n",
      " 7.46866798e-06 1.65412639e-01 1.15678089e-04 4.75072209e-02\n",
      " 3.31799414e-01 1.75934517e+00 1.84759898e+00 7.06305974e-05\n",
      " 2.09427798e+00 3.52603329e-03 5.87189464e-01 8.84309848e-01\n",
      " 6.54153503e-01 1.32540766e-04 1.33160588e+00 6.20642117e-01\n",
      " 8.92829121e-05 3.54260289e-02 6.72105726e-03 2.90148241e-09\n",
      " 3.80983556e-01 1.12585303e-03 1.18493637e-07 2.58927587e-10\n",
      " 2.36324516e-04 5.99169222e-01 1.20708900e-01 6.97328720e-01\n",
      " 3.27566783e-01 3.47001191e+00 1.58839128e-03 5.14815832e-01\n",
      " 4.75542631e-03 1.45650010e-02 2.47485463e-03 4.73223426e-07\n",
      " 1.50416576e+00 1.66800788e-01 9.64772070e-01 1.70241755e+00\n",
      " 3.25628306e-02 2.62493859e-01 2.30138408e-10 7.03687036e-02\n",
      " 3.33364737e-01 7.45036931e-04 2.63101178e-01 5.64221407e-01\n",
      " 7.69737593e-05 1.04584913e-09 2.94091004e-01 2.62617857e-05\n",
      " 2.45768041e-01 2.14716064e-10 4.85701540e-01 5.36505656e-02]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1885, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.3397, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.90864995957963\n",
      "test accuracy:  99.19453258481816\n",
      "98\n",
      "[2.39669531e-01 1.03549090e-08 7.34058564e-01 4.62460239e-01\n",
      " 4.34502239e-02 4.00533319e-06 1.97554602e-02 5.49167917e-01\n",
      " 6.76177354e-01 1.88614213e-01 8.90931700e-01 3.62970881e-07\n",
      " 4.18105935e-01 7.80834414e-01 1.45296804e-07 7.30470958e-01\n",
      " 6.22059300e-01 3.69236018e-01 9.19634448e-01 4.10034267e-04\n",
      " 1.33584482e-02 9.59988530e-01 3.97582809e-01 7.10269481e-01\n",
      " 5.94961678e-01 1.14229425e+00 3.41214404e-01 6.40554015e-01\n",
      " 1.53629262e-03 2.78363618e-01 5.19940332e-01 3.19166925e-01\n",
      " 2.84999911e-08 1.26608020e-01 6.22396311e-01 5.18983139e-01\n",
      " 5.85321576e-01 2.05738776e-01 4.70221289e-03 3.98077361e-01\n",
      " 1.13834438e+00 1.93546581e-07 1.01448280e-10 8.47250139e-01\n",
      " 7.46267728e-06 1.65102410e-01 1.07713837e-04 4.73910670e-02\n",
      " 3.30582923e-01 1.76094719e+00 1.02837452e+00 7.04242226e-05\n",
      " 2.10370842e+00 3.54398045e-03 5.86821424e-01 9.56869727e-01\n",
      " 6.60991048e-01 1.32520557e-04 1.34690792e+00 5.62888282e-01\n",
      " 8.65090077e-05 3.52413720e-02 7.08508514e-03 2.89626876e-09\n",
      " 3.71311244e-01 1.12738843e-03 3.00807425e-08 2.58638430e-10\n",
      " 2.39070108e-04 6.01914907e-01 1.42722671e-01 6.96549443e-01\n",
      " 3.27342912e-01 2.38609846e+00 1.65157928e-03 4.33162914e-01\n",
      " 4.47907194e-03 1.35658065e-02 1.84000976e-03 4.73594620e-07\n",
      " 1.50246630e+00 1.66144485e-01 9.63631981e-01 1.68778303e+00\n",
      " 3.44117628e-02 2.63277902e-01 2.32894759e-10 7.18531096e-02\n",
      " 3.70472662e-01 7.35886207e-04 2.73327228e-01 5.67375026e-01\n",
      " 7.71541599e-05 1.04155964e-09 3.01638258e-01 2.62374015e-05\n",
      " 2.38061527e-01 2.14375517e-10 4.76617146e-01 5.34653160e-02]\n",
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1927, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.3400, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.90864995957963\n",
      "test accuracy:  99.12130827434709\n",
      "99\n",
      "[8.71024872e-01 1.03636231e-08 7.34352309e-01 4.63284076e-01\n",
      " 4.40445551e-02 3.97411385e-06 1.96694002e-02 5.54196475e-01\n",
      " 2.87712379e-01 1.88480108e-01 8.91359698e-01 3.63918485e-07\n",
      " 4.16926122e-01 1.89506346e+00 1.44658341e-07 7.31077495e-01\n",
      " 6.21197589e-01 3.68777547e-01 8.86459327e-01 4.08811516e-04\n",
      " 1.33640009e-02 9.58877780e-01 4.01002722e-01 7.01764919e-01\n",
      " 5.93797272e-01 1.14591027e+00 3.41108439e-01 6.36177872e-01\n",
      " 1.53815874e-03 2.77700475e-01 5.19189205e-01 2.73269627e-01\n",
      " 2.84602929e-08 1.26593814e-01 6.23291159e-01 5.22108472e-01\n",
      " 7.60527946e-01 1.98282909e-01 4.67470470e-03 3.99248694e-01\n",
      " 1.13516910e+00 1.92837203e-07 1.01057579e-10 8.35435185e-01\n",
      " 7.41563032e-06 1.64940318e-01 1.08191844e-04 4.73467796e-02\n",
      " 4.27903740e-01 1.76143824e+00 1.02653647e+00 7.00483208e-05\n",
      " 2.10380999e+00 3.45623694e-03 5.87049463e-01 8.88195899e-01\n",
      " 6.60776118e-01 1.66571733e-04 1.34636921e+00 5.61304970e-01\n",
      " 8.62850894e-05 3.51449745e-02 7.16382289e-03 2.88963519e-09\n",
      " 3.72450312e-01 1.12972573e-03 3.06264536e-08 2.57444635e-10\n",
      " 2.38361646e-04 5.99384868e-01 1.20265652e-01 6.97478102e-01\n",
      " 3.27494794e-01 2.53275273e+00 1.64674688e-03 5.16235805e-01\n",
      " 4.51010636e-03 1.08515009e-01 1.86636055e-03 4.71218545e-07\n",
      " 1.50426860e+00 1.64289268e-01 9.66055420e-01 1.68540179e+00\n",
      " 3.42794212e-02 2.64384964e-01 2.38215128e-10 7.13741395e-02\n",
      " 3.88899610e-01 7.36567392e-04 2.70435968e-01 5.67346578e-01\n",
      " 7.82954357e-05 1.04383050e-09 3.13615964e-01 2.64993835e-05\n",
      " 2.38874233e-01 2.14828127e-10 4.77880969e-01 5.35700795e-02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 4 4 0 5 4 4 2 2 0 1 4 5 2 4 0 2 0 2 0 1 0 2 2 5 2 2 2 0 2 2 2 4 1 0 0 2\n",
      " 2 5 0 2 4 4 2 4 4 1 0 2 1 2 4 1 0 4 2 1 0 1 2 1 5 1 4 2 5 4 4 0 2 0 2 4 2\n",
      " 1 2 1 1 5 4 0 5 2 2 1 5 4 5 2 0 2 1 5 4 2 5 2 4 2 4]\n",
      "full train loss:  tensor(0.1931, device='cuda:0', dtype=torch.float64)\n",
      "full loss:  tensor(0.3404, device='cuda:0', dtype=torch.float64)\n",
      "val accuracy:  98.90864995957963\n",
      "test accuracy:  99.14571637783745\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# to do: implement early stopping and callbacks. Other than that, we're done! \n",
    "import collections\n",
    "from time import time\n",
    "import random\n",
    "k = 6\n",
    "\n",
    "seed = settings['seed']\n",
    "epochs = settings['n_epochs']\n",
    "experiment_name = settings['experiment_name']\n",
    "print(experiment_name)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "p2p = IFCA(total_clients, train_data, train_partition2, val_partition, \n",
    "           test_data, test_partition,settings['n_clients_UCB'], \n",
    "           settings['alpha'],test)\n",
    "p2p.loop(epochs, experiment_name, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0f51b9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 5 0 4 2 3 2 3 5 4 3 0 3 2 0 0 2 3 2 2 2 0 0 4 0 2 3 3 4 0 0 4 1 4 1 0 0\n",
      " 4 3 5 0 5 1 5 0 0 0 4 0 2 5 0 2 2 5 1 0 4 2 3 4 2 5 5 3 4 3 1 0 2 5 1 0 5\n",
      " 2 4 5 2 4 3 3 0 4 4 2 3 3 5 5 4 1 3 3 1 5 1 5 5 1 4]\n",
      "[ 9.36743184 10.30831967 10.43126519 11.39424943 12.99261433 10.81324634\n",
      " 10.50401541 15.81702677 16.15454371 11.95235402  9.79667609 10.6300406\n",
      " 12.99685403 16.57800359 10.40562017 11.49187793 15.87160287 12.23431364\n",
      " 15.52642678 12.40498564  9.14296638 12.36593808 16.12521356 14.64841787\n",
      " 13.12785507 16.67487069 15.02156454 16.45007342 11.47486049 16.11168727\n",
      " 14.8306929  15.59644243 10.99572713  9.42976952 11.75191086 11.7629579\n",
      " 16.30117113 15.11470363 13.04950721 11.79067681 15.64699407 11.2150761\n",
      " 10.59756757 15.11531909 10.51486146 10.67298332  9.41387582 10.99372221\n",
      " 14.94656048  9.73487899 16.36346982 10.50750775  9.86252682 11.92981055\n",
      "  9.92827041 16.23889069  9.63711866 11.92582198  9.07835158 16.32632176\n",
      "  9.75000498 13.09659082  9.44123762 10.70802076 16.23021818 12.96326305\n",
      " 10.4213794  10.91008647 11.10422632 16.34239427 11.46588629 16.42334634\n",
      " 10.05087294 16.27669313  9.29437796 15.42937046  9.88757571  9.59897288\n",
      " 13.28989227 11.02789834 12.33968005 13.06574214 16.00029144 16.03540409\n",
      "  9.5280296  12.79845993 10.68350394 13.15533526 16.07415205 11.04848737\n",
      " 15.28936728  9.45047309 13.01458783 10.3268566  15.80937825 12.73361435\n",
      " 16.4664131  10.31483843 16.01068884 10.64716685]\n",
      "[2 1 1 4 3 1 1 4 5 4 2 1 3 5 1 4 1 4 4 4 2 4 5 4 3 5 4 4 4 4 4 4 1 2 4 4 5\n",
      " 4 3 4 4 1 1 4 1 1 2 4 4 2 4 1 2 4 1 4 2 4 2 1 2 3 2 1 5 3 1 1 4 5 4 5 1 4\n",
      " 2 4 2 2 3 1 4 3 5 4 2 3 1 3 5 4 4 2 3 1 4 3 5 1 5 1]\n",
      "[ 1.64465909  0.18445445  0.36637457 14.50826234  1.91837712  0.42459683\n",
      "  0.3171502  19.08997383  1.62312476 14.47893817  1.67334329  0.36947211\n",
      "  2.04582634  2.12881664  0.3438993  14.55571777  2.38027912 14.31529897\n",
      " 20.03486096 14.36724899  1.39131907 14.65382877  1.43667125 17.19909085\n",
      "  2.0828531   2.34359098 18.2761675  18.77049791 14.96911177 19.27472937\n",
      " 17.74773634 18.4179872   0.24262843  1.5014426  15.00346638 14.86695952\n",
      "  2.27877381 17.81626681  2.0971101  14.73329206 18.49333906  0.22799958\n",
      "  0.21747471 18.47545046  0.23195675  0.64090625  1.16328188 14.70053883\n",
      " 18.8567634   1.69473913 19.16956757  0.46611536  1.71572691 14.66665241\n",
      "  0.58172552 19.30889058  1.60424358 14.43454318  1.54414191  1.72405047\n",
      "  1.17828907  2.33651722  1.3765474   0.31493065  1.67956175  2.28615743\n",
      "  0.20924643  0.20931478 14.83143206  1.76134045 15.43437954  2.51362386\n",
      "  0.34117613 19.06637855  1.11905744 18.595778    1.58036999  0.91604672\n",
      "  2.55410254  0.31543851 14.69884479  2.17293253  2.36332391 19.24904496\n",
      "  1.4792375   1.99520815  0.19760941  2.44328832  2.14526023 15.14947141\n",
      " 18.50889739  1.25705132  2.0766903   0.17762091 18.93819703  1.91780958\n",
      "  1.84009464  0.22638073  1.69437828  0.37052628]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[0.98409382 0.03351091 0.31932161 1.48787024 0.75558235 0.18045392\n",
      " 0.16187658 4.29907735 0.60389034 1.68675558 1.00047705 0.14817601\n",
      " 0.69094146 1.15203221 0.16402657 1.50750596 1.97847442 1.62803049\n",
      " 4.70542998 1.90795312 0.59876054 2.94656704 0.47660896 4.48925303\n",
      " 0.65457124 1.44155042 4.29381716 4.21848277 1.48693741 4.51379284\n",
      " 4.44454964 4.49297538 0.0388153  0.75081255 2.04797569 1.91518666\n",
      " 1.04186821 4.26665572 0.76146902 1.85004907 4.654895   0.03184901\n",
      " 0.02864262 4.72965675 0.07853101 0.59431133 0.43517306 1.51245025\n",
      " 4.92812598 1.18766503 4.78090773 0.27130761 1.32699611 1.84577636\n",
      " 0.59206438 5.20642054 0.72870494 1.37854712 1.20220882 0.79848779\n",
      " 0.3950146  0.71389759 0.59853374 0.1467398  0.62089205 0.76853994\n",
      " 0.02468887 0.02502213 1.45874797 1.48046135 2.38084943 1.47135131\n",
      " 0.28359391 4.54712943 0.47834716 4.15738964 0.45329381 0.27017236\n",
      " 1.03291576 0.0722872  2.16914984 0.86943415 1.26719098 4.56385897\n",
      " 0.76591295 0.57798452 0.02183271 0.89287708 0.95826622 1.37533854\n",
      " 4.50758693 0.66840734 0.72739758 0.034841   4.27294278 0.56788927\n",
      " 0.80453358 0.03946015 0.84094247 0.3610813 ]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[1.40777517 0.00876819 0.36824006 0.83673115 0.31791513 0.11125033\n",
      " 0.13136868 1.44809529 0.27467203 0.92477307 0.84307711 0.07375284\n",
      " 0.38065118 0.99588188 0.09952635 0.99233311 1.37354142 0.75372777\n",
      " 1.76608087 0.67808561 0.51829787 2.53437356 0.23857128 2.14087621\n",
      " 0.41471241 1.16914963 1.23448989 1.45255847 0.53291935 1.18791099\n",
      " 1.36903653 1.25300861 0.0098548  0.61176496 1.01825515 1.34421923\n",
      " 1.01087965 1.1220973  0.38504517 0.928744   2.00997864 0.00893264\n",
      " 0.00941167 2.08550852 0.06083826 0.6705426  0.25137768 0.94503799\n",
      " 1.78169232 1.1707475  1.79291327 0.24407035 1.22045527 0.93565523\n",
      " 0.63296273 1.81262775 0.64817581 0.44800205 0.98538364 0.47539686\n",
      " 0.31965993 0.35471087 0.55632055 0.02981049 0.35290805 0.38403228\n",
      " 0.00672678 0.0061686  0.60043839 0.77940637 1.29943049 0.92719247\n",
      " 0.32211593 1.97261099 0.33025593 1.52814022 0.3617661  0.17199329\n",
      " 0.41241355 0.03570224 1.24073349 0.59404982 1.5053458  1.88779792\n",
      " 0.53232987 0.27764246 0.00470428 0.48923372 0.84149421 0.63914841\n",
      " 1.73753399 0.56149801 0.30103596 0.00987208 1.12486532 0.2415917\n",
      " 0.48826052 0.01125828 0.47144486 0.37387674]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[7.71566025e-01 5.94526619e-03 4.50436972e-01 6.10794038e-01\n",
      " 2.15565977e-01 6.79143237e-02 1.12559824e-01 1.01131406e+00\n",
      " 2.14891477e-01 1.13677938e+00 6.63060409e-01 4.36491260e-02\n",
      " 2.47079982e-01 8.83753202e-01 7.95850830e-02 7.41997146e-01\n",
      " 1.21096013e+00 4.68565698e-01 1.45216544e+00 6.19134647e-01\n",
      " 3.31385055e-01 1.95547701e+00 2.37115904e-01 1.64648829e+00\n",
      " 3.34509350e-01 1.14571933e+00 8.63102737e-01 1.07963844e+00\n",
      " 3.25940816e-01 7.92702288e-01 9.58454985e-01 8.84036351e-01\n",
      " 2.86125966e-03 4.55281219e-01 7.36764532e-01 1.06740568e+00\n",
      " 6.07692736e-01 5.91198689e-01 2.63574612e-01 8.29562724e-01\n",
      " 1.48222606e+00 2.81280650e-03 2.58429619e-03 1.39328883e+00\n",
      " 3.81354715e-02 7.17378314e-01 1.07994122e-01 6.20160637e-01\n",
      " 1.16516108e+00 1.04541382e+00 1.47932688e+00 2.27267843e-01\n",
      " 1.30858140e+00 8.00457034e-01 7.27454786e-01 1.27543818e+00\n",
      " 5.51073369e-01 2.09029635e-01 1.06515899e+00 4.35589705e-01\n",
      " 1.36231846e-01 2.40796603e-01 2.92662565e-01 1.16022765e-02\n",
      " 2.36109163e-01 2.81353691e-01 1.68223450e-03 1.87478416e-03\n",
      " 2.67982868e-01 8.04576280e-01 1.04838447e+00 9.76782073e-01\n",
      " 3.94535837e-01 1.83428397e+00 2.21409044e-01 1.20792529e+00\n",
      " 1.91489774e-01 8.93752613e-02 2.53346852e-01 1.34759428e-02\n",
      " 1.20925771e+00 4.50734533e-01 1.05183422e+00 1.56837257e+00\n",
      " 4.58563953e-01 2.26217283e-01 1.13984690e-03 3.42999052e-01\n",
      " 5.94680934e-01 4.52084224e-01 1.15886331e+00 4.94932156e-01\n",
      " 1.71603728e-01 5.92356391e-03 7.57700203e-01 1.26400143e-01\n",
      " 3.81661338e-01 3.20951620e-03 4.93185735e-01 4.52115047e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[7.70015714e-01 7.67472961e-04 5.22424850e-01 5.16837760e-01\n",
      " 1.42225359e-01 3.78787727e-02 1.11469851e-01 7.59983319e-01\n",
      " 1.35041716e-01 6.65730171e-01 7.50266672e-01 1.88813738e-02\n",
      " 2.50310055e-01 9.00297563e-01 4.76115908e-02 7.26569742e-01\n",
      " 1.05439127e+00 4.56273954e-01 1.26998377e+00 4.53120378e-01\n",
      " 2.10117653e-01 1.67768329e+00 1.95834537e-01 1.18484347e+00\n",
      " 3.42105949e-01 1.03657534e+00 6.72049952e-01 9.20210108e-01\n",
      " 1.75019235e-01 6.46639538e-01 6.57480606e-01 6.62543552e-01\n",
      " 1.25198362e-03 4.64684072e-01 5.64220803e-01 9.57442821e-01\n",
      " 5.66823685e-01 4.17662700e-01 1.75967084e-01 7.08330715e-01\n",
      " 1.25328239e+00 1.09483647e-03 9.26750183e-04 1.11244320e+00\n",
      " 3.92199336e-02 8.43662990e-01 5.59889237e-02 5.46528965e-01\n",
      " 9.00802464e-01 1.18704948e+00 1.19082507e+00 2.36888536e-01\n",
      " 1.48663434e+00 5.64567957e-01 7.99889142e-01 1.08698078e+00\n",
      " 6.01900793e-01 1.25136706e-01 1.37933375e+00 3.38116949e-01\n",
      " 9.29005871e-02 1.58887313e-01 3.18936902e-01 3.84551126e-03\n",
      " 2.52944125e-01 1.89865737e-01 4.60788098e-04 4.38450342e-04\n",
      " 1.88390430e-01 7.66239370e-01 7.53207778e-01 8.23675998e-01\n",
      " 4.62869504e-01 1.70394425e+00 2.18841783e-01 9.23168050e-01\n",
      " 1.44741254e-01 6.66677893e-02 1.38709278e-01 8.56673552e-03\n",
      " 1.09323805e+00 3.98523072e-01 1.24281314e+00 1.66310588e+00\n",
      " 5.49838260e-01 1.37005571e-01 2.63885480e-04 2.64293494e-01\n",
      " 5.56992776e-01 2.98691060e-01 7.91712421e-01 5.68702408e-01\n",
      " 8.81574959e-02 1.19529870e-03 5.55760411e-01 5.23929099e-02\n",
      " 2.92117706e-01 9.77037389e-04 3.60165984e-01 4.87304972e-01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[6.99557587e-01 2.07062840e-04 5.21567007e-01 4.89326226e-01\n",
      " 1.59507793e-01 4.00211374e-02 1.51415532e-01 6.73029167e-01\n",
      " 8.59738493e-02 6.12378289e-01 6.99178650e-01 1.07668853e-02\n",
      " 1.50333930e-01 9.98965814e-01 1.44768523e-02 6.81705833e-01\n",
      " 8.93709595e-01 3.63554025e-01 1.18474856e+00 2.91338149e-01\n",
      " 1.66921555e-01 1.47669533e+00 1.20101367e-01 1.03224975e+00\n",
      " 2.65883424e-01 1.03429504e+00 6.07753273e-01 7.94736386e-01\n",
      " 1.00589428e-01 4.84625689e-01 5.84252516e-01 6.00451627e-01\n",
      " 2.79842148e-04 3.81517710e-01 5.25975505e-01 9.07686876e-01\n",
      " 6.71010171e-01 3.81787758e-01 1.74964924e-01 4.94942342e-01\n",
      " 1.14534715e+00 1.31319152e-03 8.75888924e-04 1.36803793e+00\n",
      " 5.48098171e-02 9.45778416e-01 3.55299690e-02 4.65943614e-01\n",
      " 8.09437816e-01 1.14825507e+00 1.07445450e+00 2.61261954e-01\n",
      " 1.48171088e+00 4.72102954e-01 7.91146381e-01 9.60237578e-01\n",
      " 5.62903452e-01 6.35154059e-02 1.28447162e+00 3.56964431e-01\n",
      " 6.18697721e-02 1.20599199e-01 2.50932514e-01 2.04773812e-03\n",
      " 1.15934511e-01 2.17284529e-01 2.95291955e-04 1.79552494e-04\n",
      " 1.14392843e-01 6.32930709e-01 6.73510544e-01 6.28493027e-01\n",
      " 4.88051680e-01 1.76528307e+00 6.49432400e-01 8.47353242e-01\n",
      " 1.06399443e-01 4.75193348e-02 1.83851098e-01 1.03663015e-02\n",
      " 1.12768055e+00 2.27649159e-01 1.35118159e+00 1.51885596e+00\n",
      " 4.34539515e-01 1.34383357e-01 1.89046488e-04 2.16480097e-01\n",
      " 6.09158606e-01 2.31709800e-01 6.89655781e-01 5.22850242e-01\n",
      " 8.64565208e-02 2.47116206e-04 4.61552114e-01 5.75745268e-02\n",
      " 2.42248601e-01 6.78513662e-04 2.21341858e-01 4.29934605e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[6.38778290e-01 6.93299056e-05 5.95630408e-01 4.62709751e-01\n",
      " 9.06220678e-02 2.07596097e-02 1.40803721e-01 5.74108721e-01\n",
      " 7.04489570e-02 5.60084474e-01 6.56453765e-01 4.27877275e-03\n",
      " 1.95320065e-01 9.48574437e-01 8.95511021e-03 6.16385728e-01\n",
      " 8.21275870e-01 2.93379486e-01 1.07368271e+00 2.50375605e-01\n",
      " 2.35779354e-01 1.17259288e+00 1.37885396e-01 8.94097738e-01\n",
      " 2.87421120e-01 9.28570689e-01 4.86734512e-01 6.90315879e-01\n",
      " 9.59858057e-02 4.03802105e-01 4.90979662e-01 5.05605380e-01\n",
      " 1.62493500e-04 2.96327686e-01 5.77440506e-01 8.16411431e-01\n",
      " 4.84497087e-01 3.05990350e-01 1.22164313e-01 4.92502695e-01\n",
      " 1.17592500e+00 4.64274593e-04 2.32086448e-04 9.82313891e-01\n",
      " 3.57047749e-02 9.77382062e-01 4.18666645e-02 3.63280177e-01\n",
      " 5.95180810e-01 1.09630613e+00 9.57660447e-01 2.26358178e-01\n",
      " 1.40573950e+00 4.01192333e-01 8.49014695e-01 8.74039953e-01\n",
      " 5.04582969e-01 4.48502819e-02 1.03498360e+00 2.24628329e-01\n",
      " 4.35624812e-02 9.52846962e-02 1.76340674e-01 6.28822432e-04\n",
      " 1.04688726e-01 1.24173518e-01 1.38580689e-04 3.88039288e-05\n",
      " 7.33470871e-02 6.23066319e-01 9.08829150e-01 6.08366141e-01\n",
      " 5.38657839e-01 2.31550112e+00 1.73000021e-01 6.91797500e-01\n",
      " 1.16848187e-01 5.31916439e-02 9.10915722e-02 3.72995733e-03\n",
      " 1.12669821e+00 2.93733915e-01 1.05569182e+00 1.58076047e+00\n",
      " 2.70544945e-01 9.89768990e-02 3.81531004e-05 2.10033501e-01\n",
      " 4.87842845e-01 1.98049019e-01 6.00758772e-01 4.61783270e-01\n",
      " 4.99356557e-02 8.26697712e-05 3.98234135e-01 2.98888151e-02\n",
      " 2.20452618e-01 1.83676395e-04 1.95812044e-01 4.75481452e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[6.47700979e-01 2.70346962e-05 5.97725854e-01 4.05278632e-01\n",
      " 1.03332658e-01 1.85990899e-02 1.63736882e-01 5.45434295e-01\n",
      " 5.63127048e-02 5.41269628e-01 6.76162920e-01 2.16754611e-03\n",
      " 1.41715523e-01 1.04478975e+00 2.20791843e-03 7.20785597e-01\n",
      " 7.78088451e-01 5.00666396e-01 1.01354777e+00 1.49687593e-01\n",
      " 1.71806201e-01 1.76685596e+00 8.41021135e-02 8.83339878e-01\n",
      " 2.24365419e-01 9.32961995e-01 4.35626634e-01 6.77183220e-01\n",
      " 6.29534722e-02 3.18062091e-01 4.60049339e-01 4.53676357e-01\n",
      " 1.33652178e-04 2.75105913e-01 4.46385162e-01 9.18634648e-01\n",
      " 5.88430373e-01 2.72099454e-01 1.07616919e-01 3.20395568e-01\n",
      " 1.18339942e+00 6.19384106e-04 1.28895001e-04 9.85403655e-01\n",
      " 4.24873964e-02 9.95698764e-01 2.23495558e-02 6.70210461e-01\n",
      " 5.47568974e-01 1.15099572e+00 9.86268844e-01 2.02013193e-01\n",
      " 1.48767335e+00 3.10537667e-01 7.81354619e-01 8.15691043e-01\n",
      " 5.03147944e-01 6.60481243e-02 1.15240658e+00 3.01939092e-01\n",
      " 2.70130809e-02 6.19290244e-02 1.62754270e-01 3.33670660e-04\n",
      " 8.90942176e-02 1.09618543e-01 9.48632227e-05 1.23228310e-05\n",
      " 1.67173951e-01 5.60800774e-01 5.88330144e-01 4.82018913e-01\n",
      " 5.23785443e-01 1.74213347e+00 7.79480798e-02 6.31440905e-01\n",
      " 8.36963304e-02 3.87017614e-02 9.41821287e-02 2.95820872e-03\n",
      " 1.00677320e+00 1.93908535e-01 1.35059568e+00 2.01822371e+00\n",
      " 2.90514686e-01 9.45659597e-02 1.78216192e-05 1.72209992e-01\n",
      " 5.32219870e-01 1.24950013e-01 5.54234279e-01 4.72437974e-01\n",
      " 4.03322764e-02 2.08967903e-05 3.11459907e-01 2.63201010e-02\n",
      " 2.01319646e-01 1.25442995e-04 1.35974734e-01 4.21172469e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[5.94775963e-01 1.42112392e-05 7.20898354e-01 3.76286705e-01\n",
      " 4.90766056e-02 3.92303538e-03 1.09553568e-01 5.45526071e-01\n",
      " 3.68104064e-02 4.78103165e-01 6.89463464e-01 8.99473446e-04\n",
      " 2.48662415e-01 9.50228629e-01 3.26691150e-03 6.31982871e-01\n",
      " 6.74995437e-01 3.64141702e-01 1.02003966e+00 9.93321571e-02\n",
      " 2.10683899e-01 1.39451890e+00 1.36975946e-01 7.41224170e-01\n",
      " 3.26459892e-01 1.86212813e+00 3.84409606e-01 5.72342247e-01\n",
      " 3.60079125e-02 2.85007853e-01 4.20713325e-01 3.64852093e-01\n",
      " 3.17652855e-05 2.17180313e-01 4.83283093e-01 1.36070747e+00\n",
      " 3.99933633e-01 2.32342790e-01 1.30621320e-01 2.88238295e-01\n",
      " 9.61472956e-01 1.59577830e-04 1.86459914e-05 8.20390615e-01\n",
      " 1.30092474e-02 9.67781320e-01 2.76199504e-02 5.15264341e-01\n",
      " 4.62644322e-01 1.10852199e+00 8.67004717e-01 1.57644279e-01\n",
      " 1.38951584e+00 2.35300010e-01 9.41909303e-01 7.29996534e-01\n",
      " 4.42510474e-01 3.32583916e-02 9.55912396e-01 1.79408630e-01\n",
      " 2.68955571e-02 8.56707812e-02 1.28306081e-01 1.87113463e-04\n",
      " 5.89440749e-02 8.52492577e-02 1.68443403e-05 3.75271565e-06\n",
      " 9.75037627e-02 5.20737161e-01 4.99390507e-01 4.19028330e-01\n",
      " 6.31520623e-01 1.64914292e+00 6.72038841e-02 5.96701642e-01\n",
      " 1.01806449e-01 5.91052309e-02 5.24946124e-02 4.56463703e-04\n",
      " 9.78538113e-01 3.49907951e-01 1.12771042e+00 1.53751848e+00\n",
      " 2.01824918e-01 7.02635247e-02 2.45193082e-06 1.74937466e-01\n",
      " 4.31327332e-01 1.02805304e-01 5.94991443e-01 4.14501637e-01\n",
      " 2.63522077e-02 2.11718506e-05 2.87933860e-01 1.21307788e-02\n",
      " 1.44318468e-01 1.68962350e-05 1.11052588e-01 5.63425932e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[5.47360158e-01 3.79965704e-06 7.24647722e-01 3.93031381e-01\n",
      " 3.58722684e-02 5.61276483e-03 1.68920623e-01 4.24183059e-01\n",
      " 3.00096948e-02 4.76934826e-01 7.14428484e-01 4.16264236e-04\n",
      " 3.08702128e-01 1.07520614e+00 7.09614325e-04 6.47353953e-01\n",
      " 7.41759975e-01 3.25866169e-01 1.00337752e+00 8.50227499e-02\n",
      " 2.03161863e-01 1.50822800e+00 9.20382187e-02 6.69891862e-01\n",
      " 3.31863193e-01 8.73551117e-01 3.26749562e-01 5.58624744e-01\n",
      " 2.85954609e-02 4.02871353e-01 4.02058314e-01 3.73658612e-01\n",
      " 2.06713455e-05 1.95855511e-01 5.18502552e-01 7.89080243e-01\n",
      " 5.37528887e-01 2.86874997e-01 8.24475899e-02 2.98774408e-01\n",
      " 8.48219299e-01 1.93412327e-04 1.67389800e-05 7.40273878e-01\n",
      " 1.58087710e-02 1.01723760e+00 2.36688088e-02 4.17373051e-01\n",
      " 4.29375158e-01 1.15936324e+00 7.83433798e-01 1.48600821e-01\n",
      " 1.46165758e+00 2.44631600e-01 8.85308350e-01 7.30062711e-01\n",
      " 4.58510039e-01 1.63183878e-02 9.22692512e-01 1.43807766e-01\n",
      " 2.15395769e-02 8.55022740e-02 1.14783518e-01 3.41164637e-05\n",
      " 4.02646780e-02 7.79369474e-02 1.62620998e-05 9.78243610e-07\n",
      " 6.45002915e-02 5.32360595e-01 4.75517008e-01 3.71277381e-01\n",
      " 6.27350086e-01 2.35028286e+00 5.63690687e-02 5.72709009e-01\n",
      " 1.08295432e-01 7.76759931e-02 3.18730922e-02 4.22463904e-04\n",
      " 1.06379895e+00 5.24612155e-01 1.86513379e+00 1.40691371e+00\n",
      " 1.40497431e-01 8.05084359e-02 1.60053090e-06 1.71034697e-01\n",
      " 5.00586524e-01 6.51626670e-02 3.68106884e-01 4.22226316e-01\n",
      " 3.34311369e-02 1.35753489e-05 2.84548753e-01 9.22458422e-03\n",
      " 1.17296616e-01 1.06197016e-05 8.65746502e-02 5.03375761e-01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[5.56324632e-01 1.72889111e-06 6.98964914e-01 4.06386351e-01\n",
      " 4.46897274e-02 1.04463422e-02 2.12386710e-01 3.96693946e-01\n",
      " 2.59211569e-02 4.32645806e-01 8.09288211e-01 4.40588751e-04\n",
      " 2.09826759e-01 9.72431442e-01 1.51294076e-04 6.45485693e-01\n",
      " 6.27005167e-01 4.87233187e-01 9.47605973e-01 7.86326185e-02\n",
      " 6.77364370e-01 1.03985909e+00 9.02083550e-02 6.46732870e-01\n",
      " 2.66369808e-01 8.95654175e-01 3.09085001e-01 5.07408239e-01\n",
      " 3.09479625e-02 2.26881789e-01 3.82132784e-01 2.90493889e-01\n",
      " 1.84308259e-05 1.95407301e-01 6.07740847e-01 7.63229252e-01\n",
      " 3.23325969e-01 1.77487222e-01 5.65607603e-02 2.86748561e-01\n",
      " 8.59014672e-01 7.92038996e-04 1.76506379e-05 7.98433930e-01\n",
      " 2.85822176e-02 1.10988195e+00 5.18447942e-02 3.53239911e-01\n",
      " 4.02385701e-01 1.13240582e+00 7.79958934e-01 1.82219099e-01\n",
      " 1.45397738e+00 1.84823875e-01 8.23992626e-01 6.57538309e-01\n",
      " 4.73516292e-01 1.03688479e-02 8.11784678e-01 1.34507933e-01\n",
      " 4.13030470e-02 3.99029067e-02 1.12041177e-01 3.66489391e-05\n",
      " 2.86999372e-02 5.96953779e-02 1.39595152e-05 5.51898688e-07\n",
      " 3.10352978e-02 5.44405680e-01 4.40455221e-01 4.33376649e-01\n",
      " 6.10021104e-01 1.62455583e+00 7.93961343e-02 5.05652857e-01\n",
      " 2.30133589e-01 1.45804564e-01 2.50631682e-02 6.79724939e-04\n",
      " 1.08076710e+00 2.33699552e-01 1.10021627e+00 1.41845353e+00\n",
      " 8.95189292e-02 4.97293686e-02 1.63644222e-06 1.17256483e-01\n",
      " 3.82300563e-01 5.72500546e-02 3.38693206e-01 3.99943701e-01\n",
      " 1.20620987e-02 6.85602863e-07 2.41440621e-01 6.67194929e-03\n",
      " 1.09395641e-01 1.33931023e-05 1.13798979e-01 4.27567137e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[6.29055090e-01 1.14097226e-06 7.99456031e-01 3.95388298e-01\n",
      " 4.05523593e-02 2.24355205e-03 1.46138497e-01 3.63806019e-01\n",
      " 2.19734146e-02 4.32815804e-01 9.00721617e-01 1.20360572e-04\n",
      " 2.14207977e-01 9.12428170e-01 2.74118599e-04 6.71110725e-01\n",
      " 5.93639943e-01 2.87012709e-01 9.31936267e-01 3.92695963e-02\n",
      " 4.22111216e-01 1.38672709e+00 8.98179685e-02 7.93132128e-01\n",
      " 5.21519478e-01 9.16706233e-01 2.88103699e-01 4.74486756e-01\n",
      " 1.24861197e-02 2.06804129e-01 3.76148769e-01 2.76964208e-01\n",
      " 4.25610041e-06 2.16974988e-01 5.85831939e-01 8.06089890e-01\n",
      " 3.08723380e-01 1.66724468e-01 4.47206403e-02 2.42861460e-01\n",
      " 8.25999091e-01 4.56165989e-05 3.18182595e-06 6.98119669e-01\n",
      " 3.45940076e-02 1.05211246e+00 6.81931586e-02 4.49725164e-01\n",
      " 3.77570613e-01 1.16833619e+00 7.21521772e-01 1.23574234e-01\n",
      " 1.49170166e+00 1.39018804e-01 9.73186384e-01 6.21015224e-01\n",
      " 4.95684677e-01 9.17184971e-03 6.71655490e-01 1.25855589e-01\n",
      " 4.96548580e-02 3.77064883e-02 1.09861730e-01 8.45448359e-06\n",
      " 2.55606842e-02 3.66712857e-02 2.67037880e-06 1.70067537e-07\n",
      " 3.64273881e-02 5.18796301e-01 4.32957090e-01 3.72040866e-01\n",
      " 7.00396269e-01 1.58030669e+00 1.32862179e-01 4.84664604e-01\n",
      " 2.64612912e-01 1.77504352e-01 1.82797429e-02 1.28655826e-04\n",
      " 1.08313533e+00 2.27064512e-01 9.70627287e-01 1.35610247e+00\n",
      " 8.75784142e-02 6.74545220e-02 2.59506651e-07 1.15779904e-01\n",
      " 3.57663560e-01 4.04620031e-02 3.01497916e-01 3.97620615e-01\n",
      " 8.80995420e-03 9.93649694e-07 2.33303615e-01 7.90759227e-03\n",
      " 9.77469975e-02 2.43140435e-06 9.25214655e-02 5.40735636e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[5.04081673e-01 8.73268638e-07 8.61179924e-01 3.94172690e-01\n",
      " 3.37840809e-02 1.04383845e-03 1.22084375e-01 3.48330879e-01\n",
      " 1.61152522e-02 4.21125179e-01 8.02818650e-01 7.17995216e-05\n",
      " 2.21100109e-01 8.95698235e-01 2.60637708e-04 6.91609083e-01\n",
      " 5.33238433e-01 2.99293590e-01 9.17333621e-01 2.85305933e-02\n",
      " 2.06080699e-01 1.53511095e+00 9.18908497e-02 5.48383017e-01\n",
      " 2.91992897e-01 9.50975545e-01 2.37832671e-01 4.54709967e-01\n",
      " 7.95470217e-03 1.67575397e-01 4.15898571e-01 2.04046861e-01\n",
      " 2.32686544e-06 1.65898359e-01 5.95234026e-01 8.21108076e-01\n",
      " 6.01072341e-01 1.39633979e-01 3.88176013e-02 2.33160195e-01\n",
      " 1.18487342e+00 1.84094427e-05 1.02578264e-06 1.07838307e+00\n",
      " 3.85544168e-03 1.04592372e+00 1.79090254e-02 5.19980464e-01\n",
      " 3.39662275e-01 1.24854738e+00 7.37668516e-01 1.03324877e-01\n",
      " 1.54510551e+00 1.33855164e-01 1.04963534e+00 6.29859498e-01\n",
      " 4.59503773e-01 7.15768426e-03 8.85841107e-01 1.17311485e-01\n",
      " 1.54479649e-02 3.54427494e-02 1.03337839e-01 4.21039926e-06\n",
      " 2.05977563e-02 3.34862473e-02 9.17049941e-07 9.03209239e-08\n",
      " 3.53888285e-02 4.89827222e-01 4.32605119e-01 3.09820416e-01\n",
      " 7.55378001e-01 1.60740682e+00 3.62907173e-02 4.58483100e-01\n",
      " 1.35265238e-01 1.05190393e-01 1.40304405e-02 4.33484558e-05\n",
      " 1.08115846e+00 2.33608635e-01 9.89646679e-01 1.36901721e+00\n",
      " 7.97118696e-02 4.63302492e-02 8.26081931e-08 1.08060447e-01\n",
      " 3.47829627e-01 3.20112451e-02 2.76217170e-01 4.30183214e-01\n",
      " 7.63366874e-03 8.11712233e-07 2.01494928e-01 3.88959171e-03\n",
      " 9.10867917e-02 7.41753699e-07 8.74526958e-02 5.93791603e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[4.78564083e-01 6.91165920e-07 8.81898871e-01 3.96300006e-01\n",
      " 2.82617476e-02 7.02354796e-04 1.05242119e-01 3.06591204e-01\n",
      " 1.36685211e-02 5.89424760e-01 8.46606755e-01 4.64990909e-05\n",
      " 2.11355255e-01 8.88162194e-01 1.68687229e-04 6.96716488e-01\n",
      " 5.01977571e-01 2.22920205e-01 8.84443667e-01 3.05829717e-02\n",
      " 1.77868096e-01 9.22117291e-01 9.87943218e-02 4.71400328e-01\n",
      " 2.72712793e-01 8.03368314e-01 2.33627216e-01 4.41080883e-01\n",
      " 1.10720887e-02 1.78973652e-01 3.78013886e-01 2.11300056e-01\n",
      " 1.89999005e-06 1.23740602e+00 6.62127254e-01 7.29175266e-01\n",
      " 2.68852049e-01 1.50150413e-01 3.32504869e-02 2.23121772e-01\n",
      " 6.79094298e-01 8.14627255e-06 5.27004702e-07 5.86296578e-01\n",
      " 2.64314408e-03 1.03930541e+00 1.27303483e-02 3.24914987e-01\n",
      " 2.97454013e-01 1.32201333e+00 6.57052305e-01 9.37340470e-02\n",
      " 1.66778808e+00 1.00404437e-01 1.08238916e+00 6.15509884e-01\n",
      " 5.01558403e-01 4.82095324e-03 9.86427942e-01 9.94021252e-02\n",
      " 1.13509949e-02 2.24284063e-02 1.24584310e-01 2.88009776e-06\n",
      " 1.71460631e-02 2.70474785e-02 3.84255758e-07 5.37073578e-08\n",
      " 1.93013502e-02 4.69400921e-01 3.99938531e-01 2.50325641e-01\n",
      " 7.87410153e-01 1.49972164e+00 2.84030985e-02 4.60502961e-01\n",
      " 1.32058624e-01 9.74877325e-02 1.32489308e-02 2.53185533e-05\n",
      " 1.11607460e+00 1.98630248e-01 9.81497228e-01 1.22845305e+00\n",
      " 7.22271719e-02 5.32290233e-02 4.39802844e-08 9.33895916e-02\n",
      " 3.46049856e-01 2.31537391e-02 3.29837514e-01 4.62822238e-01\n",
      " 4.11331978e-03 5.37967412e-07 1.90883501e-01 2.47857886e-03\n",
      " 8.20308049e-02 4.12078612e-07 5.30942998e-02 5.98756246e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[4.84690558e-01 2.04672029e-07 7.92123348e-01 3.74317026e-01\n",
      " 3.72736221e-02 2.11531433e-03 1.68428973e-01 3.05225042e-01\n",
      " 1.12156748e-02 3.49198868e-01 8.41816533e-01 3.32586236e-05\n",
      " 1.72984500e-01 9.46680738e-01 2.24821398e-05 6.88195866e-01\n",
      " 5.11603680e-01 2.38158560e-01 1.32738708e+00 3.05116126e-02\n",
      " 8.67853128e-02 9.25407351e-01 6.85463685e-02 6.35992305e-01\n",
      " 2.45427695e-01 8.16057053e-01 2.01713658e-01 4.30670206e-01\n",
      " 1.19369284e-02 1.49530027e-01 3.72601125e-01 1.73645809e-01\n",
      " 7.97668201e-06 2.04819894e-01 6.43585233e-01 7.10479773e-01\n",
      " 2.93717113e-01 1.24982576e-01 3.50525106e-02 2.09154909e-01\n",
      " 9.82494662e-01 2.70252227e-05 1.02132184e-06 5.96964378e-01\n",
      " 6.53589161e-03 1.03489654e+00 4.40854747e-03 4.23559069e-01\n",
      " 2.55014041e-01 1.39912429e+00 6.97206095e-01 9.91228819e-02\n",
      " 1.74743854e+00 7.45805134e-02 8.90176266e-01 5.15194417e-01\n",
      " 5.16455219e-01 4.36914040e-03 3.87256120e+00 7.85390934e-02\n",
      " 4.14345450e-03 1.77755741e-02 1.53072324e-01 2.43418386e-06\n",
      " 1.26653668e-02 2.72656746e-02 1.13162390e-06 2.58192456e-08\n",
      " 2.00406936e-02 4.73202693e-01 3.88545889e-01 2.29459487e-01\n",
      " 6.90477268e-01 1.54902831e+00 1.79695130e-02 4.20715503e-01\n",
      " 7.45273475e-02 5.19267450e-02 1.68865399e-02 5.71502860e-05\n",
      " 1.10887012e+00 1.42791070e-01 1.10968687e+00 1.51596189e+00\n",
      " 1.04934227e-01 8.73410320e-02 7.09620428e-08 1.07064022e-01\n",
      " 3.59541209e-01 1.67776940e-02 2.08085029e-01 4.97137190e-01\n",
      " 4.09233434e-03 8.12931736e-08 1.67642477e-01 2.86008622e-03\n",
      " 5.59900201e-02 8.61826359e-07 4.26671974e-02 4.51293320e-01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[4.70994046e-01 6.07009441e-07 9.61754765e-01 3.76154526e-01\n",
      " 1.59471148e-02 1.52111146e-04 6.76966530e-02 3.00959997e-01\n",
      " 1.22324617e-02 3.31116317e-01 8.42012863e-01 3.32486797e-05\n",
      " 4.97627905e-01 8.78406344e-01 1.25084788e-04 6.81232721e-01\n",
      " 4.59880959e-01 2.15463401e-01 8.50256836e-01 2.20777831e-02\n",
      " 1.24109213e-01 9.63825256e-01 8.31381892e-02 3.81566370e-01\n",
      " 2.89583216e-01 8.06104715e-01 1.88241644e-01 4.08011787e-01\n",
      " 7.20466557e-03 1.46962500e-01 3.62453114e-01 1.73968699e-01\n",
      " 2.52107817e-06 1.87545672e-01 6.64581667e-01 7.05939588e-01\n",
      " 2.32621287e-01 1.24011291e-01 2.77056040e-02 2.02489964e-01\n",
      " 6.35372928e-01 3.63460966e-06 8.06441890e-08 5.62365901e-01\n",
      " 6.08239981e-04 9.75544713e-01 5.05064310e-03 3.64939587e-01\n",
      " 2.30302044e-01 1.40867417e+00 6.57795802e-01 6.02112525e-02\n",
      " 1.73027017e+00 7.21394440e-02 3.86167932e+00 5.35882707e-01\n",
      " 5.05038112e-01 4.06649970e-03 1.08914169e+00 7.96967684e-02\n",
      " 4.42471630e-03 2.81204420e-02 1.27192223e-01 1.66522691e-06\n",
      " 1.12993917e-02 1.83277822e-02 1.18103491e-07 3.27390651e-08\n",
      " 1.38541278e-02 4.82278484e-01 3.73786677e-01 5.74500631e-01\n",
      " 8.34631592e-01 1.54093549e+00 1.57839659e-02 4.00322800e-01\n",
      " 8.59877386e-02 5.37542865e-01 6.55891701e-03 1.30336610e-05\n",
      " 1.07597347e+00 2.44812804e-01 9.17979351e-01 1.51421661e+00\n",
      " 6.90344084e-02 3.96939218e-02 1.03702096e-08 7.63844216e-02\n",
      " 3.30738780e-01 1.66160707e-02 2.60269280e-01 4.79729879e-01\n",
      " 2.86101368e-03 4.74582884e-07 1.64077872e-01 1.74312288e-03\n",
      " 5.46836417e-02 7.32829853e-08 5.73103127e-02 6.70048131e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[4.93026938e-01 1.10781144e-07 8.64862540e-01 3.64608422e-01\n",
      " 5.24363883e-02 4.85987356e-04 1.22527070e-01 2.99736336e-01\n",
      " 1.15418139e-02 3.21156450e-01 8.81029384e-01 1.02487803e-05\n",
      " 1.37084067e-01 8.90602694e-01 1.61967232e-05 6.78343263e-01\n",
      " 4.40366057e-01 2.35891645e-01 8.32258142e-01 1.51494949e-02\n",
      " 1.68770077e-01 9.92604977e-01 7.22261783e-02 3.58820706e-01\n",
      " 1.97300118e-01 8.28073186e-01 1.56052948e-01 3.97920898e-01\n",
      " 4.95476850e-03 1.39891824e-01 3.47874796e-01 1.63891337e-01\n",
      " 1.59717662e-06 1.54645314e-01 6.40959495e-01 7.19951842e-01\n",
      " 2.15998417e-01 1.15606549e-01 4.27473807e-02 1.89068664e-01\n",
      " 6.03765783e-01 1.51243051e-05 1.47884660e-07 5.80267562e-01\n",
      " 2.05725949e-03 9.66721893e-01 7.60845965e-03 3.68605941e-01\n",
      " 1.99484452e-01 1.42720825e+00 6.31740562e-01 4.36724313e-02\n",
      " 1.72130958e+00 5.63276438e-02 9.43882912e-01 5.60634474e-01\n",
      " 4.88610578e-01 3.83847015e-03 4.55529640e+00 7.33330729e-02\n",
      " 6.03504896e-03 1.73084024e-02 1.03838721e-01 1.17898369e-06\n",
      " 9.00519753e-03 3.46994422e-02 4.08036327e-07 8.33871878e-09\n",
      " 1.45007612e-02 5.00881487e-01 3.68620097e-01 2.60703446e-01\n",
      " 7.33689821e-01 1.52136029e+00 1.94582633e-02 3.80233032e-01\n",
      " 1.21993093e-01 1.01464375e-01 2.24680562e-02 1.10744834e-05\n",
      " 1.04986337e+00 7.76096861e-02 9.16843928e-01 1.27847295e+00\n",
      " 4.61013089e-02 9.85997477e-02 1.00157744e-08 2.41927225e-01\n",
      " 3.29053696e-01 1.36405427e-02 1.90230888e-01 4.69368241e-01\n",
      " 4.50288993e-03 6.31041791e-08 1.37526867e-01 3.26735337e-03\n",
      " 5.18818101e-02 1.58013405e-07 6.27319798e-02 5.00705177e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[4.52277509e-01 1.57620958e-07 9.20399455e-01 4.07593150e-01\n",
      " 2.38554987e-02 1.73215357e-04 7.86512701e-02 3.10902220e-01\n",
      " 9.03170196e-03 5.21643178e-01 8.72968145e-01 8.85058310e-06\n",
      " 1.97508007e-01 1.09461018e+00 2.46599595e-05 7.29569296e-01\n",
      " 5.28920332e-01 1.98837449e-01 9.30005206e-01 1.33956075e-02\n",
      " 8.82895530e-02 8.47101197e-01 3.08081565e-02 3.58477161e-01\n",
      " 2.84172653e-01 1.30926724e+00 1.35487330e-01 3.82173551e-01\n",
      " 4.43887809e-03 1.11424949e-01 3.45553389e-01 1.32256730e-01\n",
      " 1.71297777e-06 1.88247730e-01 7.28284832e-01 7.39479374e-01\n",
      " 3.47698661e-01 9.07477196e-02 2.30920892e-02 1.98264814e-01\n",
      " 6.58073544e-01 5.49132629e-06 5.60304350e-08 6.30995115e-01\n",
      " 7.99444589e-04 9.38768669e-01 2.98265509e-03 3.01115498e-01\n",
      " 1.80588709e-01 1.46753644e+00 6.88778092e-01 3.27203869e-02\n",
      " 1.81282773e+00 6.59316476e-02 1.03344594e+00 4.31029954e-01\n",
      " 5.25959634e-01 2.54894327e-03 1.17204871e+00 4.67083267e-02\n",
      " 2.66731142e-03 1.36193321e-02 1.32338416e-01 4.55380465e-07\n",
      " 6.60000315e-03 1.75790704e-02 1.31856972e-07 7.09986101e-09\n",
      " 7.45672823e-03 4.81143943e-01 3.75186682e-01 1.83195837e-01\n",
      " 7.90579616e-01 1.61671277e+00 1.15432095e-02 3.50679558e-01\n",
      " 7.30223627e-02 6.04765728e-02 1.06464185e-02 3.45855821e-06\n",
      " 1.16028731e+00 1.41779826e-01 1.29928902e+00 1.38664305e+00\n",
      " 8.38885217e-02 3.29553268e-02 5.09662237e-09 7.82460795e-02\n",
      " 4.06346540e-01 1.40123018e-02 1.47426872e-01 5.13229266e-01\n",
      " 1.93069416e-03 1.00215902e-07 1.19574826e-01 1.51376228e-03\n",
      " 3.21726660e-02 5.97663752e-08 2.16412209e-02 5.59680885e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[5.03412896e-01 2.95704079e-07 9.79893401e-01 3.96672437e-01\n",
      " 5.26103653e-02 5.82829442e-05 4.13633607e-02 2.72256523e-01\n",
      " 8.94685099e-03 3.15872129e-01 9.28108955e-01 1.51007510e-05\n",
      " 1.22496286e-01 8.19722322e-01 4.47853358e-05 7.31313445e-01\n",
      " 3.91349836e-01 2.04296891e-01 8.88555825e-01 1.07768401e-02\n",
      " 2.13365794e-01 9.39879955e-01 5.69359218e-02 3.22391493e-01\n",
      " 2.11348197e-01 9.19488474e-01 1.63336303e-01 4.19189940e-01\n",
      " 3.50115824e-03 1.61641587e-01 3.53989041e-01 1.96656389e-01\n",
      " 3.71553709e-06 1.30655608e-01 7.28391568e-01 7.35326171e-01\n",
      " 2.22208919e-01 1.25404655e-01 3.36825865e-02 1.89871880e-01\n",
      " 4.79046495e-01 1.68892054e-06 2.18888274e-08 4.66134937e-01\n",
      " 1.17123729e-03 9.20719363e-01 1.35875391e-02 3.24648970e-01\n",
      " 1.83833515e-01 3.57250360e+00 6.04874518e-01 4.11494189e-02\n",
      " 1.72522746e+00 5.24755633e-02 1.13496511e+00 5.96271872e-01\n",
      " 4.76759932e-01 2.23056560e-03 9.08753312e-01 8.44962219e-02\n",
      " 7.81452981e-03 1.41511612e-02 7.77284944e-02 7.66765553e-07\n",
      " 7.69550604e-03 4.32408842e-02 4.10418338e-08 1.10899225e-08\n",
      " 8.73865042e-03 4.58507435e-01 3.66854712e-01 2.19634418e-01\n",
      " 8.51653372e-01 1.49595158e+00 2.23451087e-02 4.25852784e-01\n",
      " 1.47831468e-01 1.30697160e-01 2.88560637e-02 1.07580616e-06\n",
      " 1.13781420e+00 6.10323575e-02 8.67248496e-01 1.10188227e+00\n",
      " 4.63543565e-02 9.16083587e-02 5.61915987e-09 1.30091683e-01\n",
      " 2.87017275e-01 1.04994799e-02 1.58955821e-01 4.55507450e-01\n",
      " 3.97890936e-03 2.01425300e-07 1.56436540e-01 2.77320479e-03\n",
      " 5.81496240e-02 2.71797401e-08 5.10867803e-02 6.40439326e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[4.64736755e-01 5.78617179e-08 8.53659514e-01 3.63686809e-01\n",
      " 3.32263251e-02 3.15316249e-04 9.96089926e-02 2.62497840e-01\n",
      " 7.65827964e-03 2.92484108e-01 9.41342020e-01 3.76080854e-06\n",
      " 1.56250990e-01 8.82313396e-01 4.60418114e-06 7.27181401e-01\n",
      " 3.75272579e-01 2.33952315e-01 8.55046537e-01 8.49562759e-03\n",
      " 1.54440948e-01 9.51415281e-01 4.52520162e-02 3.17142037e-01\n",
      " 2.48870171e-01 2.34677565e+00 1.51327857e-01 4.07418200e-01\n",
      " 2.80438726e-03 1.20538965e-01 3.55890951e-01 1.60049403e-01\n",
      " 1.30172466e-06 1.51246069e-01 1.11780748e+00 7.34411499e-01\n",
      " 2.15573146e-01 9.96857150e-02 2.12243093e-02 1.77435568e-01\n",
      " 5.14621417e-01 6.68152886e-06 5.39840047e-08 4.82670698e-01\n",
      " 1.32410989e-03 8.98183370e-01 5.41036360e-03 3.88027980e-01\n",
      " 1.81352494e-01 1.48302079e+00 5.75032825e-01 2.57290688e-02\n",
      " 1.89903051e+00 3.48281285e-02 9.14129049e-01 4.57335573e-01\n",
      " 5.05320817e-01 2.25778142e-03 1.03621483e+00 5.76044821e-02\n",
      " 4.28337715e-03 9.15521028e-03 9.93825721e-02 2.34249485e-07\n",
      " 4.78059045e-03 3.22481644e-02 1.61492546e-07 3.20306496e-09\n",
      " 8.97970253e-03 4.67053737e-01 3.57696551e-01 2.02404595e-01\n",
      " 7.37305646e-01 1.48823732e+00 1.44317901e-02 6.13868815e-01\n",
      " 1.22427342e-01 1.04032895e-01 1.70193690e-02 4.56772180e-06\n",
      " 1.08693184e+00 8.72644376e-02 9.61267108e-01 1.88636576e+00\n",
      " 3.63007014e-02 6.67933765e-02 4.54597284e-09 9.32815896e-02\n",
      " 3.15983933e-01 7.68515446e-03 1.37854489e-01 4.96513193e-01\n",
      " 1.94196838e-03 2.04967128e-08 1.24241717e-01 1.37979415e-03\n",
      " 3.99270207e-02 6.87726659e-08 4.78689132e-02 4.55844387e-01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[1.81474483e+00 7.70589092e-08 9.09106633e-01 3.56352836e-01\n",
      " 4.04557615e-02 1.05172670e-04 6.62788568e-02 3.16537752e-01\n",
      " 1.63608721e-02 2.78108773e-01 8.89568359e-01 3.55283276e-06\n",
      " 1.37673285e-01 8.45611436e-01 7.01090281e-06 7.14584854e-01\n",
      " 3.46053475e-01 2.25146079e-01 8.01642433e-01 7.84423165e-03\n",
      " 7.20419128e-02 9.46797534e-01 5.56981334e-02 3.07424927e-01\n",
      " 2.40631821e-01 8.94152419e-01 1.16303021e-01 4.65355035e-01\n",
      " 2.30377239e-03 1.02890996e-01 3.28241754e-01 1.29256494e-01\n",
      " 1.72179075e-06 1.81323981e-01 6.91444039e-01 7.18335315e-01\n",
      " 3.93884897e-01 8.53314816e-02 2.19473019e-02 1.66894716e-01\n",
      " 7.56626088e-01 3.77680644e-06 1.98029909e-08 5.52091626e-01\n",
      " 5.60477266e-04 8.79357635e-01 2.52539433e-03 3.82773846e-01\n",
      " 1.58668838e-01 1.51987432e+00 5.79022912e-01 7.23732185e-02\n",
      " 1.89894681e+00 3.08909389e-02 9.81195879e-01 3.98222552e-01\n",
      " 5.42329114e-01 2.12117762e-03 1.23173306e+00 5.87624370e-02\n",
      " 1.56551799e-03 9.24086570e-03 1.33365622e-01 1.85873653e-07\n",
      " 4.64294973e-03 4.30912882e-02 7.92409347e-08 2.79252522e-09\n",
      " 8.20622536e-03 4.57215233e-01 3.46708101e-01 1.73788724e-01\n",
      " 7.81418898e-01 1.51911635e+00 8.27225724e-03 3.45372468e-01\n",
      " 6.32189992e-02 5.11871401e-02 2.18819730e-02 1.67620839e-06\n",
      " 1.06701695e+00 6.29264769e-02 2.18662991e+00 1.22879833e+00\n",
      " 6.62398676e-02 1.00611649e-01 2.89483552e-09 1.07524416e-01\n",
      " 3.01696821e-01 6.28568271e-03 1.34538752e-01 5.31719406e-01\n",
      " 3.46542727e-03 3.40170554e-08 1.09078639e-01 1.36282892e-03\n",
      " 3.72791037e-02 2.69872626e-08 4.17844831e-02 5.13066299e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[4.55134272e-01 4.00638165e-08 8.62822705e-01 3.61084814e-01\n",
      " 2.70657941e-02 2.00108285e-04 9.18894236e-02 2.49946891e-01\n",
      " 5.08020505e-03 2.79869589e-01 9.23218116e-01 2.31875957e-06\n",
      " 2.18523637e-01 1.03992499e+00 2.65945732e-06 7.35479239e-01\n",
      " 4.07004170e-01 2.53739084e-01 8.19027658e-01 7.78404836e-03\n",
      " 6.09969463e-01 9.83831445e-01 2.46385432e-02 2.76213563e-01\n",
      " 3.08334366e-01 8.45589366e-01 1.21257445e-01 3.70455684e-01\n",
      " 1.93780134e-03 1.11288462e-01 3.49176759e-01 1.55508038e-01\n",
      " 1.18442384e-06 1.35748964e-01 6.90587895e-01 8.96479456e-01\n",
      " 2.70931849e-01 9.05623256e-02 1.26260561e-02 1.70814826e-01\n",
      " 4.88077866e-01 6.12417908e-06 2.71833257e-08 4.54191253e-01\n",
      " 9.19836285e-04 8.69616541e-01 3.08213607e-03 3.83310555e-01\n",
      " 1.50828475e-01 1.50778748e+00 5.72541465e-01 1.71139917e-02\n",
      " 1.82938752e+00 4.05141120e-02 8.97211749e-01 4.12189036e-01\n",
      " 4.98509033e-01 1.73522710e-03 1.06545634e+00 6.99908030e-02\n",
      " 2.36417061e-03 6.86524477e-03 9.25517838e-02 1.27437858e-07\n",
      " 2.77682616e-03 1.52799860e-02 1.26193904e-07 2.05119359e-09\n",
      " 8.33696831e-03 4.78033880e-01 3.44988657e-01 1.39907960e-01\n",
      " 7.38314354e-01 1.45683811e+00 1.00824633e-02 3.79915271e-01\n",
      " 7.60666763e-01 8.89841748e-02 7.44365533e-03 2.79767745e-06\n",
      " 1.08550427e+00 1.30458449e-01 1.10497127e+00 1.18910181e+00\n",
      " 3.51690151e-02 4.19812629e-02 2.61396766e-09 5.18464684e-02\n",
      " 3.69612812e-01 5.04901185e-03 1.19169604e-01 4.93486383e-01\n",
      " 6.87747187e-04 1.29727905e-08 1.06445636e-01 4.68453340e-04\n",
      " 2.17325188e-02 3.79758027e-08 1.93144833e-02 4.49394768e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[4.41430656e-01 5.50692229e-08 9.01931767e-01 4.08661043e-01\n",
      " 1.52451602e-02 8.51002139e-05 5.87624228e-02 2.82208422e-01\n",
      " 5.37270250e-03 2.88106604e-01 9.00223877e-01 2.29594207e-06\n",
      " 2.14333805e-01 8.96849703e-01 3.84270071e-06 7.82959860e-01\n",
      " 3.38588415e-01 2.00239002e-01 9.33074837e-01 6.37182790e-03\n",
      " 7.71409986e-02 9.00611790e-01 3.16161742e-02 2.81117731e-01\n",
      " 3.06323391e-01 8.68259000e-01 1.04749430e-01 3.60759093e-01\n",
      " 1.74012151e-03 9.39421594e-02 3.41308540e-01 1.28030973e-01\n",
      " 1.69418968e-06 1.57368748e-01 7.76826596e-01 7.29090195e-01\n",
      " 2.15762650e-01 7.80371856e-02 1.74122781e-02 1.80765502e-01\n",
      " 5.02751677e-01 2.84857045e-06 1.28525259e-08 4.78449102e-01\n",
      " 4.74588112e-04 8.55099657e-01 1.67127151e-03 2.95950710e-01\n",
      " 1.37755559e-01 1.51275744e+00 6.08014190e-01 1.40837521e-02\n",
      " 1.86314581e+00 3.61300387e-02 9.58942517e-01 3.83509207e-01\n",
      " 5.11301535e-01 1.13925673e-03 1.18472980e+00 4.21117506e-02\n",
      " 1.33021107e-03 5.77522938e-03 1.09775884e-01 1.28727536e-07\n",
      " 3.17871036e-03 1.21289743e-02 5.63623381e-08 1.77146006e-09\n",
      " 4.21673597e-03 4.61602790e-01 3.51995817e-01 1.43125709e-01\n",
      " 7.78279886e-01 1.73706936e+00 7.35104839e-03 3.51633015e-01\n",
      " 4.76046689e-01 5.29998659e-02 7.90632863e-03 1.16875105e-06\n",
      " 1.20547936e+00 1.78962236e-01 9.02418731e-01 1.17793602e+00\n",
      " 5.27652236e-02 4.98817041e-02 2.29626217e-09 5.76621826e-02\n",
      " 3.14970390e-01 5.54604135e-03 1.04483916e-01 5.11229682e-01\n",
      " 6.56641621e-04 2.14724231e-08 1.10431435e-01 4.29185448e-04\n",
      " 2.78893686e-02 1.97449170e-08 3.02216706e-02 4.87411843e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[4.44407458e-01 4.76659846e-08 8.60801998e-01 3.99905478e-01\n",
      " 9.63379560e-03 1.46010124e-04 8.01395869e-02 2.54536196e-01\n",
      " 4.44696530e-03 2.78943177e-01 9.22471949e-01 1.51885653e-06\n",
      " 2.46415094e-01 9.24348812e-01 1.64608208e-06 7.79112398e-01\n",
      " 3.49574587e-01 2.09961025e-01 7.76508982e-01 5.19248621e-03\n",
      " 5.66927668e-02 8.65741339e-01 2.55578009e-02 2.67688342e-01\n",
      " 3.06189860e-01 8.77053144e-01 1.13568187e-01 6.79029982e-01\n",
      " 1.36116653e-03 9.68632272e-02 3.30239759e-01 1.23970550e-01\n",
      " 1.24332589e-06 1.80780496e-01 7.64423560e-01 7.32113164e-01\n",
      " 2.25678421e-01 7.27654390e-02 9.55003624e-03 1.73781944e-01\n",
      " 4.75454783e-01 4.91032482e-06 1.66728582e-08 4.93742513e-01\n",
      " 6.95723963e-04 8.37417318e-01 1.07937832e-03 3.13999108e-01\n",
      " 1.20719715e-01 1.55419547e+00 5.54106885e-01 1.20725925e-02\n",
      " 1.94558832e+00 2.93427879e-02 8.81891912e-01 3.51118287e-01\n",
      " 5.42005648e-01 1.05163819e-03 1.27934550e+00 3.28816017e-02\n",
      " 9.13753817e-04 4.59533379e-03 1.31779633e-01 8.63863439e-08\n",
      " 2.34196683e-03 8.56299659e-03 9.47431948e-08 1.44866661e-09\n",
      " 4.66154454e-03 4.67998834e-01 3.51844889e-01 1.35197836e-01\n",
      " 7.35101776e-01 1.49006664e+00 6.63810846e-03 3.24185461e-01\n",
      " 5.67657850e-02 3.93774358e-02 5.10401993e-03 5.22900297e-06\n",
      " 1.18311392e+00 1.39949913e-01 9.48013782e-01 1.20647856e+00\n",
      " 3.45953232e-01 4.43187668e-02 1.91016034e-09 4.65119593e-02\n",
      " 3.28257461e-01 4.34157076e-03 9.80765160e-02 5.42360095e-01\n",
      " 4.47759847e-04 9.17398045e-09 8.60019967e-02 2.70093641e-04\n",
      " 2.62678435e-02 2.45451570e-08 2.74535537e-02 4.33017567e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[4.39082162e-01 3.32139689e-08 8.74589675e-01 3.98542562e-01\n",
      " 9.05119036e-03 9.39021409e-05 6.37835187e-02 2.45184704e-01\n",
      " 4.14539925e-03 2.61271842e-01 9.29487406e-01 1.32095380e-06\n",
      " 2.47069386e-01 8.75182133e-01 1.68649285e-06 7.79908995e-01\n",
      " 3.20668337e-01 1.95073783e-01 9.20618930e-01 5.48146343e-03\n",
      " 9.85638515e-02 8.66715156e-01 3.43326274e-02 2.65710363e-01\n",
      " 3.09405615e-01 9.14541572e-01 1.14844142e-01 3.36404769e-01\n",
      " 1.44909497e-03 8.77962614e-02 3.32513643e-01 1.18782017e-01\n",
      " 1.42949792e-06 1.30191136e-01 7.70081448e-01 7.07599576e-01\n",
      " 2.11397723e-01 7.12874481e-02 8.86810084e-03 1.71903157e-01\n",
      " 4.62306930e-01 3.60134268e-06 1.06788528e-08 5.25765962e-01\n",
      " 4.94107526e-04 8.21618986e-01 1.86261905e-03 2.77780979e-01\n",
      " 1.15247369e-01 1.52788955e+00 5.42190054e-01 9.78345092e-03\n",
      " 1.85830565e+00 4.09070932e-02 8.98780670e-01 3.53242667e-01\n",
      " 4.94132297e-01 1.01916860e-03 1.10735950e+00 3.88304565e-02\n",
      " 1.95707490e-03 4.21962728e-03 8.71730569e-02 7.69388263e-08\n",
      " 2.40196388e-03 7.69640359e-03 6.66703643e-08 1.20656043e-09\n",
      " 3.84170636e-03 4.62361474e-01 3.35352470e-01 1.27454213e-01\n",
      " 7.47107135e-01 1.46663910e+00 7.21545871e-03 3.45166055e-01\n",
      " 8.29915595e-02 7.32008121e-02 5.06166770e-03 1.10891173e-06\n",
      " 1.18628948e+00 1.23755531e-01 8.73120703e-01 1.15331518e+00\n",
      " 3.29675304e-02 3.66514673e-02 1.74418171e-09 4.66919234e-02\n",
      " 3.05420521e-01 3.97158479e-03 9.12518194e-02 5.02709012e-01\n",
      " 3.78868937e-04 1.03103657e-08 9.01548032e-02 2.37531921e-04\n",
      " 2.33532287e-02 1.70349680e-08 3.14869142e-02 4.42695500e-01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[4.35430052e-01 3.41965713e-08 8.85875762e-01 4.05671649e-01\n",
      " 1.83111138e-02 7.09038584e-05 5.70356239e-02 5.43799404e-01\n",
      " 3.99635880e-03 2.58030234e-01 9.23567464e-01 1.22639266e-06\n",
      " 1.90958801e-01 8.52742309e-01 1.60929105e-06 7.98866029e-01\n",
      " 3.10824032e-01 2.03157360e-01 7.88561949e-01 4.63317174e-03\n",
      " 8.52267028e-02 8.48230215e-01 3.09375909e-02 2.44623015e-01\n",
      " 2.61724052e-01 8.95849547e-01 8.55104157e-02 3.29108760e-01\n",
      " 1.09284246e-03 8.56639834e-02 3.36483806e-01 1.41832957e-01\n",
      " 1.48385026e-06 1.31779862e-01 7.80774954e-01 7.26495811e-01\n",
      " 2.01207006e-01 6.37376307e-02 1.07814053e-02 1.72782093e-01\n",
      " 4.55465882e-01 2.50282687e-06 7.89962288e-09 4.75557139e-01\n",
      " 3.61807136e-04 8.13871479e-01 1.35235220e-03 2.97045969e-01\n",
      " 1.08071297e-01 1.55372306e+00 5.40179519e-01 9.01361983e-03\n",
      " 1.89678938e+00 2.38204656e-02 9.15329542e-01 4.24312128e-01\n",
      " 5.66544557e-01 1.04599655e-03 1.16197109e+00 3.80977831e-02\n",
      " 9.52493636e-04 3.63518367e-03 9.07012905e-02 7.26795560e-08\n",
      " 2.33826232e-03 1.22496813e-02 4.71237556e-08 1.02786651e-09\n",
      " 3.21610280e-03 4.55226753e-01 3.36324369e-01 1.22067833e-01\n",
      " 7.61848142e-01 1.47628714e+00 5.90846934e-03 3.38871536e-01\n",
      " 6.90098349e-02 5.88043240e-02 9.68107747e-03 7.72980428e-07\n",
      " 1.20468703e+00 6.98575641e-02 8.55921973e-01 1.33325689e+00\n",
      " 3.74828586e-02 5.95479595e-02 2.37423097e-09 7.78594635e-02\n",
      " 2.94502283e-01 3.50674094e-03 8.26709327e-02 5.24576403e-01\n",
      " 5.88406656e-04 1.03177515e-08 8.53624318e-02 3.90455056e-04\n",
      " 2.18659974e-02 1.25443463e-08 3.18318807e-02 4.50786451e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[4.34416379e-01 1.87572125e-08 8.39358955e-01 7.94616661e-01\n",
      " 8.33869708e-03 1.36419832e-04 8.31728803e-02 2.41594417e-01\n",
      " 4.03363624e-03 2.67047930e-01 9.30850541e-01 9.10880590e-07\n",
      " 2.38445081e-01 8.23271093e-01 7.43639457e-07 7.60773089e-01\n",
      " 2.98205365e-01 2.73407955e-01 7.55315118e-01 4.98246076e-03\n",
      " 7.87398083e-02 1.09553291e+00 3.38431414e-02 2.34049656e-01\n",
      " 3.10034390e-01 9.26934615e-01 7.81062159e-02 3.24347361e-01\n",
      " 1.11854547e-03 7.35090727e-02 3.57153348e-01 1.00004306e-01\n",
      " 1.05614037e-06 1.37270244e-01 6.77433903e-01 7.33542209e-01\n",
      " 1.90995361e-01 5.87623987e-02 7.98524641e-03 1.66487821e-01\n",
      " 4.50005345e-01 4.75683979e-06 1.13105740e-08 4.76382037e-01\n",
      " 6.19837319e-04 7.98575500e-01 1.15769694e-03 4.15312518e-01\n",
      " 9.95257300e-02 1.56256773e+00 5.38187871e-01 8.43184959e-03\n",
      " 1.91027199e+00 1.62131790e-02 8.37553657e-01 3.57834785e-01\n",
      " 5.11103093e-01 1.14803074e-03 1.19507921e+00 4.54492846e-02\n",
      " 1.37887746e-03 3.31527320e-03 9.34659507e-02 5.26191173e-08\n",
      " 2.36878903e-03 6.87413013e-03 8.78997407e-08 1.06110449e-09\n",
      " 8.87667566e-03 4.51668122e-01 3.35564735e-01 1.19860498e-01\n",
      " 7.10370270e-01 1.48237826e+00 5.26869128e-03 3.27506462e-01\n",
      " 6.64082734e-02 5.29405294e-02 4.84946800e-03 1.32785247e-06\n",
      " 1.09068260e+00 1.11060549e-01 8.40146670e-01 1.15623483e+00\n",
      " 3.85959503e-02 4.25597352e-02 1.25722890e-09 4.80101989e-02\n",
      " 2.78845023e-01 2.76900164e-03 7.63578253e-02 5.20341738e-01\n",
      " 2.93914704e-04 4.10166804e-09 8.96787988e-02 1.87760069e-04\n",
      " 2.44315839e-02 1.74278070e-08 3.42675670e-02 3.93122442e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[4.40785793e-01 4.91690320e-08 9.35960027e-01 4.24472005e-01\n",
      " 1.05868951e-02 2.48680697e-05 3.25788164e-02 2.42727914e-01\n",
      " 2.29743924e-03 2.38141664e-01 9.40940796e-01 1.53620658e-06\n",
      " 2.18568170e-01 9.82619968e-01 2.31290905e-06 8.19463859e-01\n",
      " 3.52818838e-01 1.62713225e-01 7.86379224e-01 4.68366723e-03\n",
      " 6.45752790e-02 6.82453827e-01 1.82286693e-02 2.21444164e-01\n",
      " 2.82987002e-01 8.98684160e-01 7.87060699e-02 3.09362920e-01\n",
      " 1.22915084e-03 7.50014335e-02 3.37079155e-01 1.04418095e-01\n",
      " 2.44957972e-06 1.60896517e-01 8.42782027e-01 7.97211082e-01\n",
      " 2.32799812e-01 5.87824846e-02 8.30689415e-03 1.75909739e-01\n",
      " 7.09224988e-01 1.13673997e-06 3.77419866e-09 4.73716358e-01\n",
      " 1.50818290e-04 7.99907717e-01 8.88394238e-04 2.21193938e-01\n",
      " 9.14980059e-02 1.58311044e+00 5.23465723e-01 9.75153810e-03\n",
      " 1.95299901e+00 2.53011985e-02 9.80117852e-01 3.63796507e-01\n",
      " 5.30018416e-01 8.73434092e-04 1.28181233e+00 2.36661045e-02\n",
      " 6.16261902e-04 2.66989043e-03 1.08618436e-01 8.74791450e-08\n",
      " 1.25413365e-03 8.30299484e-03 2.01005709e-08 1.32220921e-09\n",
      " 1.82843069e-03 4.70442990e-01 3.37997360e-01 9.41396911e-02\n",
      " 8.04080786e-01 1.45512885e+00 4.85537584e-03 3.25486159e-01\n",
      " 5.61506206e-02 3.71889936e-02 6.42419142e-03 2.90162102e-07\n",
      " 1.26483098e+00 7.91283890e-02 1.03193568e+00 1.13278221e+00\n",
      " 5.04410646e-02 5.19197156e-02 2.85751566e-09 6.07797548e-02\n",
      " 1.81192862e+00 3.01235507e-03 7.62460665e-02 5.36005610e-01\n",
      " 3.21829072e-04 1.70800914e-08 7.48419300e-02 1.98718109e-04\n",
      " 1.24669922e-02 6.68057033e-09 1.76440438e-02 4.97139596e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[4.47128426e-01 2.97933999e-08 8.96297346e-01 3.93127663e-01\n",
      " 1.79377196e-02 4.36842993e-05 4.33495554e-02 2.32944817e-01\n",
      " 2.22900119e-03 2.39760179e-01 9.75883210e-01 8.86292402e-07\n",
      " 1.75072662e-01 9.61315139e-01 1.15453824e-06 8.09538272e-01\n",
      " 3.15815015e-01 2.14224871e-01 7.30190981e-01 3.68783621e-03\n",
      " 6.77031271e-02 8.39442675e-01 2.19774710e-02 2.13792122e-01\n",
      " 2.43978457e-01 9.19587398e-01 8.05585016e-02 3.09570322e-01\n",
      " 8.33425468e-04 6.84769174e-02 3.53159283e-01 8.65231480e-02\n",
      " 1.60421711e-06 1.59079932e-01 7.74452488e-01 7.07717663e-01\n",
      " 2.18150095e-01 5.02566268e-02 1.14642980e-02 1.64566298e-01\n",
      " 4.40341306e-01 1.48118143e-06 4.19461442e-09 7.34528926e-01\n",
      " 2.27792735e-04 7.88432564e-01 9.54006923e-04 5.01200479e-01\n",
      " 8.99856762e-02 1.61004467e+00 5.39519654e-01 6.91210309e-03\n",
      " 1.97228219e+00 1.59806510e-02 9.23536054e-01 3.20105573e-01\n",
      " 5.31183726e-01 6.36175102e-04 1.26124443e+00 2.59505374e-02\n",
      " 5.73078805e-04 2.86035109e-03 1.03344304e-01 5.50180982e-08\n",
      " 1.11069923e-03 1.53324728e-02 2.65452112e-08 6.95315942e-10\n",
      " 3.76262108e-03 4.65250220e-01 3.24042707e-01 8.85398551e-02\n",
      " 7.72579014e-01 1.49520156e+00 4.86648931e-03 4.47190390e-01\n",
      " 6.50526201e-02 4.36131627e-02 1.35576397e-02 4.14945533e-07\n",
      " 2.15382268e+00 4.17272433e-02 1.00034404e+00 1.16889276e+00\n",
      " 4.18276670e-02 7.38462253e-02 1.40161513e-09 8.84466493e-02\n",
      " 3.32126774e-01 1.91799412e-03 6.69321808e-02 5.41310923e-01\n",
      " 4.63447918e-04 8.34372861e-09 7.75694714e-02 3.23038410e-04\n",
      " 1.23500357e-02 7.25146877e-09 1.95048368e-02 4.47139131e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[4.52400934e-01 1.52137982e-08 8.34964626e-01 4.15548461e-01\n",
      " 5.80258652e-03 1.10124238e-04 7.00195671e-02 2.26539026e-01\n",
      " 1.94839176e-03 2.39687684e-01 9.90027312e-01 5.82261626e-07\n",
      " 2.59462747e-01 9.02297832e-01 5.03322599e-07 8.33575683e-01\n",
      " 3.04184306e-01 2.02680783e-01 7.33598322e-01 3.18453206e-03\n",
      " 8.79847405e-02 8.33411494e-01 2.85455774e-02 2.04682780e-01\n",
      " 3.07367124e-01 9.02723841e-01 6.57891993e-02 3.09834385e-01\n",
      " 7.33072778e-04 6.31340407e-02 3.73213384e-01 8.39445208e-02\n",
      " 9.52120321e-07 1.40531144e-01 8.09455265e-01 7.13646940e-01\n",
      " 2.07869845e-01 4.80883683e-02 6.37605217e-03 1.66786636e-01\n",
      " 4.26527361e-01 2.86933564e-06 7.18052132e-09 4.43754142e-01\n",
      " 4.80438343e-04 7.76188121e-01 1.23467916e-03 2.61376022e-01\n",
      " 8.86501987e-02 1.59932030e+00 5.40858880e-01 6.52866423e-03\n",
      " 1.93437503e+00 1.73836819e-02 8.38656659e-01 2.97872505e-01\n",
      " 5.09681952e-01 5.46217054e-04 1.18792406e+00 2.96058676e-02\n",
      " 7.00733375e-04 2.04539358e-03 8.37276965e-02 3.90457542e-08\n",
      " 1.21945078e-03 5.85384957e-03 5.23405134e-08 6.84507408e-10\n",
      " 2.69623818e-03 4.54745633e-01 3.29620692e-01 7.41371289e-02\n",
      " 7.17334472e-01 1.49382578e+00 5.50146659e-03 3.09624612e-01\n",
      " 7.85581631e-02 5.62220931e-02 4.12185497e-03 8.46224927e-07\n",
      " 1.25022363e+00 9.15941401e-02 9.57877523e-01 1.14105343e+00\n",
      " 3.05913230e-02 3.87227596e-02 1.01022690e-09 4.41907708e-02\n",
      " 3.19877324e-01 1.86182258e-03 6.33485800e-02 5.33195542e-01\n",
      " 1.73795368e-04 2.89697111e-09 6.62231489e-02 1.04381088e-04\n",
      " 1.32745113e-02 1.18471905e-08 1.82638008e-02 3.79388034e-01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[4.60453723e-01 4.49384268e-08 8.22414932e-01 3.97789449e-01\n",
      " 5.10226426e-03 1.24995030e-04 7.30780427e-02 2.30606003e-01\n",
      " 1.68224168e-03 2.36075818e-01 1.01455167e+00 5.20050496e-07\n",
      " 2.64355850e-01 9.40202425e-01 4.26541417e-07 8.34383441e-01\n",
      " 3.11290050e-01 2.22892827e-01 7.17357475e-01 3.50240422e-03\n",
      " 7.10133166e-02 8.58818237e-01 2.36076142e-02 3.34027979e-01\n",
      " 3.16727129e-01 8.91003811e-01 6.30339412e-02 3.14786516e-01\n",
      " 6.77690730e-04 5.04510395e-02 3.59618647e-01 7.40585978e-02\n",
      " 8.77926796e-07 1.59361297e-01 7.86724821e-01 7.15209740e-01\n",
      " 2.18550268e-01 4.20055721e-02 7.93734082e-03 1.65158980e-01\n",
      " 4.43548092e-01 2.73665833e-06 7.30967118e-09 4.68120146e-01\n",
      " 4.94425957e-04 7.67279668e-01 8.08907756e-04 2.79334090e-01\n",
      " 8.63183453e-02 1.65289149e+00 5.51765168e-01 6.46142307e-03\n",
      " 2.00832910e+00 1.36981517e-02 8.26770668e-01 2.79491895e-01\n",
      " 5.38298821e-01 4.87502969e-04 1.28683990e+00 2.27980025e-02\n",
      " 4.67593374e-04 2.09233858e-03 9.99421269e-02 3.65553608e-08\n",
      " 9.49644572e-04 5.09106959e-03 5.03313581e-08 6.45133099e-10\n",
      " 3.44394763e-03 4.51017907e-01 3.28578062e-01 6.62057983e-02\n",
      " 7.10232549e-01 1.51483180e+00 4.37218663e-03 2.98165225e-01\n",
      " 6.97635434e-02 4.44661363e-02 3.49468396e-03 8.29076738e-07\n",
      " 1.22647285e+00 9.37150636e-02 1.03386822e+00 1.17481040e+00\n",
      " 3.74789778e-02 3.85852662e-02 9.61381861e-10 4.09144975e-02\n",
      " 3.40258457e-01 1.47163414e-03 5.82550485e-02 5.50053269e-01\n",
      " 1.94702412e-04 2.24123202e-09 6.50096217e-02 8.58946024e-05\n",
      " 1.03951094e-02 1.17373096e-08 1.33274287e-02 3.64156460e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[4.46333050e-01 1.95518398e-08 8.65464051e-01 4.44062124e-01\n",
      " 7.11591563e-03 5.82660740e-05 4.87674010e-02 2.30443594e-01\n",
      " 1.85397603e-03 2.26779962e-01 1.00006567e+00 5.07417525e-07\n",
      " 2.36241556e-01 8.83856917e-01 5.44149334e-07 8.68876323e-01\n",
      " 2.84194343e-01 1.72750595e-01 7.12285945e-01 3.70709813e-03\n",
      " 6.36733110e-02 6.90206777e-01 2.90716228e-02 2.03972652e-01\n",
      " 2.86662606e-01 9.02725073e-01 6.02114827e-02 3.05535366e-01\n",
      " 8.93412677e-04 4.97388136e-02 3.58228136e-01 7.45809892e-02\n",
      " 1.17940610e-06 1.51835864e-01 8.69295110e-01 7.07300180e-01\n",
      " 2.02209982e-01 4.33418657e-02 5.85595170e-03 1.81067328e-01\n",
      " 7.40957796e-01 1.25633653e-06 3.90679433e-09 4.85172277e-01\n",
      " 2.64018706e-04 7.69485646e-01 6.25169520e-04 1.98600760e-01\n",
      " 8.20840847e-02 1.66349975e+00 5.54808468e-01 5.43599125e-03\n",
      " 2.03027758e+00 2.11956141e-02 8.86749962e-01 3.15735701e-01\n",
      " 5.44621772e-01 5.54283085e-04 1.30140638e+00 2.65761407e-02\n",
      " 3.96002531e-04 1.72024030e-03 9.73820705e-02 3.67921465e-08\n",
      " 1.00374585e-03 6.18569059e-03 2.27000746e-08 4.55871104e-10\n",
      " 1.44166520e-03 4.44322214e-01 3.29825881e-01 6.09163579e-02\n",
      " 7.55208181e-01 1.50648167e+00 3.64254542e-03 2.99472422e-01\n",
      " 6.03889526e-02 4.65216073e-02 4.96275679e-03 4.03520858e-07\n",
      " 1.32142891e+00 6.82069139e-02 9.63323506e-01 1.14475898e+00\n",
      " 3.33188811e-02 4.47672371e-02 1.03188957e-09 5.40881502e-02\n",
      " 3.20265461e-01 1.87325733e-03 5.65032027e-02 5.54687640e-01\n",
      " 1.87327256e-04 3.72966938e-09 6.28663058e-02 1.06338757e-04\n",
      " 1.07325797e-02 6.72483873e-09 1.63026733e-02 4.03541282e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[4.33404313e-01 2.17612052e-08 8.81428072e-01 4.14423262e-01\n",
      " 5.64747165e-03 4.13668734e-05 4.12853116e-02 2.23692190e-01\n",
      " 1.82265488e-03 2.23598598e-01 9.99788915e-01 5.08155384e-07\n",
      " 2.59396226e-01 8.73745374e-01 5.65030264e-07 8.55491314e-01\n",
      " 2.75670912e-01 4.56260410e-01 7.36596577e-01 2.76011806e-03\n",
      " 6.35193657e-02 7.83071925e-01 2.99100780e-02 1.88178354e-01\n",
      " 3.07113014e-01 9.07532013e-01 6.58780642e-02 3.83274924e-01\n",
      " 5.87068269e-04 5.35142963e-02 3.70577239e-01 8.50281586e-02\n",
      " 1.41482350e-06 1.46185598e-01 8.24696370e-01 7.06030441e-01\n",
      " 2.01252428e-01 4.57820461e-02 6.67639147e-03 1.67659216e-01\n",
      " 3.97097581e-01 9.91034602e-07 2.95919834e-09 4.09881371e-01\n",
      " 1.91359613e-04 7.62019776e-01 5.74675849e-04 2.49347719e-01\n",
      " 7.73268853e-02 1.67246906e+00 5.50006073e-01 5.08753989e-03\n",
      " 2.04273988e+00 1.37094466e-02 9.02932928e-01 2.93597031e-01\n",
      " 5.45430320e-01 4.12158749e-04 1.29329990e+00 2.74081704e-02\n",
      " 3.79441788e-04 1.87931565e-03 9.42306351e-02 3.67846970e-08\n",
      " 1.68349641e-03 4.11124903e-03 1.74933582e-08 4.25867230e-10\n",
      " 2.32614192e-03 4.35541255e-01 3.18698875e-01 5.59732292e-02\n",
      " 7.67052724e-01 1.45889619e+00 3.26314663e-03 3.41861840e-01\n",
      " 5.64996612e-02 5.06778547e-02 2.85492582e-03 2.84622079e-07\n",
      " 1.26729810e+00 8.85559156e-02 9.56728799e-01 1.07435153e+00\n",
      " 2.82541976e-02 3.53086905e-02 1.10460475e-09 4.63247160e-02\n",
      " 3.16049970e-01 1.30425979e-03 5.73525231e-02 5.56124948e-01\n",
      " 1.59609234e-04 4.25120065e-09 6.31029571e-02 7.70261596e-05\n",
      " 1.04700369e-02 5.25712420e-09 1.55737413e-02 4.16073669e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[4.24806261e-01 1.36240952e-08 8.40165062e-01 4.25154555e-01\n",
      " 4.40203192e-03 7.01962710e-05 5.40163516e-02 2.30523365e-01\n",
      " 1.40642959e-03 2.12851872e-01 9.86261189e-01 3.66648002e-07\n",
      " 2.77724536e-01 9.28295649e-01 3.48692689e-07 8.69785788e-01\n",
      " 3.22283403e-01 1.89538289e-01 8.01133687e-01 2.98257713e-03\n",
      " 5.87677688e-02 7.30411738e-01 2.08829360e-02 1.92623958e-01\n",
      " 3.29895686e-01 9.02117025e-01 5.13689742e-02 2.98328923e-01\n",
      " 6.23239429e-04 4.67224507e-02 3.74822450e-01 7.45225406e-02\n",
      " 1.00256666e-06 1.43482590e-01 8.45696544e-01 6.94624161e-01\n",
      " 2.22286249e-01 3.77293838e-02 4.26992172e-03 1.73580439e-01\n",
      " 4.11295545e-01 1.49653321e-06 3.81823963e-09 4.42501057e-01\n",
      " 2.96488292e-04 7.47061446e-01 5.18126821e-04 2.19761293e-01\n",
      " 7.58363160e-02 1.66315237e+00 5.83104537e-01 4.53027170e-03\n",
      " 2.03796761e+00 1.40812466e-02 8.48587001e-01 2.66229599e-01\n",
      " 5.43212868e-01 3.83858998e-04 1.29711948e+00 2.01999402e-02\n",
      " 3.50061831e-04 2.12644423e-03 9.29445718e-02 2.76450716e-08\n",
      " 7.12820556e-04 3.26546117e-03 2.62772386e-08 4.08170941e-10\n",
      " 1.74513342e-03 4.42896895e-01 3.15472364e-01 5.49544278e-02\n",
      " 7.30224671e-01 1.49878779e+00 3.03022169e-03 2.97518033e-01\n",
      " 5.17869463e-02 4.72142235e-02 2.07566197e-03 4.28499829e-07\n",
      " 1.29637931e+00 9.58036265e-02 1.01510301e+00 1.12518463e+00\n",
      " 2.85887468e-02 2.88332916e-02 8.60798612e-10 4.00500391e-02\n",
      " 3.33966442e-01 1.24432675e-03 5.60438993e-02 5.59710987e-01\n",
      " 1.33481201e-04 2.18678780e-09 5.77750078e-02 6.12350635e-05\n",
      " 8.04843735e-03 6.75807776e-09 1.18243739e-02 3.72284660e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[4.29959609e-01 1.66725632e-08 8.64518185e-01 4.28264052e-01\n",
      " 6.97167276e-03 4.47440449e-05 4.22908021e-02 5.20824664e-01\n",
      " 8.63404783e-03 2.18657533e-01 1.00978147e+00 3.81380575e-07\n",
      " 2.37566474e-01 1.41391401e+00 3.96557107e-07 8.75818015e-01\n",
      " 2.39126705e-01 2.02435866e-01 6.88121615e-01 4.02018104e-03\n",
      " 4.94092091e-02 1.08444214e+00 3.28053937e-02 1.92676338e-01\n",
      " 2.86847287e-01 9.36593623e-01 4.79143542e-02 2.95140715e-01\n",
      " 5.41201744e-04 4.22046928e-02 3.55395334e-01 6.68198559e-02\n",
      " 1.53055016e-06 1.66995382e-01 8.37638848e-01 7.82319366e-01\n",
      " 1.87038014e-01 3.30457719e-02 4.60272016e-03 1.75168970e-01\n",
      " 4.23685420e-01 9.68904350e-07 2.67545897e-09 4.56629154e-01\n",
      " 2.04320498e-04 7.47323834e-01 4.02872816e-04 2.29144949e-01\n",
      " 7.22410975e-02 1.70517480e+00 5.48278763e-01 4.29075695e-03\n",
      " 2.10326946e+00 1.33268493e-02 9.94176117e-01 2.47696639e-01\n",
      " 5.71248302e-01 3.24831492e-04 1.38125928e+00 3.66508372e-02\n",
      " 2.84178913e-04 1.45072940e-03 1.09357132e-01 2.90792511e-08\n",
      " 1.25359323e-03 5.00895100e-03 1.69235840e-08 7.91875717e-10\n",
      " 1.85368038e-03 4.29290635e-01 3.13602417e-01 5.85876330e-02\n",
      " 7.54158297e-01 1.53702941e+00 2.74707462e-03 2.77895626e-01\n",
      " 4.68990011e-02 3.80708446e-02 3.71535486e-03 2.82566813e-07\n",
      " 1.29769718e+00 5.86137763e-02 8.46368616e-01 1.17132781e+00\n",
      " 3.38899321e-02 4.19470419e-02 9.47107932e-10 5.81687942e-02\n",
      " 2.76935615e-01 1.07651126e-03 5.71339898e-02 5.80197431e-01\n",
      " 1.75910105e-04 2.92246368e-09 5.36689260e-02 7.98215589e-05\n",
      " 1.32152980e-02 5.03718502e-09 2.14352668e-02 3.94469929e-01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[5.03128970e-01 1.95039159e-08 8.83001616e-01 4.35896174e-01\n",
      " 6.85995631e-03 2.98878477e-05 3.27778149e-02 2.31242742e-01\n",
      " 1.67515979e-03 2.21986035e-01 1.03067249e+00 4.13240274e-07\n",
      " 2.32061602e-01 8.24603427e-01 4.44829213e-07 8.89079175e-01\n",
      " 2.61316776e-01 2.05275106e-01 6.85800376e-01 2.30294541e-03\n",
      " 7.21646471e-02 7.87683553e-01 2.65412120e-02 1.83489124e-01\n",
      " 2.84545061e-01 2.03145206e+00 4.64425944e-02 2.88133088e-01\n",
      " 4.66364088e-04 4.17488129e-02 3.56351606e-01 6.79057563e-02\n",
      " 1.57581472e-06 1.42033915e-01 8.46249855e-01 7.12144999e-01\n",
      " 2.03721090e-01 3.23320973e-02 4.56148378e-03 1.75506318e-01\n",
      " 4.17521742e-01 7.21176180e-07 2.01180686e-09 4.51046064e-01\n",
      " 1.48371767e-04 7.42595192e-01 5.85697202e-04 2.32907833e-01\n",
      " 7.56658266e-02 1.70251515e+00 5.41659153e-01 4.29139952e-03\n",
      " 2.06601394e+00 1.27561379e-02 9.03069815e-01 2.40584684e-01\n",
      " 5.43445670e-01 2.88641618e-04 1.28803599e+00 2.81652985e-02\n",
      " 5.96877609e-04 1.36272029e-03 8.69009198e-02 3.18580370e-08\n",
      " 8.79400378e-04 5.37978365e-03 1.33065594e-08 3.31867727e-10\n",
      " 3.42308053e-03 4.90003744e-01 3.15943920e-01 4.97522128e-02\n",
      " 7.69443854e-01 1.53220192e+00 1.26020596e-02 2.75311122e-01\n",
      " 6.43402303e-02 5.51248391e-02 4.04319170e-03 1.97298178e-07\n",
      " 1.31777528e+00 5.18483122e-02 8.95567476e-01 1.16297232e+00\n",
      " 2.24638442e-02 4.85432718e-02 1.06689080e-09 5.79445755e-02\n",
      " 2.92355351e-01 9.86835062e-04 5.15360127e-02 5.66428426e-01\n",
      " 2.25175687e-04 3.68222148e-09 5.16354818e-02 7.73299790e-05\n",
      " 1.05493029e-02 7.24620797e-09 1.55689802e-02 4.10628659e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[4.30076916e-01 1.77350920e-08 8.82847812e-01 4.39092378e-01\n",
      " 6.21182723e-03 2.70024508e-05 3.24439834e-02 2.29839564e-01\n",
      " 1.19599963e-03 2.21354389e-01 1.02069161e+00 3.88510474e-07\n",
      " 2.36168407e-01 9.04884563e-01 4.06746850e-07 8.95168383e-01\n",
      " 2.95730640e-01 2.10606274e-01 6.87420147e-01 2.11954023e-03\n",
      " 6.41822703e-02 8.13622228e-01 2.00717602e-02 1.77942365e-01\n",
      " 2.97810942e-01 9.28414099e-01 4.45320383e-02 2.84921636e-01\n",
      " 4.18322341e-04 4.08557150e-02 3.67762791e-01 6.92965465e-02\n",
      " 1.49715907e-06 1.41437280e-01 8.42704968e-01 7.16211393e-01\n",
      " 2.24256030e-01 3.57863538e-02 4.31757968e-03 1.75753612e-01\n",
      " 4.02335748e-01 7.15159995e-07 1.83176391e-09 4.45985355e-01\n",
      " 1.38244313e-04 7.35334018e-01 4.65737499e-04 2.43560312e-01\n",
      " 6.57099271e-02 1.70346384e+00 5.39861364e-01 3.97440264e-03\n",
      " 2.07795810e+00 1.94115728e-02 8.97575619e-01 2.37577905e-01\n",
      " 5.46024927e-01 2.65069777e-04 1.31217288e+00 2.04449015e-02\n",
      " 4.27535812e-04 1.54472183e-03 8.73783464e-02 2.91099033e-08\n",
      " 5.90438928e-04 4.46057391e-03 1.20082190e-08 3.10893117e-10\n",
      " 1.62153498e-03 4.39870905e-01 3.14237743e-01 5.35800103e-02\n",
      " 7.66244889e-01 2.00811577e+00 2.68196981e-03 2.78394194e-01\n",
      " 5.45582113e-02 4.79496118e-02 3.69217229e-03 1.84111135e-07\n",
      " 1.32144435e+00 5.60004148e-02 9.82222910e-01 1.15211331e+00\n",
      " 2.41212187e-02 4.07944060e-02 1.00203785e-09 5.26846503e-02\n",
      " 3.22763006e-01 1.14189794e-03 4.70349268e-02 5.62200902e-01\n",
      " 1.37740663e-04 1.37416083e-08 4.94643185e-02 7.07044690e-05\n",
      " 7.52408664e-03 3.78933310e-09 1.22544317e-02 4.08029988e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[4.37457721e-01 1.62103890e-08 8.76027700e-01 4.54312799e-01\n",
      " 4.21034253e-03 2.65269864e-05 3.18471947e-02 2.27007740e-01\n",
      " 1.41286561e-03 2.13530026e-01 1.03031936e+00 3.48316493e-07\n",
      " 2.63302711e-01 8.54067634e-01 3.50395708e-07 9.08267753e-01\n",
      " 2.68816611e-01 1.91406950e-01 6.82185340e-01 2.51705564e-03\n",
      " 7.39174578e-02 7.35715887e-01 2.43275773e-02 1.72981234e-01\n",
      " 3.15502171e-01 9.40408049e-01 5.43760380e-02 2.81122391e-01\n",
      " 5.15930490e-04 4.47116268e-02 3.58866034e-01 6.94934675e-02\n",
      " 1.48102146e-06 1.30012708e-01 8.73406847e-01 7.03863187e-01\n",
      " 2.00090824e-01 3.09662281e-02 3.61808348e-03 1.83045181e-01\n",
      " 4.06145123e-01 6.95881523e-07 1.75077092e-09 4.41243389e-01\n",
      " 1.44174156e-04 7.28744177e-01 5.16697425e-04 1.96135945e-01\n",
      " 6.44564973e-02 1.70429429e+00 5.37285070e-01 3.72444624e-03\n",
      " 2.06811035e+00 1.37720694e-02 8.88821183e-01 2.32116867e-01\n",
      " 5.36044387e-01 2.55781659e-04 1.27803172e+00 2.53259104e-02\n",
      " 3.14626016e-04 1.50501638e-03 7.72544236e-02 2.71034951e-08\n",
      " 6.45337391e-04 3.25106873e-03 1.17391564e-08 2.88363444e-10\n",
      " 1.18680371e-03 4.45051183e-01 3.14873050e-01 4.92595266e-02\n",
      " 7.61466477e-01 1.52468579e+00 2.82499325e-03 2.73169527e-01\n",
      " 6.03362876e-02 5.64711205e-02 2.48018770e-03 1.84456722e-07\n",
      " 1.35868364e+00 6.89493382e-02 2.38025812e+00 1.15366002e+00\n",
      " 2.04522920e-02 3.10053033e-02 9.72587689e-10 4.19885458e-02\n",
      " 3.03533918e-01 1.07708859e-03 4.60804005e-02 5.55669503e-01\n",
      " 1.00777345e-04 2.98480879e-09 4.92771816e-02 5.03347668e-05\n",
      " 8.82263450e-03 3.78441317e-09 1.69165692e-02 4.11576802e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[4.42166037e-01 8.31116649e-09 8.11844568e-01 4.53361086e-01\n",
      " 3.30027941e-03 6.67762923e-05 5.45134418e-02 2.23643805e-01\n",
      " 9.95193998e-04 2.07596884e-01 1.04157237e+00 2.42201923e-07\n",
      " 2.79854638e-01 9.03479160e-01 2.36935318e-07 9.11405878e-01\n",
      " 2.98763747e-01 1.90740488e-01 6.94754184e-01 2.43353177e-03\n",
      " 7.93982826e-02 7.41411099e-01 1.81900933e-02 1.69797615e-01\n",
      " 3.23711539e-01 9.75233492e-01 4.20869571e-02 2.78926329e-01\n",
      " 4.93168816e-04 4.63109585e-02 3.58578471e-01 6.54921398e-02\n",
      " 8.34335549e-07 1.22072068e-01 8.75230787e-01 7.05260336e-01\n",
      " 2.17698810e-01 2.91212814e-02 3.30934614e-03 1.82396451e-01\n",
      " 4.02310712e-01 1.53149202e-06 2.98341530e-09 4.37069278e-01\n",
      " 3.03064189e-04 7.08137689e-01 5.14193184e-04 1.96986463e-01\n",
      " 6.35918627e-02 1.71556445e+00 5.35481864e-01 3.33996822e-03\n",
      " 2.11390897e+00 1.31626518e-02 8.09243841e-01 2.39454489e-01\n",
      " 5.30928037e-01 2.51116773e-04 1.26120647e+00 1.91761497e-02\n",
      " 3.00380164e-04 2.55722196e-03 7.19309050e-02 1.89301602e-08\n",
      " 5.88388928e-04 2.68461721e-03 2.61191120e-08 3.51080859e-10\n",
      " 1.07488168e-03 4.46574533e-01 3.12210926e-01 5.42554268e-02\n",
      " 7.00821087e-01 1.52146801e+00 2.86487535e-03 2.82484449e-01\n",
      " 6.32171431e-02 6.07888012e-02 1.90806318e-03 3.90955704e-07\n",
      " 1.35867750e+00 8.00171991e-02 9.72224754e-01 1.44682370e+00\n",
      " 1.81779316e-02 2.76551514e-02 6.36102531e-10 3.56738962e-02\n",
      " 3.24025982e-01 8.77743745e-04 4.49806194e-02 5.51381110e-01\n",
      " 9.14592658e-05 1.10835416e-09 4.79356965e-02 4.04475832e-05\n",
      " 7.02836158e-03 5.89381852e-09 1.20370692e-02 3.33023143e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[4.37430627e-01 7.68339271e-09 8.01834050e-01 4.59744192e-01\n",
      " 3.14100067e-03 7.20526306e-05 5.77081876e-02 2.25993587e-01\n",
      " 2.57153290e-03 2.09788750e-01 1.03008169e+00 2.34465081e-07\n",
      " 2.80783742e-01 9.35816395e-01 2.35067218e-07 9.22367714e-01\n",
      " 3.10847322e-01 1.93596689e-01 6.71510680e-01 2.25392300e-03\n",
      " 4.44102823e-01 7.59824709e-01 1.67105743e-02 1.69583170e-01\n",
      " 3.20947702e-01 9.24689805e-01 4.98848429e-02 2.80777363e-01\n",
      " 4.21790141e-04 3.60061418e-02 3.60683017e-01 6.20850501e-02\n",
      " 7.81686294e-07 1.23570357e-01 8.80584616e-01 7.12403932e-01\n",
      " 2.27626809e-01 2.68978128e-02 3.06757816e-03 1.84929438e-01\n",
      " 5.28461907e-01 1.56872325e-06 3.11105959e-09 4.45673605e-01\n",
      " 3.17610107e-04 7.00480011e-01 3.91517620e-04 1.99426807e-01\n",
      " 6.41990565e-02 1.72392326e+00 5.41546785e-01 3.30075228e-03\n",
      " 2.09815257e+00 1.31888579e-02 7.98600807e-01 2.21318536e-01\n",
      " 5.39231463e-01 2.26083633e-04 1.29508144e+00 1.64048725e-02\n",
      " 2.45359855e-04 1.31976263e-03 7.43874735e-02 1.83684727e-08\n",
      " 4.23452526e-04 2.66465151e-03 2.73103074e-08 3.62200325e-10\n",
      " 9.83644266e-04 4.52494724e-01 3.12756320e-01 4.26031851e-02\n",
      " 6.92005169e-01 1.54909053e+00 2.39321383e-03 2.67819872e-01\n",
      " 5.35861656e-02 5.42068648e-02 1.88940426e-03 4.04630896e-07\n",
      " 1.37193430e+00 7.60795428e-02 1.03268825e+00 2.15176780e+00\n",
      " 2.03392947e-02 2.75655744e-02 6.09619646e-10 3.65244989e-02\n",
      " 3.34762328e-01 8.88337827e-04 4.30386286e-02 5.59371675e-01\n",
      " 8.39055625e-05 9.35605059e-10 5.03524382e-02 3.77132619e-05\n",
      " 5.88916341e-03 6.04959305e-09 1.02338610e-02 3.22169209e-01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[4.31613570e-01 8.47901828e-09 8.14086402e-01 4.53151942e-01\n",
      " 3.38534319e-03 5.37202310e-05 4.84617669e-02 2.32928476e-01\n",
      " 1.10888034e-03 2.10759947e-01 1.02562805e+00 2.21171944e-07\n",
      " 2.72711872e-01 8.64394019e-01 2.24241377e-07 9.18342060e-01\n",
      " 2.72131010e-01 2.05540318e-01 6.62903505e-01 1.86076855e-03\n",
      " 5.46711622e-02 8.11266990e-01 1.94896943e-02 1.71359689e-01\n",
      " 3.10997131e-01 9.45286573e-01 3.68310540e-02 2.78403974e-01\n",
      " 3.31596487e-04 3.41853338e-02 3.56148758e-01 5.82956857e-02\n",
      " 8.83068936e-07 1.43451219e-01 8.66299739e-01 7.19650076e-01\n",
      " 2.03128854e-01 2.47525047e-02 2.97262398e-03 1.79750749e-01\n",
      " 4.14908860e-01 1.21351118e-06 2.42010589e-09 4.61353738e-01\n",
      " 2.53892503e-04 6.93656570e-01 2.79570482e-04 2.22011812e-01\n",
      " 6.30288778e-02 1.74496135e+00 5.48835111e-01 2.84087964e-03\n",
      " 2.14764538e+00 1.12188099e-02 8.12250245e-01 2.10531806e-01\n",
      " 5.65444233e-01 2.13059896e-04 1.37913550e+00 2.29068335e-02\n",
      " 2.05104427e-04 1.10608683e-03 8.92536519e-02 1.74515412e-08\n",
      " 5.96841069e-04 3.81987296e-03 2.10881447e-08 3.06139475e-10\n",
      " 1.09572915e-03 4.47854713e-01 3.09850389e-01 4.32959931e-02\n",
      " 7.02978519e-01 1.55600882e+00 1.97224978e-03 2.52925921e-01\n",
      " 4.31604373e-02 4.04685526e-02 2.09648396e-03 3.10513553e-07\n",
      " 1.35736128e+00 6.35288223e-02 9.10116930e-01 1.18154761e+00\n",
      " 2.65168769e-02 3.12197788e-02 1.15406916e-09 3.91858181e-02\n",
      " 3.05453656e-01 7.27352490e-04 4.10594789e-02 5.82585685e-01\n",
      " 1.09904505e-04 1.13268360e-09 4.52024933e-02 3.74131291e-05\n",
      " 7.81413267e-03 4.98076636e-09 1.47864925e-02 3.32288669e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[4.27694805e-01 1.11367184e-08 8.39442562e-01 4.40055850e-01\n",
      " 4.11897573e-03 3.35540294e-05 3.68251150e-02 2.30841421e-01\n",
      " 8.97604308e-04 2.21541886e-01 1.02063191e+00 2.34455619e-07\n",
      " 2.55081028e-01 9.13151855e-01 2.29298812e-07 9.17237338e-01\n",
      " 3.54638865e-01 2.34632977e-01 6.82513656e-01 1.57965546e-03\n",
      " 4.58471422e-02 9.18351160e-01 1.42992996e-02 1.65027616e-01\n",
      " 2.93956641e-01 9.33453335e-01 3.68216177e-02 2.75433893e-01\n",
      " 2.80249498e-04 3.41688412e-02 3.62694266e-01 6.13863067e-02\n",
      " 1.17120005e-06 1.54583491e-01 8.31066339e-01 7.36524072e-01\n",
      " 2.15351777e-01 2.53203456e-02 3.19542261e-03 1.77085461e-01\n",
      " 3.86930147e-01 7.93561860e-07 1.72828629e-09 4.40586135e-01\n",
      " 1.67273477e-04 6.91189432e-01 2.16325336e-04 2.57128796e-01\n",
      " 6.21265087e-02 1.75883457e+00 5.37630573e-01 2.61488290e-03\n",
      " 2.18478672e+00 8.95413032e-03 8.41923633e-01 2.18431833e-01\n",
      " 5.82038377e-01 1.84477871e-04 1.43329111e+00 1.77201691e-02\n",
      " 1.73990670e-04 1.47099494e-03 9.91107343e-02 1.88091818e-08\n",
      " 4.05952856e-04 4.74818914e-03 1.37829839e-08 2.54535573e-10\n",
      " 1.56622751e-03 4.55520094e-01 3.04841550e-01 6.86961578e-02\n",
      " 7.26949188e-01 1.53554074e+00 1.78154183e-03 2.65989668e-01\n",
      " 3.59341126e-02 3.34726680e-02 2.60720286e-03 1.99508494e-07\n",
      " 1.33088577e+00 5.24800831e-02 9.62474877e-01 1.14431537e+00\n",
      " 3.08160572e-02 3.28580207e-02 8.17386504e-10 4.59248774e-02\n",
      " 3.24342607e-01 5.96190465e-04 4.01843222e-02 5.95948564e-01\n",
      " 1.04850822e-04 1.65438173e-09 4.36099372e-02 4.27411915e-05\n",
      " 6.10773033e-03 3.78649114e-09 1.23273802e-02 3.56194328e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[4.23319655e-01 1.00787412e-08 8.34449706e-01 4.59243952e-01\n",
      " 3.92935321e-03 3.32293628e-05 3.69394869e-02 2.31879197e-01\n",
      " 9.28353158e-04 2.18492438e-01 1.02659520e+00 2.17417514e-07\n",
      " 2.53978274e-01 8.95104835e-01 2.15527619e-07 9.35950392e-01\n",
      " 2.96000023e-01 2.23017879e-01 7.73552068e-01 1.57745983e-03\n",
      " 4.49356583e-02 8.76093814e-01 1.52335102e-02 1.78948106e-01\n",
      " 2.84264166e-01 9.35032758e-01 3.59585498e-02 2.73085720e-01\n",
      " 2.72562046e-04 3.63182018e-02 9.07828231e-01 5.85120951e-02\n",
      " 1.12238816e-06 1.55592934e-01 8.58159016e-01 7.36779875e-01\n",
      " 2.10535199e-01 4.28843707e-02 3.73004874e-03 2.65961197e-01\n",
      " 3.95550792e-01 8.42413639e-07 1.65358319e-09 4.57761152e-01\n",
      " 1.66907402e-04 6.83864300e-01 2.14395594e-04 2.29502230e-01\n",
      " 5.98953863e-02 1.76324120e+00 5.44553943e-01 2.42292759e-03\n",
      " 2.18620476e+00 1.04005969e-02 8.92857995e-01 2.19886328e-01\n",
      " 5.81983858e-01 1.62367421e-04 2.24879333e+00 1.84895047e-02\n",
      " 1.78282812e-04 1.36000165e-03 1.00546258e-01 1.73268547e-08\n",
      " 4.02845724e-04 3.83853538e-03 1.43370687e-08 2.49717941e-10\n",
      " 1.18167168e-03 4.53613526e-01 3.07187074e-01 4.35471616e-02\n",
      " 7.20337719e-01 1.55199832e+00 1.73275703e-03 2.55834234e-01\n",
      " 3.67741232e-02 3.36975329e-02 2.81313397e-03 1.99172999e-07\n",
      " 1.37121344e+00 8.32313726e-02 9.36644008e-01 1.17629312e+00\n",
      " 2.92057128e-02 3.57201865e-02 7.72679266e-10 4.51493282e-02\n",
      " 3.18221674e-01 6.26390549e-04 4.28994412e-02 5.96520927e-01\n",
      " 1.04258066e-04 1.49106239e-09 4.14601026e-02 4.03153505e-05\n",
      " 6.68287695e-03 3.67517946e-09 1.32627902e-02 3.50191865e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[4.23209739e-01 1.00366943e-08 8.40094677e-01 4.51276784e-01\n",
      " 2.89792650e-03 2.90913445e-05 3.50517859e-02 2.30644552e-01\n",
      " 6.82690332e-04 2.17701636e-01 1.03693362e+00 2.14064651e-07\n",
      " 2.74362872e-01 9.63532397e-01 2.09435560e-07 9.33878337e-01\n",
      " 3.19197069e-01 2.30209507e-01 6.68745509e-01 1.47578424e-03\n",
      " 5.26791357e-02 9.06267027e-01 1.16782853e-02 1.59415140e-01\n",
      " 5.95021896e-01 9.29509656e-01 3.37222760e-02 2.67753962e-01\n",
      " 3.22169602e-04 3.27662859e-02 3.57403670e-01 5.91748988e-02\n",
      " 1.12394724e-06 1.44500522e-01 8.45618048e-01 7.34490256e-01\n",
      " 2.32022315e-01 2.30325838e-02 2.76083971e-03 1.81866596e-01\n",
      " 3.94704390e-01 7.78709480e-07 1.48736442e-09 4.52766830e-01\n",
      " 1.51072630e-04 6.82478536e-01 2.42936924e-04 2.47241931e-01\n",
      " 5.70458349e-02 1.76554028e+00 5.40669283e-01 2.33476745e-03\n",
      " 2.17247264e+00 8.72764789e-03 8.38227256e-01 2.05340702e-01\n",
      " 5.70339108e-01 2.18450092e-04 1.39714037e+00 1.54529295e-02\n",
      " 1.73979147e-04 9.11971086e-04 9.15647253e-02 1.68234731e-08\n",
      " 3.03016592e-04 3.12764142e-03 1.30886370e-08 2.34378086e-10\n",
      " 1.30982972e-03 4.61156583e-01 3.03245085e-01 4.16701505e-02\n",
      " 7.23618742e-01 1.70584731e+00 1.74105832e-03 2.55429494e-01\n",
      " 4.24411475e-02 4.05465096e-02 2.23000611e-03 1.77901180e-07\n",
      " 1.35814314e+00 5.58407855e-02 1.04233402e+00 1.16390492e+00\n",
      " 2.36025029e-02 2.85785764e-02 7.57025968e-10 3.75350751e-02\n",
      " 3.47163336e-01 5.41042004e-04 6.38965084e-02 5.88277408e-01\n",
      " 7.63157097e-05 1.52806123e-09 4.02596881e-02 2.99119798e-05\n",
      " 4.31927248e-03 3.33788815e-09 9.65786243e-03 3.53596467e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[4.22396829e-01 9.76609049e-09 8.38964767e-01 4.71643562e-01\n",
      " 2.92401164e-03 2.77441677e-05 3.39538982e-02 2.17461919e-01\n",
      " 6.36307070e-04 2.11572953e-01 1.04135595e+00 2.05999129e-07\n",
      " 2.71043508e-01 9.61687253e-01 2.02389254e-07 9.51657628e-01\n",
      " 3.15072680e-01 2.08428729e-01 7.04210820e-01 1.51512369e-03\n",
      " 4.12783264e-02 8.56230920e-01 1.26723318e-02 1.41386333e-01\n",
      " 2.96376918e-01 9.35091725e-01 3.63628140e-02 5.78168365e-01\n",
      " 2.60775802e-04 3.46956068e-02 3.68276253e-01 6.57275791e-02\n",
      " 1.12942834e-06 1.62825254e-01 8.86128639e-01 7.28873422e-01\n",
      " 2.31699647e-01 2.58649262e-02 2.69228221e-03 2.57671255e-01\n",
      " 3.52819251e-01 7.44228986e-07 1.40264006e-09 4.10874959e-01\n",
      " 1.45199578e-04 6.77006401e-01 1.79166193e-04 2.13041901e-01\n",
      " 5.24137540e-02 1.79122274e+00 5.17658059e-01 2.22621383e-03\n",
      " 2.22444092e+00 1.08767054e-02 8.37019242e-01 2.11814016e-01\n",
      " 5.95279553e-01 1.56616683e-04 1.47511624e+00 1.31919034e-02\n",
      " 1.41658953e-04 8.32065703e-04 1.06632631e-01 1.63858141e-08\n",
      " 2.88829801e-04 4.21606643e-03 1.24703295e-08 2.25225782e-10\n",
      " 9.05641725e-04 4.61912634e-01 3.09744266e-01 3.87344601e-02\n",
      " 7.22783211e-01 1.49060692e+00 1.56406197e-03 2.74872400e-01\n",
      " 3.46634549e-02 3.19174326e-02 2.24886534e-03 1.68597409e-07\n",
      " 1.40445112e+00 5.33833090e-02 1.09410920e+00 1.10387401e+00\n",
      " 2.91583306e-02 3.26376616e-02 7.55176698e-10 3.79649823e-02\n",
      " 3.47878837e-01 6.31448093e-04 3.95196178e-02 6.10054182e-01\n",
      " 7.77420130e-05 1.47734542e-09 3.95223487e-02 2.89313053e-05\n",
      " 4.21821362e-03 3.21565613e-09 9.61429565e-03 3.50877834e-01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[4.28001254e-01 9.01828622e-09 8.31573067e-01 4.69372238e-01\n",
      " 2.19953401e-03 2.95701834e-05 3.58747031e-02 2.29806692e-01\n",
      " 6.20531039e-04 2.09169252e-01 1.05645100e+00 1.92928686e-07\n",
      " 2.90227284e-01 9.65959822e-01 1.94535186e-07 9.54481570e-01\n",
      " 3.13190867e-01 2.09236423e-01 6.56737160e-01 1.47904442e-03\n",
      " 5.33833503e-02 8.15530991e-01 1.21268580e-02 1.57986636e-01\n",
      " 3.07250295e-01 9.43525612e-01 3.19745056e-02 2.64309936e-01\n",
      " 3.04269346e-04 3.13697292e-02 3.56227480e-01 5.51412821e-02\n",
      " 1.06658098e-06 1.46490164e-01 8.87446377e-01 7.23011623e-01\n",
      " 2.28076747e-01 4.00058523e-02 2.37040585e-03 1.83868037e-01\n",
      " 3.85672098e-01 8.01073485e-07 1.42837770e-09 4.57797101e-01\n",
      " 1.50349139e-04 6.69591702e-01 2.41593163e-04 2.15982378e-01\n",
      " 1.21448181e-01 1.78406516e+00 1.24489491e+00 2.09699796e-03\n",
      " 2.18720679e+00 9.82229909e-03 8.26855932e-01 1.88363880e-01\n",
      " 5.73442195e-01 1.46996281e-04 1.40810685e+00 1.31072975e-02\n",
      " 1.54422207e-04 8.58450001e-04 9.15972842e-02 1.50844149e-08\n",
      " 2.70970337e-04 3.37240713e-03 1.34301077e-08 2.29977426e-10\n",
      " 9.07031903e-04 4.66477311e-01 3.06540949e-01 3.85559577e-02\n",
      " 7.14931244e-01 1.55507913e+00 1.68208738e-03 2.42714940e-01\n",
      " 4.56310037e-02 4.15044397e-02 1.69888858e-03 1.75237468e-07\n",
      " 1.40714725e+00 6.28635010e-02 1.03257239e+00 1.18558861e+00\n",
      " 2.19670062e-02 2.63207788e-02 7.25366627e-10 3.09718392e-02\n",
      " 3.49079452e-01 5.72680172e-04 3.89652555e-02 5.92998804e-01\n",
      " 6.24807600e-05 1.31280452e-09 3.86613258e-02 2.28591887e-05\n",
      " 4.66141466e-03 3.23635770e-09 1.02369275e-02 3.42683507e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[4.27174862e-01 8.24345230e-09 8.24510878e-01 4.89851292e-01\n",
      " 1.86991147e-03 3.10407722e-05 3.77915784e-02 2.19059619e-01\n",
      " 5.75329696e-04 2.00789744e-01 1.06259078e+00 3.62951417e-07\n",
      " 3.00747425e-01 9.78294139e-01 1.87909525e-07 9.69573366e-01\n",
      " 3.20465388e-01 1.83210387e-01 6.70216638e-01 1.76593977e-03\n",
      " 5.07911603e-02 7.23599368e-01 1.07835635e-02 1.46007487e-01\n",
      " 3.14761862e-01 9.49750031e-01 3.42025827e-02 2.58590515e-01\n",
      " 3.08311653e-04 3.14715157e-02 3.68797614e-01 5.76163778e-02\n",
      " 1.00339184e-06 1.49495015e-01 9.33236275e-01 7.16896226e-01\n",
      " 2.31694613e-01 2.25328634e-02 2.22163813e-03 1.93084452e-01\n",
      " 3.63788366e-01 8.89823374e-07 1.43510110e-09 4.23643691e-01\n",
      " 1.54056319e-04 6.60323094e-01 2.22068828e-04 1.74134987e-01\n",
      " 5.59816694e-02 1.79490578e+00 6.44626661e-01 1.93556671e-03\n",
      " 2.20363861e+00 1.29859971e-02 8.15454115e-01 2.04780731e-01\n",
      " 5.77868181e-01 1.65752482e-04 1.42610366e+00 1.22801635e-02\n",
      " 1.37529922e-04 8.96386911e-04 9.70565008e-02 1.38483920e-08\n",
      " 3.08799780e-04 2.78836481e-03 1.47972417e-08 2.39347098e-10\n",
      " 5.83947585e-04 4.70711071e-01 3.15469773e-01 3.91091571e-02\n",
      " 7.05260402e-01 1.51155150e+00 1.60345792e-03 2.60126821e-01\n",
      " 4.48167346e-02 3.88481726e-02 1.41479724e-03 1.81543735e-07\n",
      " 1.45439543e+00 6.76794678e-02 1.04443240e+00 1.13288562e+00\n",
      " 2.26893999e-02 2.28509343e-02 6.88637424e-10 2.76023840e-02\n",
      " 3.53920521e-01 6.91586658e-04 3.73638122e-02 5.97108342e-01\n",
      " 5.47646600e-05 1.15665162e-09 4.26790943e-02 2.01550513e-05\n",
      " 4.39564875e-03 3.20603683e-09 1.02661612e-02 3.34141691e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[4.24465629e-01 8.14475280e-09 8.22956494e-01 4.75938341e-01\n",
      " 2.61173402e-03 3.05900006e-05 3.65553484e-02 2.26901330e-01\n",
      " 6.49356113e-04 3.87702708e-01 1.06271247e+00 3.44927818e-07\n",
      " 2.72459519e-01 9.21693391e-01 1.84229354e-07 9.61456236e-01\n",
      " 2.93334727e-01 2.02994569e-01 6.61080982e-01 1.48515261e-03\n",
      " 5.06637856e-02 9.91464539e-01 1.35706129e-02 1.50735532e-01\n",
      " 2.92096788e-01 9.54172533e-01 3.13225444e-02 2.57020802e-01\n",
      " 2.49842670e-04 3.15268578e-02 3.62228336e-01 5.61205981e-02\n",
      " 1.02649997e-06 1.47324035e-01 1.49114896e+00 7.19115032e-01\n",
      " 2.18270857e-01 2.07247122e-02 2.48537116e-03 1.85398725e-01\n",
      " 4.85976817e-01 8.22529039e-07 1.37534440e-09 4.39558579e-01\n",
      " 1.51349768e-04 6.57521259e-01 2.16245137e-04 1.97197055e-01\n",
      " 5.22050629e-02 1.79761264e+00 5.31638635e-01 1.89932540e-03\n",
      " 2.20848530e+00 1.01093138e-02 8.15807461e-01 1.90708777e-01\n",
      " 5.77682597e-01 1.43337000e-04 1.42366906e+00 1.47169224e-02\n",
      " 1.32907982e-04 7.53352205e-04 9.37984769e-02 1.36222167e-08\n",
      " 2.79503389e-04 3.91752864e-03 1.37243357e-08 2.27114771e-10\n",
      " 7.67730376e-04 4.59490646e-01 3.06335881e-01 3.49859872e-02\n",
      " 7.05450017e-01 1.53051091e+00 1.56664391e-03 2.48573641e-01\n",
      " 4.40728467e-02 3.97463183e-02 2.11311618e-03 1.72488230e-07\n",
      " 1.42268523e+00 4.86405450e-02 9.81240322e-01 1.15074236e+00\n",
      " 2.14220529e-02 3.24421846e-02 6.92337450e-10 3.54493583e-02\n",
      " 3.33109358e-01 1.10855469e-03 3.64042848e-02 5.97222294e-01\n",
      " 7.09882581e-05 1.12056641e-09 3.72654714e-02 2.47443643e-05\n",
      " 4.54417037e-03 3.14362865e-09 1.18021973e-02 3.31667745e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[4.24141512e-01 7.74089401e-09 8.19458655e-01 4.62832931e-01\n",
      " 2.22594563e-03 3.08127236e-05 3.64296902e-02 2.31439621e-01\n",
      " 5.45127259e-04 2.02690372e-01 1.06610038e+00 1.63856784e-07\n",
      " 2.80287496e-01 9.51010544e-01 1.80397039e-07 9.52848890e-01\n",
      " 3.12756798e-01 2.13810171e-01 6.54059617e-01 1.35389169e-03\n",
      " 4.68814567e-02 8.37300832e-01 1.15544468e-02 1.57386522e-01\n",
      " 3.00741258e-01 9.47031649e-01 2.96046426e-02 2.60165599e-01\n",
      " 2.33963502e-04 2.88021940e-02 3.60027345e-01 5.24573770e-02\n",
      " 1.00898247e-06 1.21451804e+00 8.78295621e-01 7.16366145e-01\n",
      " 2.30049117e-01 1.91041722e-02 2.34620346e-03 1.79438066e-01\n",
      " 3.83261478e-01 8.23406551e-07 1.34404312e-09 5.60568916e-01\n",
      " 1.52944045e-04 6.53509757e-01 1.86995936e-04 2.13570110e-01\n",
      " 5.75825206e-02 1.81397000e+00 5.44128309e-01 1.83573879e-03\n",
      " 2.23408092e+00 8.27044849e-03 8.11822705e-01 1.84697400e-01\n",
      " 5.87629790e-01 1.36425425e-04 1.45515419e+00 1.23040526e-02\n",
      " 1.20407833e-04 7.56407129e-04 9.90086812e-02 1.31147946e-08\n",
      " 2.43985577e-04 2.88528009e-03 1.36423702e-08 2.23075294e-10\n",
      " 8.93028004e-04 4.62220417e-01 2.98235603e-01 3.45877895e-02\n",
      " 7.02261694e-01 1.55298047e+00 1.42433525e-03 2.42776606e-01\n",
      " 3.19076245e-01 3.75711348e-02 1.86963453e-03 1.71898103e-07\n",
      " 1.39944905e+00 5.24447784e-02 1.02270719e+00 1.17114762e+00\n",
      " 2.21494879e-02 3.00413208e-02 7.04574786e-10 3.14173994e-02\n",
      " 3.45556451e-01 4.53962366e-04 3.50912434e-02 6.06461046e-01\n",
      " 5.99897664e-05 1.04318223e-09 4.21745232e-02 2.13602992e-05\n",
      " 4.05629448e-03 3.11652860e-09 9.98846914e-03 3.26861871e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[4.22161251e-01 8.72106162e-09 8.30308250e-01 4.64451544e-01\n",
      " 1.45811831e-03 2.50080120e-05 3.19518530e-02 2.24594847e-01\n",
      " 6.03239056e-04 2.00278552e-01 1.06321495e+00 1.86777812e-07\n",
      " 3.09833004e-01 9.14428565e-01 1.74502589e-07 9.57255327e-01\n",
      " 2.94184413e-01 2.13147814e-01 6.66846952e-01 1.31931748e-03\n",
      " 8.90840440e-02 8.36515324e-01 1.34467345e-02 1.45808218e-01\n",
      " 3.23153862e-01 9.49112817e-01 3.80576448e-02 2.59836943e-01\n",
      " 2.15849433e-04 2.93090808e-02 3.68263535e-01 5.70659409e-02\n",
      " 1.12625329e-06 1.68010666e-01 8.81682427e-01 7.15069831e-01\n",
      " 2.23942866e-01 2.03348477e-02 2.08586279e-03 1.79406569e-01\n",
      " 3.62313839e-01 6.65007473e-07 1.16291959e-09 4.30481686e-01\n",
      " 1.30912665e-04 6.54183679e-01 1.31728662e-04 2.11905009e-01\n",
      " 5.10401860e-02 1.83715695e+00 5.31989422e-01 4.21529089e-03\n",
      " 2.28636592e+00 8.12775118e-03 8.24480614e-01 1.90977841e-01\n",
      " 6.10766555e-01 1.34987837e-04 1.52750641e+00 1.38756907e-02\n",
      " 1.04484477e-04 8.78482510e-04 1.12429330e-01 1.37211563e-08\n",
      " 2.58506797e-04 1.95624638e-03 1.10110897e-08 1.99902900e-10\n",
      " 8.52844380e-04 4.52987109e-01 2.97678173e-01 3.20221165e-02\n",
      " 7.13319832e-01 1.52361220e+00 1.27668291e-03 2.57455456e-01\n",
      " 3.17192507e-02 2.96011960e-02 1.23049901e-03 1.41395375e-07\n",
      " 1.40526545e+00 6.90178907e-02 9.79542152e-01 1.13468762e+00\n",
      " 2.73819501e-02 2.10154233e-02 1.80393734e-09 2.27569872e-02\n",
      " 3.31774785e-01 4.32171586e-04 3.49164723e-02 6.26290207e-01\n",
      " 4.31219404e-05 1.18747247e-09 3.64328186e-02 2.76349780e-05\n",
      " 4.21774326e-03 2.99498001e-09 1.04757185e-02 3.35997552e-01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[4.23439851e-01 7.23651789e-09 8.16345614e-01 4.58538186e-01\n",
      " 1.33724502e-03 3.03350439e-05 3.64050510e-02 2.25280456e-01\n",
      " 6.81883206e-04 2.01017552e-01 1.07565064e+00 1.50862198e-07\n",
      " 3.13929964e-01 8.80332808e-01 1.72599540e-07 9.55247363e-01\n",
      " 2.77486435e-01 2.18703291e-01 6.53914119e-01 1.27914213e-03\n",
      " 5.10025205e-02 8.48250152e-01 1.51326096e-02 1.49619290e-01\n",
      " 3.23571430e-01 9.55557611e-01 2.90845125e-02 2.58333545e-01\n",
      " 2.07359880e-04 2.84470162e-02 3.63997908e-01 5.41218210e-02\n",
      " 9.56528581e-07 1.43431428e-01 8.73601840e-01 7.19099458e-01\n",
      " 2.14075521e-01 1.89747338e-02 1.98451923e-03 1.77521003e-01\n",
      " 3.73895021e-01 7.77620993e-07 1.26479078e-09 4.42467110e-01\n",
      " 1.53576842e-04 6.58786651e-01 1.92044074e-04 2.17460168e-01\n",
      " 6.20978622e-02 1.81833824e+00 5.37342953e-01 1.73839179e-03\n",
      " 2.22603437e+00 7.17078201e-03 8.08740246e-01 3.09667022e-01\n",
      " 5.79778235e-01 1.29647052e-04 1.43134444e+00 1.56006115e-02\n",
      " 1.15547494e-04 8.58079010e-04 9.12648639e-02 1.22763681e-08\n",
      " 2.69215630e-04 2.02051353e-03 1.28750932e-08 2.11581932e-10\n",
      " 9.78114469e-04 4.47338134e-01 2.94613804e-01 3.08491446e-02\n",
      " 6.99885161e-01 1.54271827e+00 1.38107252e-03 2.45949841e-01\n",
      " 4.38360428e-02 4.30684122e-02 1.15595949e-03 1.64620616e-07\n",
      " 1.39690496e+00 7.00532002e-02 1.03697676e+00 1.16391205e+00\n",
      " 1.87159069e-02 2.03891123e-02 6.38803898e-10 2.14582503e-02\n",
      " 3.21068832e-01 3.76928942e-04 3.43487967e-02 6.00744420e-01\n",
      " 4.15916249e-05 9.28888834e-10 3.57742493e-02 1.46485447e-05\n",
      " 4.51243245e-03 4.80270311e-09 1.19186258e-02 3.20933212e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[4.19244185e-01 7.80126644e-09 8.25141999e-01 4.67564360e-01\n",
      " 2.21969767e-03 2.54391397e-05 3.32296817e-02 2.23015203e-01\n",
      " 6.53460997e-04 1.93833726e-01 1.06705022e+00 1.54691720e-07\n",
      " 3.23456941e-01 8.96257582e-01 1.66848913e-07 9.64667194e-01\n",
      " 2.83034227e-01 2.06768021e-01 6.80051833e-01 1.34217092e-03\n",
      " 3.68194823e-02 8.99325292e-01 1.34832974e-02 1.57743218e-01\n",
      " 3.28840797e-01 9.60716336e-01 2.89733233e-02 3.00378059e-01\n",
      " 2.28023050e-04 2.85158584e-02 3.67606946e-01 5.27984435e-02\n",
      " 1.03943770e-06 1.64775316e-01 8.94153948e-01 7.07347667e-01\n",
      " 2.16543988e-01 1.92396216e-02 1.93233051e-03 1.80441397e-01\n",
      " 3.62118090e-01 6.91943276e-07 1.11890129e-09 4.33516113e-01\n",
      " 1.31179433e-04 6.46045323e-01 1.26771184e-04 1.99309825e-01\n",
      " 4.99040571e-02 1.84032724e+00 5.39468898e-01 1.66430626e-03\n",
      " 2.28399294e+00 7.93626346e-03 8.16314636e-01 2.20994905e-01\n",
      " 6.08533742e-01 1.25852238e-04 1.52498372e+00 1.47107617e-02\n",
      " 9.72964657e-05 8.70637437e-04 1.10258691e-01 1.24112570e-08\n",
      " 2.49670583e-04 1.65556823e-03 1.13826074e-08 1.97564826e-10\n",
      " 7.83121820e-04 4.51151468e-01 2.96533560e-01 3.08134912e-02\n",
      " 7.06267243e-01 1.53022698e+00 1.19882904e-03 2.51644216e-01\n",
      " 3.24198514e-02 3.07797882e-02 9.91869811e-04 1.40572603e-07\n",
      " 1.41851689e+00 7.61364572e-02 9.66901695e-01 1.15427041e+00\n",
      " 2.56313298e-02 1.85247173e-02 6.83863853e-10 1.90587606e-02\n",
      " 3.26698100e-01 4.03829874e-04 3.36281910e-02 6.24459470e-01\n",
      " 3.60677452e-05 1.04054206e-09 3.49743218e-02 1.30703960e-05\n",
      " 4.27210803e-03 2.71700601e-09 1.15523024e-02 3.28572909e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[4.19178607e-01 7.92248642e-09 8.27621639e-01 4.97082464e-01\n",
      " 1.54324524e-03 2.34336402e-05 3.15556112e-02 2.25164063e-01\n",
      " 4.96106070e-04 1.84288211e-01 1.07660782e+00 1.52211029e-07\n",
      " 2.96938817e-01 9.44498060e-01 1.63755289e-07 9.90955961e-01\n",
      " 3.09831450e-01 1.75036142e-01 6.47714937e-01 1.96736886e-03\n",
      " 4.54375452e-02 6.93840816e-01 1.07502798e-02 1.45833043e-01\n",
      " 3.07784256e-01 9.52776922e-01 2.80352523e-02 2.55949666e-01\n",
      " 2.92968770e-04 3.31481666e-02 3.66667768e-01 5.17441584e-02\n",
      " 1.08304738e-06 1.51477095e-01 9.55492509e-01 1.03244576e+00\n",
      " 2.31489023e-01 1.81997273e-02 1.96111327e-03 1.94647986e-01\n",
      " 3.65279565e-01 6.42962731e-07 1.04625612e-09 4.39752830e-01\n",
      " 1.21607416e-04 6.43281131e-01 1.55453759e-04 1.56649793e-01\n",
      " 4.89946430e-02 1.83865030e+00 5.39334332e-01 1.62674964e-03\n",
      " 2.26295697e+00 1.19042811e-02 8.18823686e-01 1.75832017e-01\n",
      " 5.94868473e-01 1.49104302e-04 1.47572974e+00 1.14648602e-02\n",
      " 1.00314335e-04 6.56110754e-04 9.80479941e-02 1.23970601e-08\n",
      " 2.10771537e-04 2.97327304e-03 1.05518716e-08 2.32492636e-10\n",
      " 4.35794689e-04 4.56101318e-01 3.12087070e-01 3.10398422e-02\n",
      " 7.08134272e-01 1.54277704e+00 1.21588300e-03 2.42400104e-01\n",
      " 3.84618527e-02 3.90958139e-02 1.46987369e-03 1.29175786e-07\n",
      " 1.48687105e+00 5.59463960e-02 1.03124367e+00 1.16591669e+00\n",
      " 1.99178396e-02 2.68687149e-02 7.05787594e-10 2.49413966e-02\n",
      " 3.45689238e-01 5.61564570e-04 3.71332096e-02 6.14544858e-01\n",
      " 4.48235193e-05 1.06960899e-09 3.44653182e-02 1.46868993e-05\n",
      " 3.39384715e-03 2.59873138e-09 4.80066108e-02 3.30092657e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[4.22340759e-01 7.58932299e-09 8.22929927e-01 4.77222250e-01\n",
      " 1.41206852e-03 2.42369616e-05 3.23292067e-02 2.23648885e-01\n",
      " 4.78034926e-04 1.91094895e-01 1.08964557e+00 1.46882609e-07\n",
      " 3.03551515e-01 9.54782604e-01 1.63389012e-07 9.77553131e-01\n",
      " 3.12360706e-01 2.01877071e-01 7.13743281e-01 1.27038190e-03\n",
      " 4.89290466e-02 8.12817324e-01 9.94658912e-03 1.44838010e-01\n",
      " 3.10249771e-01 9.61313217e-01 2.75671910e-02 2.55159333e-01\n",
      " 2.07734239e-04 2.83900989e-02 3.68863559e-01 4.94438013e-02\n",
      " 1.05103887e-06 1.49926077e-01 9.13117824e-01 7.10292745e-01\n",
      " 2.31476609e-01 1.77558141e-02 2.72239483e-03 1.83655484e-01\n",
      " 3.64924469e-01 6.65826892e-07 1.05694046e-09 4.39038491e-01\n",
      " 1.24660103e-04 6.38043283e-01 1.69353816e-04 1.91362749e-01\n",
      " 4.86062384e-02 1.84449815e+00 5.37080310e-01 1.55493318e-03\n",
      " 2.26334085e+00 8.35661891e-03 8.12805417e-01 1.76777955e-01\n",
      " 5.91437706e-01 1.20077217e-04 1.46387676e+00 1.12896245e-02\n",
      " 9.95125136e-05 6.48750726e-04 9.53876006e-02 1.18944379e-08\n",
      " 2.02398542e-04 2.16684329e-03 1.09669557e-08 2.07181647e-10\n",
      " 6.55899574e-04 4.64143397e-01 3.00333847e-01 4.83481743e-02\n",
      " 7.02679616e-01 1.54465739e+00 1.24814960e-03 2.42629092e-01\n",
      " 4.32272386e-02 4.17875865e-02 1.30645885e-03 1.31858658e-07\n",
      " 1.44255562e+00 5.90880816e-02 2.69107609e+00 1.28577987e+00\n",
      " 9.31265567e-02 2.61817243e-02 6.86941017e-10 2.35164069e-02\n",
      " 3.47362828e-01 3.99247439e-04 3.28411973e-02 6.12353778e-01\n",
      " 4.16249278e-05 9.96869662e-10 3.45412061e-02 1.33240958e-05\n",
      " 3.33313166e-03 2.60046042e-09 9.69981384e-03 3.24658964e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[4.20906681e-01 6.50724352e-09 8.08443057e-01 4.88924439e-01\n",
      " 1.55508629e-03 2.91536977e-05 3.62612860e-02 2.21888297e-01\n",
      " 4.78767724e-04 1.87574085e-01 1.08999614e+00 1.34662874e-07\n",
      " 2.96949280e-01 9.48580723e-01 1.68861480e-07 9.87752436e-01\n",
      " 3.06727309e-01 1.91087961e-01 6.52580019e-01 1.36121949e-03\n",
      " 5.47889696e-02 7.63225178e-01 1.01271181e-02 1.42347006e-01\n",
      " 3.04695159e-01 9.64465058e-01 2.77673308e-02 2.51271381e-01\n",
      " 2.24545614e-04 2.67587628e-02 3.70779331e-01 5.08958973e-02\n",
      " 9.29726018e-07 1.38289866e-01 9.34738053e-01 8.03122313e-01\n",
      " 2.28769887e-01 1.91638701e-02 1.93968296e-03 1.89621365e-01\n",
      " 3.56847549e-01 7.92373518e-07 1.15592318e-09 4.28968096e-01\n",
      " 1.45177543e-04 6.30161151e-01 1.93041820e-04 1.75227644e-01\n",
      " 4.69918225e-02 1.83376795e+00 5.31276288e-01 1.47679008e-03\n",
      " 2.23854219e+00 9.54887625e-03 7.96572308e-01 1.78749306e-01\n",
      " 5.78303929e-01 1.21704859e-04 1.42233157e+00 1.15421395e-02\n",
      " 1.07398128e-04 6.12752434e-04 8.70523867e-02 1.08296835e-08\n",
      " 1.98465555e-04 2.12926045e-03 1.30306276e-08 2.10147108e-10\n",
      " 5.34484575e-04 4.62654935e-01 3.04626886e-01 3.12216748e-02\n",
      " 6.88021249e-01 1.53244079e+00 1.31077043e-03 2.54012554e-01\n",
      " 4.70079582e-02 4.79617842e-02 1.40382517e-03 1.53606878e-07\n",
      " 1.46803851e+00 5.39359728e-02 1.10366322e+00 1.14890449e+00\n",
      " 1.54334549e-02 2.63937199e-02 6.12169064e-10 2.55450370e-02\n",
      " 3.45519570e-01 4.28432890e-04 3.30111150e-02 6.00930230e-01\n",
      " 4.38439763e-05 7.92435557e-10 3.40206137e-02 1.38308193e-05\n",
      " 3.35566896e-03 2.76405703e-09 9.92186466e-03 3.09413304e-01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[4.15274124e-01 5.75421846e-09 7.96566754e-01 4.87437221e-01\n",
      " 1.44352389e-03 3.42629281e-05 4.02613065e-02 2.25066143e-01\n",
      " 4.53778447e-04 1.87220290e-01 1.08147478e+00 1.27644179e-07\n",
      " 3.00323764e-01 9.49734175e-01 1.79109989e-07 9.87306270e-01\n",
      " 3.08128768e-01 1.94109022e-01 7.00817489e-01 1.27502186e-03\n",
      " 3.86354600e-02 1.01766949e+00 1.05765711e-02 1.44567100e-01\n",
      " 3.03586013e-01 9.65793081e-01 2.66983512e-02 2.50528181e-01\n",
      " 2.06504469e-04 2.62612588e-02 3.68001810e-01 4.98746170e-02\n",
      " 8.38119729e-07 1.61445595e-01 9.31059264e-01 7.10373253e-01\n",
      " 2.31189593e-01 1.69482148e-02 1.80636287e-03 1.88036791e-01\n",
      " 3.60749886e-01 9.15468085e-07 1.26155331e-09 4.36464951e-01\n",
      " 1.64308699e-04 6.24069076e-01 1.27168656e-04 1.82552970e-01\n",
      " 4.66356814e-02 1.85718208e+00 5.35480081e-01 1.45143472e-03\n",
      " 2.29942301e+00 9.15960364e-03 7.83370061e-01 1.89477933e-01\n",
      " 6.08196488e-01 1.33979674e-04 1.51908400e+00 1.11805505e-02\n",
      " 8.58950568e-05 5.77223811e-04 1.07837292e-01 1.02160766e-08\n",
      " 2.22097022e-04 2.09394308e-03 1.50736642e-08 2.33485287e-10\n",
      " 5.35262121e-04 4.58681470e-01 3.03434975e-01 2.93483478e-02\n",
      " 6.75696203e-01 1.54431492e+00 1.11451391e-03 2.42346323e-01\n",
      " 3.52075917e-02 3.33299406e-02 1.37882163e-03 4.91402281e-07\n",
      " 1.46362570e+00 5.42385189e-02 1.03233429e+00 1.16263322e+00\n",
      " 2.21250405e-02 2.67154304e-02 5.49747899e-10 2.45705771e-02\n",
      " 3.47587136e-01 4.12120849e-04 3.26460259e-02 6.25942123e-01\n",
      " 4.18524552e-05 6.46861242e-10 3.37380985e-02 1.29734335e-05\n",
      " 3.18044616e-03 2.92488644e-09 9.30780332e-03 2.96611566e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[4.14203699e-01 5.41639580e-09 7.88723155e-01 4.75653179e-01\n",
      " 1.31800634e-03 3.72929335e-05 4.22750897e-02 2.29789169e-01\n",
      " 4.38417648e-04 2.07570301e-01 1.08613461e+00 1.24883174e-07\n",
      " 3.05288607e-01 9.46214033e-01 1.88813994e-07 9.80660638e-01\n",
      " 3.10239974e-01 2.09139110e-01 6.37832130e-01 1.14935903e-03\n",
      " 4.36534246e-02 8.09971119e-01 1.04611319e-02 1.62073774e-01\n",
      " 3.06336676e-01 9.70105091e-01 2.57497442e-02 2.49302989e-01\n",
      " 1.86904708e-04 2.53061157e-02 3.63582067e-01 4.76941506e-02\n",
      " 8.10980737e-07 1.52315680e-01 9.06917726e-01 7.12941644e-01\n",
      " 2.33461783e-01 1.57134503e-02 2.05849053e-03 1.82621937e-01\n",
      " 3.68557648e-01 9.95145625e-07 1.32161896e-09 4.51537151e-01\n",
      " 1.74103257e-04 6.17379117e-01 1.44652436e-04 1.99316982e-01\n",
      " 4.72818152e-02 1.85221908e+00 5.41728814e-01 1.42789127e-03\n",
      " 2.28235733e+00 7.44063419e-03 3.07708578e+00 1.68540387e-01\n",
      " 5.99238065e-01 1.05797430e-04 1.48546997e+00 1.09888642e-02\n",
      " 1.18794099e-04 5.86429914e-04 9.97272455e-02 9.96662910e-09\n",
      " 1.79832987e-04 2.36530277e-03 1.63698900e-08 2.46836551e-10\n",
      " 6.67471808e-04 4.54847193e-01 2.96854092e-01 2.78400014e-02\n",
      " 6.67443846e-01 2.61500303e+00 1.13228795e-03 2.29968630e-01\n",
      " 3.89617052e-02 3.79071162e-02 1.25323393e-03 1.85537420e-07\n",
      " 1.43984399e+00 5.77516950e-02 1.04384208e+00 1.18476898e+00\n",
      " 1.90213520e-02 2.33632539e-02 5.29296648e-10 2.29039766e-02\n",
      " 3.48887421e-01 3.43092626e-04 3.42504846e-02 6.18347420e-01\n",
      " 3.83732236e-05 5.79384232e-10 3.35520814e-02 1.22477221e-05\n",
      " 3.06174142e-03 3.02291588e-09 8.84298254e-03 2.88193599e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[4.15163164e-01 5.80894597e-09 7.84584586e-01 4.72219419e-01\n",
      " 1.13604963e-03 3.93591422e-05 4.40039854e-02 2.35488536e-01\n",
      " 3.77677821e-04 1.93327740e-01 1.07943890e+00 1.22556512e-07\n",
      " 3.16159987e-01 9.75503005e-01 1.93552381e-07 9.79040509e-01\n",
      " 3.27436659e-01 2.16029043e-01 6.30100259e-01 1.16551107e-03\n",
      " 2.04906053e-01 8.38278324e-01 9.80165914e-03 1.57166982e-01\n",
      " 3.11641796e-01 9.66597891e-01 2.50629256e-02 2.48658140e-01\n",
      " 1.72267130e-04 2.49502091e-02 3.68872626e-01 4.63075845e-02\n",
      " 7.74525772e-07 1.73287891e-01 8.96047146e-01 7.14832264e-01\n",
      " 2.43486491e-01 1.51752197e-02 1.75861405e-03 1.82163588e-01\n",
      " 3.77504777e-01 1.03478469e-06 1.35118314e-09 5.75240798e-01\n",
      " 1.80902194e-04 1.10008733e+00 9.86072352e-05 2.11252158e-01\n",
      " 4.82867328e-02 1.87465725e+00 5.48226196e-01 1.42826609e-03\n",
      " 2.34061833e+00 6.94494229e-03 7.70468898e-01 1.64252357e-01\n",
      " 6.26831818e-01 1.00667136e-04 1.57636520e+00 9.56823028e-03\n",
      " 7.66770714e-05 6.16510786e-04 1.18443585e-01 9.75514932e-09\n",
      " 1.63164637e-04 2.11086024e-03 1.70180583e-08 2.53033289e-10\n",
      " 7.33369745e-04 4.58315505e-01 2.93767100e-01 2.76750042e-02\n",
      " 6.63288337e-01 1.58552626e+00 1.04572071e-03 2.22262223e-01\n",
      " 2.95685952e-02 2.73381766e-02 1.06650432e-03 1.92311584e-07\n",
      " 1.43085588e+00 1.05675853e-01 1.08863177e+00 1.20668164e+00\n",
      " 2.58402624e-02 2.05805986e-02 6.89801271e-10 2.04690117e-02\n",
      " 3.61386774e-01 3.19955671e-04 3.34593426e-02 6.41857697e-01\n",
      " 3.41838597e-05 5.28809092e-10 3.34824854e-02 1.10461439e-05\n",
      " 2.64500353e-03 3.06599494e-09 7.49371258e-03 2.83241474e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[4.11972499e-01 5.88269521e-09 8.00400109e-01 4.70887422e-01\n",
      " 6.78657731e-04 2.97829642e-05 3.69287326e-02 2.28549827e-01\n",
      " 4.23428787e-04 1.96003536e-01 1.08104341e+00 1.21338909e-07\n",
      " 3.57156746e-01 9.41425170e-01 1.68734139e-07 9.81576015e-01\n",
      " 3.17982414e-01 2.21830457e-01 6.45297998e-01 1.05432258e-03\n",
      " 3.36841364e-02 8.59379170e-01 1.16882180e-02 1.45311333e-01\n",
      " 3.38811906e-01 9.71757949e-01 2.54505575e-02 2.43740599e-01\n",
      " 1.66897822e-04 2.54546558e-02 3.64244608e-01 5.13120901e-02\n",
      " 8.89381124e-07 1.67775636e-01 8.90619784e-01 7.18180587e-01\n",
      " 2.33467410e-01 1.59409232e-02 1.68391704e-03 1.81791973e-01\n",
      " 3.53546259e-01 7.97511242e-07 1.09965231e-09 4.41665892e-01\n",
      " 1.43042492e-04 6.16829046e-01 1.01266683e-04 2.12561018e-01\n",
      " 5.22324586e-02 1.87592001e+00 5.36620510e-01 1.33149930e-03\n",
      " 2.33836688e+00 6.53279095e-03 7.87486769e-01 1.75521234e-01\n",
      " 6.22831389e-01 9.45725833e-05 1.56177567e+00 1.08995354e-02\n",
      " 7.58371837e-05 8.82495022e-04 1.14500083e-01 9.82675094e-09\n",
      " 1.69083222e-04 1.17077778e-03 1.30715456e-08 2.08933024e-10\n",
      " 7.55431756e-04 4.53142424e-01 2.92208489e-01 2.56039700e-02\n",
      " 6.79009467e-01 1.55038169e+00 1.00521797e-03 2.36277030e-01\n",
      " 3.05119443e-02 2.97214628e-02 5.97880289e-04 1.48571065e-07\n",
      " 1.42882164e+00 9.11191994e-02 1.09938968e+00 1.16629734e+00\n",
      " 2.35402620e-02 1.31639817e-02 5.78582612e-10 1.39455802e-02\n",
      " 3.48627724e-01 2.99779126e-04 3.16991048e-02 5.10938064e+00\n",
      " 2.60253229e-05 6.73141414e-10 3.18331741e-02 9.36100366e-06\n",
      " 2.88768687e-03 2.63449857e-09 8.59776647e-03 2.98293623e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[4.12631049e-01 5.93169869e-09 8.01718473e-01 4.81729366e-01\n",
      " 6.08725881e-04 3.28615697e-05 3.52938676e-02 5.39892882e-01\n",
      " 3.98903635e-04 2.34038900e-01 1.08880989e+00 1.18074322e-07\n",
      " 3.63807042e-01 9.46717439e-01 1.65094908e-07 9.91359505e-01\n",
      " 3.15152381e-01 2.10229819e-01 6.47304348e-01 1.11639861e-03\n",
      " 4.37078399e-02 8.17913011e-01 1.12623640e-02 1.41569326e-01\n",
      " 3.41760042e-01 9.71251965e-01 2.55713079e-02 3.15428448e-01\n",
      " 1.73829504e-04 2.60259606e-02 3.65973535e-01 5.18931344e-02\n",
      " 9.17369377e-07 1.48873532e-01 9.11803883e-01 7.14403683e-01\n",
      " 2.35755404e-01 2.01366901e-02 1.65634357e-03 1.85846577e-01\n",
      " 3.47333391e-01 7.50146783e-07 1.04548932e-09 4.61955760e-01\n",
      " 1.39364386e-04 6.16114751e-01 1.30507143e-04 1.94378249e-01\n",
      " 4.31617297e-02 1.86486684e+00 5.31260360e-01 1.31031749e-03\n",
      " 2.29963548e+00 7.57534126e-03 7.89878534e-01 1.69184905e-01\n",
      " 6.00527409e-01 1.10532483e-04 1.49173269e+00 1.04597271e-02\n",
      " 8.15716501e-05 8.87782406e-04 9.78811319e-02 9.75172356e-09\n",
      " 1.63378800e-04 1.61901777e-03 1.22107037e-08 1.98292785e-10\n",
      " 6.13734063e-04 4.53501818e-01 2.95404421e-01 2.52923459e-02\n",
      " 6.81194417e-01 1.69230290e+00 1.03759256e-03 2.38017035e-01\n",
      " 3.76043045e-02 3.86262921e-02 5.63542851e-04 1.40987930e-07\n",
      " 1.45365594e+00 9.30001664e-02 1.04928687e+00 1.16944579e+00\n",
      " 1.78747114e-02 2.34934614e-02 1.46168186e-09 1.30930946e-02\n",
      " 3.51094098e-01 3.29979745e-04 3.20668512e-02 6.21243207e-01\n",
      " 2.46691758e-05 6.77947334e-10 3.12377406e-02 8.74373122e-06\n",
      " 2.76302274e-03 2.59080773e-09 8.23424815e-03 2.98843016e-01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[4.14879858e-01 5.96218836e-09 8.02829014e-01 4.74448266e-01\n",
      " 1.15704261e-03 2.71752371e-05 3.42895536e-02 2.28602510e-01\n",
      " 3.09953365e-04 1.93847446e-01 1.09546280e+00 1.16339415e-07\n",
      " 3.66045557e-01 1.00481387e+00 1.62857356e-07 9.85782235e-01\n",
      " 3.56061306e-01 2.21146619e-01 6.62547514e-01 1.05348659e-03\n",
      " 4.98626692e-02 8.59769657e-01 8.58169167e-03 2.38701829e-01\n",
      " 3.42847538e-01 9.68582068e-01 2.68854613e-02 2.42200426e-01\n",
      " 1.58957700e-04 2.47202481e-02 3.65523947e-01 4.93957427e-02\n",
      " 9.35344938e-07 1.40458859e-01 8.93526360e-01 7.17809721e-01\n",
      " 2.58339809e-01 1.52195324e-02 1.63231883e-03 1.83810138e-01\n",
      " 3.54346392e-01 7.37268284e-07 1.18909880e-09 4.42716194e-01\n",
      " 1.33389571e-04 6.13428008e-01 1.50800268e-04 2.06922820e-01\n",
      " 4.40261473e-02 1.86120987e+00 5.35467933e-01 1.26787292e-03\n",
      " 2.28255579e+00 6.57738385e-03 7.90068508e-01 1.67094806e-01\n",
      " 5.90481718e-01 8.87154775e-05 1.45867369e+00 8.05507113e-03\n",
      " 8.65107117e-05 8.72286294e-04 9.02984834e-02 9.60592836e-09\n",
      " 1.45474508e-04 1.11044639e-03 1.19104510e-08 1.95364794e-10\n",
      " 7.07181333e-04 4.60103014e-01 2.90705495e-01 2.71990215e-02\n",
      " 6.81258746e-01 2.27148636e+00 1.09020174e-03 2.34340948e-01\n",
      " 4.26856304e-02 4.42032918e-02 5.43150810e-04 1.35051341e-07\n",
      " 1.43596751e+00 9.32642700e-02 1.14161706e+00 1.16552800e+00\n",
      " 1.52923936e-02 1.21950659e-02 5.97298030e-10 1.28224156e-02\n",
      " 3.73203209e-01 2.94159860e-04 5.23462682e-02 6.12623514e-01\n",
      " 2.40483056e-05 6.86840428e-10 3.13721207e-02 8.52219856e-06\n",
      " 2.19205076e-03 4.22156297e-09 5.95218921e-03 2.99311645e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[4.11163469e-01 6.26278368e-09 8.10489975e-01 4.76394581e-01\n",
      " 5.81610711e-04 2.34293442e-05 3.17573809e-02 2.50281454e-01\n",
      " 3.64039560e-04 1.91986288e-01 1.09501223e+00 1.17957499e-07\n",
      " 3.64747608e-01 9.61667768e-01 6.10251353e-07 9.88111477e-01\n",
      " 3.22423821e-01 2.19642717e-01 6.67611357e-01 1.17324995e-03\n",
      " 4.80355946e-02 1.01102088e+00 1.02330149e-02 1.41452929e-01\n",
      " 3.41920398e-01 9.73305608e-01 2.47495152e-02 2.42097507e-01\n",
      " 1.56444714e-04 2.45836932e-02 3.67346776e-01 4.92869070e-02\n",
      " 9.88315144e-07 1.40378575e-01 8.97240265e-01 7.18470531e-01\n",
      " 2.42338142e-01 1.53973222e-02 1.61020461e-03 1.84418594e-01\n",
      " 3.66976939e-01 6.70504812e-07 9.08219635e-10 4.37162368e-01\n",
      " 1.19011826e-04 6.13164399e-01 1.42774054e-04 2.04603961e-01\n",
      " 4.34764525e-02 1.86286204e+00 5.39729785e-01 1.23116520e-03\n",
      " 2.28894350e+00 9.79881907e-03 7.96762445e-01 1.67150277e-01\n",
      " 5.97353918e-01 8.78751650e-05 1.46192958e+00 9.49662565e-03\n",
      " 8.38942262e-05 8.17223772e-04 9.09436273e-02 9.66456752e-09\n",
      " 1.51137842e-04 1.12803819e-03 1.07637256e-08 1.83276144e-10\n",
      " 6.72011896e-04 4.54117391e-01 2.93720669e-01 2.50423432e-02\n",
      " 6.87258274e-01 1.54551406e+00 1.05488392e-03 2.45096091e-01\n",
      " 4.09886828e-02 4.39153430e-02 5.70591950e-04 1.20249296e-07\n",
      " 1.44063766e+00 9.06117706e-02 1.07044026e+00 1.15882483e+00\n",
      " 1.53363693e-02 1.23605823e-02 6.29641893e-10 1.27093339e-02\n",
      " 3.55686272e-01 2.92567414e-04 3.08002902e-02 6.14144565e-01\n",
      " 2.33137476e-05 7.56091046e-10 4.27915607e-02 8.22525348e-06\n",
      " 2.52624212e-03 2.33263520e-09 7.50570943e-03 3.08970725e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[4.09000614e-01 6.26722607e-09 8.10102247e-01 4.82032164e-01\n",
      " 6.00827796e-04 2.30293513e-05 3.14267501e-02 5.58180597e-01\n",
      " 3.13068914e-04 1.85915996e-01 1.09635738e+00 1.16910563e-07\n",
      " 3.58789830e-01 9.96145287e-01 1.54141232e-07 9.91505523e-01\n",
      " 3.43324249e-01 2.09305884e-01 6.31285422e-01 1.13733003e-03\n",
      " 4.35016298e-02 8.14686065e-01 8.80677187e-03 1.49220468e-01\n",
      " 3.35923051e-01 9.67082270e-01 2.37261369e-02 2.45009848e-01\n",
      " 1.68788532e-04 2.44567000e-02 3.63488311e-01 4.57376955e-02\n",
      " 9.96137071e-07 1.48300421e-01 9.13101963e-01 7.11426657e-01\n",
      " 2.53883872e-01 2.67829150e-02 1.54355584e-03 1.85845441e-01\n",
      " 3.62439720e-01 6.61797003e-07 8.91828774e-10 4.56656660e-01\n",
      " 1.15753602e-04 6.10126416e-01 1.27321492e-04 1.91792862e-01\n",
      " 4.62451660e-02 1.87250968e+00 5.44690328e-01 1.20272644e-03\n",
      " 2.30885256e+00 7.15111506e-03 7.95969435e-01 1.61899525e-01\n",
      " 6.01553487e-01 9.01827695e-05 1.49126325e+00 7.99385814e-03\n",
      " 7.76322103e-05 7.21544448e-04 9.73642560e-02 9.57030399e-09\n",
      " 1.39650513e-04 1.20231457e-03 1.06388971e-08 2.20362964e-10\n",
      " 5.76386934e-04 4.58557146e-01 2.92686864e-01 2.62189254e-02\n",
      " 6.86169943e-01 1.58266773e+00 9.96685293e-04 2.26193958e-01\n",
      " 3.84025106e-02 4.00938293e-02 6.40670054e-04 1.17288817e-07\n",
      " 1.45497508e+00 8.25186348e-02 1.12283320e+00 1.18718945e+00\n",
      " 1.67782423e-02 1.31950280e-02 6.36359561e-10 1.33576678e-02\n",
      " 3.70277490e-01 3.09560482e-04 3.02631047e-02 6.22351734e-01\n",
      " 2.63243463e-05 7.57218339e-10 3.15514688e-02 7.74368552e-06\n",
      " 2.16975191e-03 2.29623278e-09 6.27392849e-03 3.05154868e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[4.09964760e-01 6.26724985e-09 8.09841585e-01 4.76999214e-01\n",
      " 6.38375788e-04 2.25146790e-05 3.09167958e-02 2.33250922e-01\n",
      " 3.34900657e-04 1.88733164e-01 1.09895764e+00 1.15577538e-07\n",
      " 3.52110654e-01 9.75551526e-01 1.53310614e-07 9.87914303e-01\n",
      " 3.29568344e-01 2.17225830e-01 6.39371859e-01 1.02310546e-03\n",
      " 1.05707972e-01 8.43132332e-01 9.59141130e-03 1.46938932e-01\n",
      " 6.55338817e-01 9.70719668e-01 2.74944912e-02 2.44283290e-01\n",
      " 1.57784551e-04 2.67075724e-02 3.62978559e-01 4.61405879e-02\n",
      " 1.01746875e-06 1.50566119e-01 9.01367973e-01 7.13678185e-01\n",
      " 2.46664526e-01 1.41839001e-02 2.28983016e-03 1.83810707e-01\n",
      " 3.58179135e-01 6.50084865e-07 8.71967079e-10 4.55709515e-01\n",
      " 1.13106514e-04 6.06848184e-01 1.22741264e-04 2.01369493e-01\n",
      " 4.69678469e-02 1.87949217e+00 5.43364371e-01 1.17054981e-03\n",
      " 2.31745054e+00 6.49579879e-03 7.95410541e-01 1.80920952e-01\n",
      " 6.05055959e-01 8.72773140e-05 1.50217139e+00 8.68707033e-03\n",
      " 1.07847628e-04 6.42328515e-04 9.89338836e-02 9.47200929e-09\n",
      " 1.40811365e-04 1.33154405e-03 1.04736537e-08 1.81991991e-10\n",
      " 6.45158213e-04 4.56199018e-01 2.91596754e-01 2.51048257e-02\n",
      " 6.85442765e-01 1.56709596e+00 9.75965165e-04 2.92208920e-01\n",
      " 3.81252273e-02 3.91685831e-02 7.01396490e-04 1.14101740e-07\n",
      " 1.44334082e+00 7.64960862e-02 1.08716918e+00 1.18080190e+00\n",
      " 1.68842326e-02 1.44268626e-02 6.47122452e-10 1.41629461e-02\n",
      " 3.61879485e-01 2.83218472e-04 2.98763609e-02 6.25386323e-01\n",
      " 2.70430992e-05 7.56576006e-10 3.31625995e-02 7.57706300e-06\n",
      " 2.30080061e-03 2.28543139e-09 7.48899650e-03 3.04217436e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[4.07563508e-01 6.29606374e-09 8.10263506e-01 4.82342756e-01\n",
      " 6.81508008e-04 2.19017871e-05 3.02876599e-02 2.34886462e-01\n",
      " 3.03467623e-04 1.86786537e-01 1.09855265e+00 1.14760450e-07\n",
      " 3.44727398e-01 9.96261053e-01 1.52137232e-07 9.92432661e-01\n",
      " 3.42516124e-01 2.13947451e-01 6.32428512e-01 1.07526899e-03\n",
      " 4.39555643e-02 8.27082719e-01 8.87636244e-03 1.47500239e-01\n",
      " 3.23828398e-01 9.70668400e-01 2.33801350e-02 2.42489573e-01\n",
      " 1.54708356e-04 2.33035250e-02 3.63529564e-01 4.63332595e-02\n",
      " 1.03248215e-06 1.46650568e-01 9.11199666e-01 8.26395151e-01\n",
      " 2.54202973e-01 1.38958822e-02 1.47628906e-03 1.85137023e-01\n",
      " 5.96413720e-01 6.32445543e-07 8.49280643e-10 4.55787736e-01\n",
      " 1.09494920e-04 6.04620405e-01 1.26700432e-04 1.95501124e-01\n",
      " 4.55600547e-02 1.87514213e+00 5.43010785e-01 1.14761693e-03\n",
      " 2.31034051e+00 6.86659856e-03 7.95814143e-01 2.77414895e-01\n",
      " 6.00845886e-01 1.10602970e-04 1.48817422e+00 7.88590762e-03\n",
      " 7.54718485e-05 1.08240402e-03 9.61346177e-02 9.42590605e-09\n",
      " 1.33636327e-04 1.31441082e-03 1.02533644e-08 1.79685960e-10\n",
      " 5.80783617e-04 4.58774453e-01 2.92011859e-01 2.53824276e-02\n",
      " 6.85404090e-01 1.56946906e+00 9.78518951e-04 2.27740462e-01\n",
      " 3.89962416e-02 4.11592558e-02 7.71313169e-04 1.10445401e-07\n",
      " 1.45474568e+00 7.15500038e-02 1.85595541e+00 1.29363570e+00\n",
      " 1.58864830e-02 1.55161658e-02 6.55621070e-10 1.51303896e-02\n",
      " 3.70445905e-01 2.95317514e-04 3.35628237e-02 6.21495959e-01\n",
      " 2.34615235e-05 7.64020287e-10 3.08937866e-02 7.61970782e-06\n",
      " 2.12210409e-03 2.22945870e-09 6.18746605e-03 3.04020107e-01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[4.06534427e-01 7.82273103e-09 8.18895583e-01 4.73317420e-01\n",
      " 7.37863747e-04 1.87816156e-05 2.71201338e-02 2.30820613e-01\n",
      " 3.14436919e-04 1.91006227e-01 1.09984947e+00 1.19386001e-07\n",
      " 3.37131505e-01 9.85019675e-01 4.14914057e-07 9.86217539e-01\n",
      " 3.34657060e-01 2.24788732e-01 1.11472299e+00 9.52609190e-04\n",
      " 4.11545760e-02 8.73840132e-01 9.03217288e-03 1.40914316e-01\n",
      " 3.16304529e-01 9.67794463e-01 2.35109707e-02 2.41093926e-01\n",
      " 1.40643717e-04 2.35564628e-02 3.69889279e-01 4.89739143e-02\n",
      " 1.15268574e-06 1.51061176e-01 8.92126477e-01 7.16387537e-01\n",
      " 2.50454240e-01 1.58991976e-02 1.44568335e-03 1.81309158e-01\n",
      " 6.34738491e-01 5.42319531e-07 7.75574686e-10 4.41362080e-01\n",
      " 9.61581293e-05 6.06048150e-01 1.60421631e-04 2.11515940e-01\n",
      " 4.27113929e-02 1.88339507e+00 5.35012346e-01 1.16749317e-03\n",
      " 2.32506386e+00 5.95443297e-03 8.05970471e-01 1.64015252e-01\n",
      " 6.07366345e-01 8.35563671e-05 1.50872485e+00 8.12765999e-03\n",
      " 7.12510156e-05 5.27696945e-04 1.00598679e-01 9.98650559e-09\n",
      " 1.34722513e-04 1.51191434e-03 2.61309948e-08 1.64683794e-10\n",
      " 6.81066378e-04 4.58784042e-01 2.87683336e-01 2.50823255e-02\n",
      " 6.94646080e-01 1.54719670e+00 9.43962057e-04 2.41615711e-01\n",
      " 3.70877880e-02 3.87745774e-02 8.70731646e-04 9.52886235e-08\n",
      " 1.43549182e+00 1.08780833e-01 1.17216929e+00 1.15271456e+00\n",
      " 1.66223098e-02 1.71943058e-02 7.23485742e-10 1.60545646e-02\n",
      " 3.65659180e-01 2.71238156e-04 2.95200232e-02 6.27170098e-01\n",
      " 2.44226411e-05 8.73301633e-10 3.03343254e-02 7.63963627e-06\n",
      " 2.19146964e-03 2.13611355e-09 6.61146646e-03 3.12224287e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[4.05209120e-01 6.84981857e-09 8.18101944e-01 4.76182417e-01\n",
      " 9.08512511e-04 1.87386853e-05 2.72480998e-02 2.37569724e-01\n",
      " 3.29399827e-04 1.89186419e-01 1.09528180e+00 1.17078923e-07\n",
      " 3.20726505e-01 9.68531504e-01 1.44634151e-07 9.89032975e-01\n",
      " 3.33561016e-01 5.01830805e-01 6.37809458e-01 9.53321137e-04\n",
      " 3.42123277e-02 9.69403025e-01 9.62534078e-03 1.49256435e-01\n",
      " 3.02001136e-01 2.03030728e+00 2.25863681e-02 2.43344770e-01\n",
      " 1.41493607e-04 2.27622728e-02 3.61481250e-01 4.50620282e-02\n",
      " 1.14452264e-06 1.63863964e-01 8.98206150e-01 1.16907026e+00\n",
      " 2.44327824e-01 1.33533296e-02 1.45761111e-03 1.81873299e-01\n",
      " 3.61708154e-01 5.46950779e-07 7.67349529e-10 4.63156641e-01\n",
      " 9.54556128e-05 6.10236171e-01 9.29100039e-05 2.07612965e-01\n",
      " 4.52786536e-02 1.89617824e+00 5.47178008e-01 1.14432144e-03\n",
      " 2.35827516e+00 6.11097115e-03 8.04512483e-01 1.56949305e-01\n",
      " 6.24185492e-01 8.22905559e-05 1.56106669e+00 8.69104802e-03\n",
      " 6.50495353e-05 5.68715074e-04 1.11101618e-01 9.76911276e-09\n",
      " 1.39472065e-04 1.67989792e-03 9.65380631e-09 1.64519925e-10\n",
      " 6.45620741e-04 4.57148136e-01 2.88197542e-01 2.43480673e-02\n",
      " 6.93414885e-01 1.57716088e+00 8.91162735e-04 2.24678706e-01\n",
      " 3.16207223e-02 3.17524104e-02 1.08206660e-03 9.46994780e-08\n",
      " 1.44123987e+00 5.48205007e-02 1.07102468e+00 1.19445549e+00\n",
      " 2.14810158e-02 2.11695872e-02 7.17686811e-10 1.90187614e-02\n",
      " 3.59402840e-01 2.65223591e-04 2.91110011e-02 6.41142029e-01\n",
      " 4.70326033e-05 8.52049647e-10 3.04424970e-02 8.23603527e-06\n",
      " 2.32478656e-03 2.11278315e-09 7.19654452e-03 3.10875237e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[4.05382595e-01 6.67965718e-09 8.16741588e-01 4.76857152e-01\n",
      " 9.22090569e-04 1.89123987e-05 2.76420914e-02 2.41579221e-01\n",
      " 3.19268290e-04 1.87281175e-01 1.09332744e+00 1.14442911e-07\n",
      " 3.24834589e-01 9.74665665e-01 1.43840876e-07 9.90383371e-01\n",
      " 3.26489113e-01 2.18842989e-01 6.22619737e-01 9.62689487e-04\n",
      " 3.15261966e-02 8.54385106e-01 9.25142982e-03 1.52938181e-01\n",
      " 2.99976617e-01 9.73848442e-01 2.20373684e-02 2.79480498e-01\n",
      " 1.41479200e-04 2.24612143e-02 3.69151944e-01 4.42125927e-02\n",
      " 1.11941744e-06 1.68363338e-01 9.00265483e-01 7.13616472e-01\n",
      " 2.45890563e-01 1.29039350e-02 1.43809341e-03 1.82293543e-01\n",
      " 3.70725648e-01 5.50350080e-07 7.63258038e-10 4.74415389e-01\n",
      " 9.59763068e-05 6.03075832e-01 8.30567548e-05 2.04779959e-01\n",
      " 4.45534104e-02 1.90329932e+00 6.53190702e-01 1.12539815e-03\n",
      " 2.37638224e+00 6.13839176e-03 8.02961921e-01 1.62958377e-01\n",
      " 6.31711738e-01 8.19215274e-05 1.58605289e+00 8.42652088e-03\n",
      " 6.31097623e-05 4.42944410e-04 1.15733901e-01 9.53847637e-09\n",
      " 1.32180337e-04 1.68128233e-03 8.81703061e-09 1.63883990e-10\n",
      " 6.22361717e-04 4.59283731e-01 2.87863455e-01 2.43673928e-02\n",
      " 6.92018103e-01 2.66588553e+00 8.68646586e-04 2.22747373e-01\n",
      " 2.88063392e-02 2.91444617e-02 1.09045728e-03 9.47964976e-08\n",
      " 1.44446851e+00 5.40036921e-02 1.07864132e+00 1.20125772e+00\n",
      " 2.16417394e-02 2.08999395e-02 7.02178897e-10 1.95622269e-02\n",
      " 3.62089787e-01 2.78616216e-04 2.89286663e-02 6.47530037e-01\n",
      " 2.81102101e-05 8.19354523e-10 3.03899436e-02 8.18604142e-06\n",
      " 2.28729835e-03 2.09010087e-09 7.17932104e-03 1.23281382e+00]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[4.04762551e-01 6.51039437e-09 8.15573606e-01 4.78251522e-01\n",
      " 8.04828496e-04 1.90748096e-05 2.79242210e-02 2.42591076e-01\n",
      " 3.27960840e-04 1.86916866e-01 1.09493476e+00 1.13122584e-07\n",
      " 3.36143608e-01 9.61102516e-01 1.43163744e-07 9.92209068e-01\n",
      " 3.19432021e-01 2.17638577e-01 6.78334079e-01 9.32952554e-04\n",
      " 3.48660958e-02 8.50970904e-01 9.91259582e-03 1.54111469e-01\n",
      " 3.08864304e-01 9.74043646e-01 2.24101078e-02 2.44167794e-01\n",
      " 1.38088980e-04 2.20061502e-02 3.58871885e-01 4.31999372e-02\n",
      " 1.09619502e-06 1.61520183e-01 9.03490448e-01 7.13107860e-01\n",
      " 2.41433124e-01 1.25401236e-02 1.38554009e-03 1.81563916e-01\n",
      " 3.70351313e-01 5.52971553e-07 7.58557409e-10 4.79054678e-01\n",
      " 9.66765719e-05 6.02023371e-01 9.16454910e-05 2.03926479e-01\n",
      " 4.50231011e-02 1.89750492e+00 6.56890785e-01 1.10881453e-03\n",
      " 2.36047661e+00 6.08324231e-03 8.01610032e-01 2.62873028e-01\n",
      " 6.23462153e-01 8.14403926e-05 1.56101999e+00 8.81656799e-03\n",
      " 6.41070933e-05 4.65466637e-04 1.09328354e-01 9.34374279e-09\n",
      " 1.33150306e-04 1.48445467e-03 1.66066685e-08 1.63156877e-10\n",
      " 6.11841949e-04 4.56688413e-01 2.88485085e-01 2.34133032e-02\n",
      " 6.90848274e-01 1.59718450e+00 8.63388991e-04 2.17517803e-01\n",
      " 3.32804643e-02 3.20556525e-02 9.37801843e-04 9.51846042e-08\n",
      " 1.44961613e+00 5.95065747e-02 1.06302521e+00 1.20427374e+00\n",
      " 1.94843836e-02 1.90011421e-02 6.94167472e-10 1.74621713e-02\n",
      " 3.57949707e-01 2.66552340e-04 2.86647658e-02 6.41030645e-01\n",
      " 2.79038706e-05 7.91072522e-10 3.04566793e-02 1.28227400e-05\n",
      " 2.31125346e-03 2.07065231e-09 7.43055169e-03 3.07212643e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[4.02622479e-01 6.33242656e-09 8.13197714e-01 4.75497450e-01\n",
      " 7.36018767e-04 1.96867613e-05 2.84714551e-02 2.49311477e-01\n",
      " 3.27330665e-04 1.86080321e-01 1.09548947e+00 1.09769719e-07\n",
      " 3.35891129e-01 9.61219311e-01 1.43986481e-07 9.90849796e-01\n",
      " 3.18048469e-01 2.19245810e-01 6.17027163e-01 9.33061074e-04\n",
      " 3.54064068e-02 8.61157754e-01 9.93120339e-03 1.60197522e-01\n",
      " 3.11662235e-01 9.75276090e-01 2.26049767e-02 2.46775161e-01\n",
      " 1.37644746e-04 2.17228610e-02 3.56208486e-01 5.13683480e-02\n",
      " 1.06812869e-06 1.59685947e-01 8.99464286e-01 7.10687739e-01\n",
      " 2.40896606e-01 1.20323011e-02 1.36735730e-03 1.80733845e-01\n",
      " 3.82498062e-01 5.59516806e-07 7.64155445e-10 8.40517167e-01\n",
      " 9.87293592e-05 6.00403702e-01 9.39222838e-05 2.04649126e-01\n",
      " 4.66689770e-02 1.89562232e+00 5.62364888e-01 1.09072035e-03\n",
      " 2.35594823e+00 6.25621082e-03 7.99264380e-01 1.49209898e-01\n",
      " 6.21249841e-01 8.06304913e-05 1.55380302e+00 8.76153012e-03\n",
      " 7.04658753e-05 4.79459248e-04 1.08089050e-01 9.14866973e-09\n",
      " 1.75252177e-04 1.51045170e-03 8.99605671e-09 1.63952421e-10\n",
      " 1.30309999e-03 4.56934849e-01 2.86273707e-01 2.32029672e-02\n",
      " 6.88508019e-01 1.85201467e+00 8.58828201e-04 2.12111589e-01\n",
      " 3.17976585e-02 3.28978300e-02 8.68266488e-04 1.07060596e-07\n",
      " 1.44453745e+00 6.27925177e-02 1.06420563e+00 1.22385679e+00\n",
      " 1.87197294e-02 1.73514308e-02 6.69896123e-10 1.64861824e-02\n",
      " 3.58636559e-01 2.58983332e-04 2.85989900e-02 6.39265774e-01\n",
      " 2.41759444e-05 7.57915934e-10 3.08070465e-02 7.24915635e-06\n",
      " 2.27974278e-03 2.06883382e-09 7.43211419e-03 3.04522459e-01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[4.01582766e-01 7.58161800e-09 8.07872603e-01 4.77401362e-01\n",
      " 7.83589766e-04 2.13419260e-05 3.01337564e-02 2.41664715e-01\n",
      " 3.06535712e-04 1.84750365e-01 1.09417089e+00 1.05508559e-07\n",
      " 3.30355887e-01 9.74449950e-01 1.47114833e-07 9.92777650e-01\n",
      " 3.35273765e-01 2.16864674e-01 6.25969511e-01 9.49599682e-04\n",
      " 3.41426148e-02 8.74122662e-01 9.31765379e-03 1.51046028e-01\n",
      " 3.07479471e-01 9.74444180e-01 2.12132144e-02 2.44166775e-01\n",
      " 2.50849815e-04 2.15440996e-02 3.62849558e-01 4.35679322e-02\n",
      " 9.93122020e-07 1.61097158e-01 9.03268690e-01 7.09740826e-01\n",
      " 2.45891585e-01 1.25373118e-02 1.36981284e-03 1.81333793e-01\n",
      " 3.63456465e-01 5.95837843e-07 7.90306887e-10 4.70470096e-01\n",
      " 4.22736540e-04 5.98601469e-01 8.95205395e-05 2.01610610e-01\n",
      " 4.41700207e-02 1.89641337e+00 5.52125278e-01 1.06998859e-03\n",
      " 2.36099292e+00 5.91167621e-03 7.93642841e-01 1.53889983e-01\n",
      " 6.23218563e-01 7.98339816e-05 1.56172165e+00 8.16799755e-03\n",
      " 6.23608960e-05 4.57378239e-04 1.09482863e-01 8.75927843e-09\n",
      " 1.25144448e-04 1.45560774e-03 9.59248975e-09 1.70823383e-10\n",
      " 5.97673909e-04 4.57322891e-01 3.45153649e-01 2.32833062e-02\n",
      " 6.83349620e-01 1.59005785e+00 8.48030864e-04 2.22536733e-01\n",
      " 3.07287631e-02 3.16258367e-02 9.42200825e-04 2.79830439e-07\n",
      " 1.44876296e+00 5.87781578e-02 1.08739776e+00 1.27303421e+00\n",
      " 1.92881427e-02 3.53824289e-02 6.23840686e-10 1.87881998e-02\n",
      " 3.64329072e-01 2.55864182e-04 2.81052144e-02 5.12332942e+00\n",
      " 2.67404959e-05 6.86393647e-10 3.00417102e-02 7.38252951e-06\n",
      " 2.12854416e-03 2.08877325e-09 6.83302001e-03 2.98598198e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[4.00375133e-01 5.78872772e-09 8.06137346e-01 4.72138192e-01\n",
      " 7.74773836e-04 2.15531852e-05 3.03697877e-02 2.40995716e-01\n",
      " 3.20255513e-04 1.87818294e-01 1.09023889e+00 1.03763111e-07\n",
      " 3.30473263e-01 9.62905826e-01 1.48142838e-07 9.89819242e-01\n",
      " 3.17755334e-01 2.23910810e-01 6.26880349e-01 1.01731039e-03\n",
      " 2.93638311e-02 8.73529943e-01 9.75429174e-03 1.50605061e-01\n",
      " 3.05802700e-01 9.76536420e-01 2.14361200e-02 2.43921928e-01\n",
      " 1.32225364e-04 2.15055274e-02 3.66356086e-01 4.33533106e-02\n",
      " 9.80412112e-07 1.70349071e-01 8.92050779e-01 7.10780294e-01\n",
      " 2.41462419e-01 1.26848389e-02 1.36203671e-03 1.79852731e-01\n",
      " 3.64646287e-01 6.07271758e-07 7.88977159e-10 4.69819036e-01\n",
      " 1.06640708e-04 5.96331131e-01 7.57256328e-05 2.09834017e-01\n",
      " 4.37999762e-02 1.90365246e+00 5.53583032e-01 1.04817936e-03\n",
      " 2.38426297e+00 5.37499789e-03 7.91238483e-01 1.53623260e-01\n",
      " 6.35409209e-01 7.78514217e-05 1.59987234e+00 9.23164536e-03\n",
      " 6.05021332e-05 4.47147766e-04 1.18071071e-01 8.58995517e-09\n",
      " 1.26084948e-04 1.45489706e-03 1.07591362e-08 1.70639378e-10\n",
      " 6.73265602e-04 4.56095412e-01 5.77327398e-01 2.27984124e-02\n",
      " 6.81080136e-01 1.64041899e+00 8.35606714e-04 2.22405690e-01\n",
      " 2.67559978e-02 2.73343682e-02 9.88089651e-04 1.04165145e-07\n",
      " 1.43790586e+00 5.84246342e-02 2.63158994e+00 1.22501896e+00\n",
      " 2.20242929e-02 1.81488176e-02 6.15691871e-10 1.70963565e-02\n",
      " 3.59840335e-01 2.32539231e-04 3.07325501e-02 6.50315692e-01\n",
      " 2.45623347e-05 6.66976845e-10 2.98274523e-02 7.29666565e-06\n",
      " 2.20772407e-03 2.08579598e-09 7.28992463e-03 2.96511996e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[3.99501863e-01 5.93185738e-09 8.08070101e-01 4.71244517e-01\n",
      " 8.93300286e-04 2.06510400e-05 2.95551534e-02 2.35780753e-01\n",
      " 3.26465236e-04 2.01025604e-01 1.09207534e+00 1.04195668e-07\n",
      " 3.19155616e-01 9.53822834e-01 1.45843799e-07 9.89443041e-01\n",
      " 3.14303430e-01 2.24920005e-01 6.83971813e-01 9.02053034e-04\n",
      " 2.98877386e-02 8.76146271e-01 5.40316722e-02 1.42080195e-01\n",
      " 2.97255281e-01 9.75869374e-01 2.14934376e-02 2.69402713e-01\n",
      " 1.31977157e-04 3.08468877e-02 3.69793962e-01 4.64426820e-02\n",
      " 1.00904365e-06 1.69193819e-01 8.90520519e-01 7.09859103e-01\n",
      " 2.39984463e-01 1.31560066e-02 1.38629783e-03 1.79527783e-01\n",
      " 3.47922266e-01 5.83767508e-07 7.66769521e-10 4.50823309e-01\n",
      " 1.01898452e-04 5.95491231e-01 7.72426441e-05 2.10904581e-01\n",
      " 4.11558393e-02 1.90464203e+00 5.43368646e-01 1.04003469e-03\n",
      " 2.38229724e+00 5.24334720e-03 7.93203751e-01 1.57866483e-01\n",
      " 6.33907125e-01 7.70239784e-05 1.59501956e+00 8.82140772e-03\n",
      " 5.97623662e-05 4.09907909e-04 1.17196435e-01 8.64989087e-09\n",
      " 1.27353018e-04 1.72238832e-03 1.03446715e-08 1.66965136e-10\n",
      " 6.86145214e-04 4.53786174e-01 2.82600839e-01 2.20536052e-02\n",
      " 6.82910237e-01 1.56599655e+00 8.27651764e-04 2.33815420e-01\n",
      " 2.75045758e-02 2.82446843e-02 1.09889183e-03 9.94718360e-08\n",
      " 2.63676987e+00 5.16156109e-02 1.06097488e+00 1.16041443e+00\n",
      " 2.11966834e-02 2.04759932e-02 6.34078774e-10 1.92639487e-02\n",
      " 3.57250422e-01 2.26598449e-04 2.77224799e-02 6.49220724e-01\n",
      " 2.67919830e-05 6.87636305e-10 2.93723494e-02 7.77339963e-06\n",
      " 2.25904516e-03 2.04849978e-09 7.53082576e-03 3.00976450e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[4.00250342e-01 5.85867337e-09 8.07678406e-01 4.71104907e-01\n",
      " 8.41554162e-04 2.05025596e-05 2.95373504e-02 2.36909684e-01\n",
      " 3.21971227e-04 1.89094877e-01 1.09483201e+00 1.03039884e-07\n",
      " 3.23485456e-01 9.54029438e-01 1.45633216e-07 9.90286319e-01\n",
      " 3.13940294e-01 5.15000259e-01 6.36044218e-01 8.93236206e-04\n",
      " 3.01023283e-02 8.82999722e-01 1.00509009e-02 1.41315850e-01\n",
      " 3.01307479e-01 9.75804258e-01 2.11825673e-02 2.41740954e-01\n",
      " 1.27468146e-04 2.16004571e-02 3.70333046e-01 4.62607842e-02\n",
      " 1.00430528e-06 1.69550592e-01 8.89870732e-01 7.63805567e-01\n",
      " 2.39959910e-01 1.31518707e-02 1.35033264e-03 3.30503751e-01\n",
      " 3.45648321e-01 2.34922228e-06 7.58695188e-10 4.51334904e-01\n",
      " 1.01112097e-04 5.93849656e-01 7.73023107e-05 4.24925868e-01\n",
      " 4.08465673e-02 1.90781969e+00 5.45066199e-01 1.02221303e-03\n",
      " 2.38469231e+00 5.14791625e-03 7.92191191e-01 1.56810642e-01\n",
      " 6.34369909e-01 7.55747617e-05 1.59640032e+00 8.78508502e-03\n",
      " 6.36554549e-05 4.16198340e-04 1.17100680e-01 8.51898765e-09\n",
      " 1.35888621e-04 1.52538213e-03 1.03897731e-08 1.67163880e-10\n",
      " 6.90412331e-04 4.55293715e-01 2.82363551e-01 2.22012913e-02\n",
      " 6.81905909e-01 1.56477306e+00 8.19722704e-04 2.38874741e-01\n",
      " 2.78350100e-02 2.85857328e-02 1.02533610e-03 9.89755632e-08\n",
      " 1.43610089e+00 5.40930206e-02 1.13611734e+00 2.26502388e+00\n",
      " 1.01571561e-01 1.94453249e-02 1.65966948e-09 1.84379333e-02\n",
      " 3.56827865e-01 2.21879546e-04 2.74867101e-02 6.50139393e-01\n",
      " 2.72288944e-05 6.83185616e-10 2.89836934e-02 7.45948889e-06\n",
      " 2.25377178e-03 2.03367127e-09 7.42053377e-03 2.97432415e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[4.01234985e-01 5.83669873e-09 8.06711193e-01 4.74853794e-01\n",
      " 6.55062368e-04 2.06116509e-05 2.94953793e-02 2.35456983e-01\n",
      " 3.07209123e-04 1.86474900e-01 1.09654296e+00 1.01769975e-07\n",
      " 3.42426741e-01 9.61501902e-01 1.45816955e-07 9.93172559e-01\n",
      " 3.19173696e-01 2.21935340e-01 6.36958880e-01 8.90251642e-04\n",
      " 2.84939305e-02 1.16177674e+00 9.71744080e-03 1.39337933e-01\n",
      " 3.16149379e-01 9.75424362e-01 2.11367662e-02 2.82023444e-01\n",
      " 1.27956131e-04 2.15810171e-02 3.70757258e-01 4.68054869e-02\n",
      " 9.99524202e-07 1.74322444e-01 1.49661580e+00 7.08958555e-01\n",
      " 2.43488913e-01 1.31381937e-02 1.30367159e-03 1.80378586e-01\n",
      " 3.42917200e-01 5.84967894e-07 7.54948574e-10 4.47987893e-01\n",
      " 1.01753240e-04 5.92853160e-01 7.28625678e-05 2.06742939e-01\n",
      " 4.01661120e-02 1.91399094e+00 5.42099868e-01 1.01318543e-03\n",
      " 2.39685803e+00 5.37708510e-03 7.91479113e-01 1.70193390e-01\n",
      " 6.40234198e-01 7.52171846e-05 1.61430147e+00 8.44704860e-03\n",
      " 5.69882125e-05 4.81098056e-04 1.21049368e-01 8.44552780e-09\n",
      " 1.23653456e-04 1.24644895e-03 9.38475559e-09 1.66209005e-10\n",
      " 6.38464546e-04 4.54335428e-01 2.83650667e-01 2.19315493e-02\n",
      " 6.81093304e-01 1.55995820e+00 8.18705346e-04 2.34875787e-01\n",
      " 2.68792553e-02 2.69511799e-02 7.81190555e-04 9.89078203e-08\n",
      " 1.46478926e+00 6.56811723e-02 1.07159903e+00 1.22857491e+00\n",
      " 2.19606174e-02 1.57819985e-02 6.25175813e-10 1.51632623e-02\n",
      " 3.59953275e-01 2.27652152e-04 2.74346488e-02 6.54943887e-01\n",
      " 2.12537224e-05 6.66834223e-10 2.87651485e-02 6.65148253e-06\n",
      " 2.17081936e-03 2.02637605e-09 6.93474272e-03 2.96130330e-01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[4.01484651e-01 5.93205267e-09 8.08899114e-01 4.81324903e-01\n",
      " 6.67701000e-04 1.96064417e-05 2.84060080e-02 2.37367985e-01\n",
      " 3.28617688e-04 1.83120387e-01 1.09824031e+00 1.02102948e-07\n",
      " 3.41159990e-01 9.43472991e-01 1.43283863e-07 9.98894335e-01\n",
      " 3.09090587e-01 2.14781477e-01 6.79842850e-01 8.91811653e-04\n",
      " 2.81032508e-02 8.38863740e-01 1.04571622e-02 1.41406212e-01\n",
      " 3.14804402e-01 9.78764389e-01 2.10219315e-02 2.41213830e-01\n",
      " 1.34673069e-04 2.12950685e-02 3.71244540e-01 4.59729809e-02\n",
      " 3.82780130e-06 1.75523101e-01 9.12490079e-01 7.07089919e-01\n",
      " 2.37968722e-01 1.27420066e-02 1.29063960e-03 1.82536677e-01\n",
      " 3.45304436e-01 5.56622376e-07 9.24549851e-10 4.52856253e-01\n",
      " 9.80019305e-05 5.92627529e-01 7.15654311e-05 1.96878071e-01\n",
      " 4.04011275e-02 1.91669278e+00 5.44981567e-01 1.01071145e-03\n",
      " 2.40083040e+00 5.81812604e-03 7.94042917e-01 1.55708423e-01\n",
      " 6.41735818e-01 7.49936140e-05 1.61896315e+00 9.09993601e-03\n",
      " 5.63908891e-05 4.64419812e-04 1.21938496e-01 8.54189814e-09\n",
      " 1.27320514e-04 1.25063039e-03 8.92673532e-09 1.61428690e-10\n",
      " 5.69584686e-04 4.51115828e-01 5.91206871e-01 2.12099595e-02\n",
      " 6.83404644e-01 1.56662179e+00 8.14012675e-04 2.32332869e-01\n",
      " 2.67529181e-02 2.65972290e-02 7.93019609e-04 1.04714437e-07\n",
      " 1.45952751e+00 6.42843066e-02 1.04986165e+00 1.16100739e+00\n",
      " 2.21110673e-02 1.60812979e-02 6.45909325e-10 1.53085681e-02\n",
      " 3.53345727e-01 2.40969130e-04 2.72141795e-02 6.56243027e-01\n",
      " 2.13990478e-05 6.90769896e-10 2.87143855e-02 6.60674401e-06\n",
      " 2.30666409e-03 2.00120939e-09 7.45181039e-03 2.98056355e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[4.00057413e-01 6.04813404e-09 8.10925040e-01 4.80125837e-01\n",
      " 6.22388635e-04 1.87935060e-05 2.75756799e-02 2.38011186e-01\n",
      " 3.18341131e-04 1.83598170e-01 1.09821221e+00 1.02447555e-07\n",
      " 3.45503492e-01 9.47864509e-01 1.40903673e-07 9.98750087e-01\n",
      " 3.13075492e-01 2.16415541e-01 6.32909155e-01 8.83747682e-04\n",
      " 2.90020245e-02 8.42084903e-01 1.02088610e-02 1.42148890e-01\n",
      " 6.34667446e-01 9.77485083e-01 2.06757671e-02 2.40876217e-01\n",
      " 1.28400952e-04 2.11741676e-02 3.72116235e-01 4.53553981e-02\n",
      " 1.06176974e-06 1.73493693e-01 9.10322052e-01 7.07885331e-01\n",
      " 2.40746314e-01 1.26305775e-02 1.28535501e-03 1.82040274e-01\n",
      " 3.52139753e-01 5.31439197e-07 7.10340312e-10 4.54361491e-01\n",
      " 9.47589484e-05 5.92723319e-01 7.25827343e-05 1.98949703e-01\n",
      " 4.06039960e-02 1.91583626e+00 5.48736956e-01 1.01043878e-03\n",
      " 2.39800911e+00 5.66061206e-03 7.96461654e-01 1.54756768e-01\n",
      " 6.39904623e-01 7.40146275e-05 1.61242545e+00 8.80829689e-03\n",
      " 5.59636136e-05 4.74863605e-04 1.20290153e-01 8.66332562e-09\n",
      " 1.28430922e-04 1.19669855e-03 8.52499411e-09 1.56875263e-10\n",
      " 5.74659408e-04 4.51425534e-01 2.85752841e-01 4.25082023e-02\n",
      " 6.85539538e-01 1.56987495e+00 8.05287483e-04 2.37802375e-01\n",
      " 2.70347444e-02 2.72607696e-02 1.44605280e-03 9.04797523e-08\n",
      " 1.45775596e+00 6.75245404e-02 1.05594445e+00 1.16428153e+00\n",
      " 2.14111652e-02 1.53033332e-02 6.57081305e-10 1.44922949e-02\n",
      " 3.55056492e-01 2.35913614e-04 2.70656246e-02 6.54776031e-01\n",
      " 2.05151436e-05 7.09233210e-10 3.01274329e-02 6.40645584e-06\n",
      " 2.25270148e-03 2.08567441e-09 7.16611955e-03 2.99769915e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[3.99393608e-01 5.87875662e-09 8.08684830e-01 4.75096078e-01\n",
      " 5.63810796e-04 1.92541917e-05 2.80628167e-02 5.69830071e-01\n",
      " 3.07635504e-04 1.84670124e-01 1.10297950e+00 1.00607523e-07\n",
      " 3.53310001e-01 9.55024377e-01 1.42167640e-07 9.95252912e-01\n",
      " 3.15846659e-01 2.20586271e-01 6.34463949e-01 9.11242716e-04\n",
      " 3.20742687e-02 8.56855215e-01 1.00061847e-02 1.40989974e-01\n",
      " 3.22555717e-01 9.78734093e-01 2.06553216e-02 6.00009510e-01\n",
      " 1.26501644e-04 2.20314801e-02 3.70956351e-01 4.60370645e-02\n",
      " 1.04134623e-06 1.66400825e-01 9.02171455e-01 7.06832464e-01\n",
      " 2.42115647e-01 1.41232276e-02 1.28616071e-03 1.80198475e-01\n",
      " 3.53407412e-01 5.46363459e-07 7.13873653e-10 4.49562815e-01\n",
      " 9.62298282e-05 5.90516989e-01 8.21240175e-05 2.04847758e-01\n",
      " 4.26529462e-02 1.91214235e+00 5.42990963e-01 9.90074439e-04\n",
      " 2.38139201e+00 5.27187717e-03 7.93672235e-01 1.55334434e-01\n",
      " 6.30801441e-01 7.34371827e-05 1.58284491e+00 8.55424737e-03\n",
      " 1.65828047e-04 5.06345903e-04 1.13619073e-01 8.42491488e-09\n",
      " 1.22040306e-04 1.10954868e-03 8.75825291e-09 1.59402727e-10\n",
      " 6.22238518e-04 4.52784265e-01 2.83253973e-01 2.12275805e-02\n",
      " 6.82851612e-01 1.56276448e+00 7.99936036e-04 2.33159877e-01\n",
      " 3.00915446e-02 3.10076354e-02 6.67644186e-04 9.21443671e-08\n",
      " 1.44871446e+00 7.14529846e-02 1.12069351e+00 1.15799039e+00\n",
      " 1.88891780e-02 1.40588110e-02 6.45554650e-10 1.33941163e-02\n",
      " 3.57671949e-01 2.19938612e-04 4.53616552e-02 6.47839528e-01\n",
      " 1.93328286e-05 6.82982112e-10 4.74032986e-02 6.19217117e-06\n",
      " 2.15952458e-03 1.97072905e-09 7.05708344e-03 2.97196753e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[6.94991980e-01 5.92647728e-09 8.09232091e-01 4.77500061e-01\n",
      " 5.41786809e-04 1.88625280e-05 2.77147956e-02 2.38053358e-01\n",
      " 3.42169066e-04 1.82766466e-01 1.10542531e+00 1.00314404e-07\n",
      " 7.11154364e-01 9.31091457e-01 1.41232229e-07 9.97357803e-01\n",
      " 3.01137783e-01 2.17540894e-01 6.32061339e-01 1.46998444e-03\n",
      " 3.26250170e-02 9.50743509e-01 1.11663546e-02 1.42510076e-01\n",
      " 3.24181131e-01 9.83678588e-01 2.04719153e-02 2.40153353e-01\n",
      " 1.25726664e-04 2.09537437e-02 3.69409839e-01 4.48501426e-02\n",
      " 1.05562721e-06 1.65639471e-01 9.07983359e-01 7.62414020e-01\n",
      " 2.34161215e-01 1.35038773e-02 1.28196256e-03 1.80931964e-01\n",
      " 3.46555727e-01 5.35503201e-07 7.03715501e-10 4.74845141e-01\n",
      " 9.43111143e-05 5.89652584e-01 8.33516184e-05 2.00877646e-01\n",
      " 4.71917446e-02 1.91386572e+00 5.46391949e-01 9.83426133e-04\n",
      " 2.38082581e+00 5.41265204e-03 7.94272461e-01 1.53425540e-01\n",
      " 6.30126091e-01 7.34700655e-05 1.58029986e+00 9.60412148e-03\n",
      " 5.59536155e-05 5.12060479e-04 1.12795119e-01 8.43161484e-09\n",
      " 1.28200511e-04 1.08366578e-03 8.58968128e-09 1.57694191e-10\n",
      " 5.86818646e-04 4.49581680e-01 2.85715845e-01 2.06179450e-02\n",
      " 6.83376840e-01 1.57083767e+00 7.94817635e-04 2.29760986e-01\n",
      " 3.08727172e-02 3.18806067e-02 6.46121428e-04 9.00549962e-08\n",
      " 1.45449001e+00 7.29245158e-02 1.03094801e+00 1.19654228e+00\n",
      " 1.83616399e-02 1.37790993e-02 6.54102840e-10 1.29243383e-02\n",
      " 3.48562844e-01 2.27099508e-04 2.69485842e-02 6.47571616e-01\n",
      " 1.87671873e-05 6.87631309e-10 3.07968808e-02 6.05965294e-06\n",
      " 2.39346650e-03 1.95455903e-09 8.00848214e-03 2.97509215e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[3.99572731e-01 5.88332142e-09 8.08894637e-01 4.77085200e-01\n",
      " 6.01987648e-04 1.88714361e-05 2.78288682e-02 2.42475294e-01\n",
      " 3.37602418e-04 1.82135167e-01 2.47812221e+00 9.97144469e-08\n",
      " 6.86678988e-01 9.32784235e-01 1.41085217e-07 9.97452227e-01\n",
      " 3.09362337e-01 2.17489953e-01 6.22004685e-01 8.60784065e-04\n",
      " 3.33998810e-02 8.45629266e-01 1.11287253e-02 1.47654078e-01\n",
      " 3.17374740e-01 9.83177679e-01 1.99803176e-02 2.41141812e-01\n",
      " 1.24789985e-04 2.05648595e-02 3.64861271e-01 8.29159839e-02\n",
      " 1.04484162e-06 1.65029097e-01 1.46815184e+00 1.15490549e+00\n",
      " 2.34796492e-01 1.31098290e-02 1.25658894e-03 1.80819402e-01\n",
      " 3.56735725e-01 5.33814578e-07 7.75743579e-10 4.68758578e-01\n",
      " 9.41915695e-05 5.89098645e-01 3.42289304e-04 2.04543749e-01\n",
      " 4.17906431e-02 1.91508532e+00 5.54307489e-01 9.77729911e-04\n",
      " 2.38085171e+00 5.35550159e-03 7.93868865e-01 1.48589940e-01\n",
      " 6.29486663e-01 7.32713583e-05 1.57713326e+00 9.49510844e-03\n",
      " 6.01880700e-05 4.62614493e-04 1.12198765e-01 8.37685320e-09\n",
      " 1.28583841e-04 1.16964188e-03 8.58007694e-09 1.57306945e-10\n",
      " 5.84613712e-04 4.49064060e-01 3.43780032e-01 2.04866122e-02\n",
      " 6.83004677e-01 1.58934597e+00 7.92501764e-04 2.21476810e-01\n",
      " 3.15075681e-02 3.24189533e-02 7.22563443e-04 8.98158307e-08\n",
      " 1.45425976e+00 6.66998554e-02 2.74185425e+00 1.19195487e+00\n",
      " 1.80136019e-02 1.52987623e-02 6.48745431e-10 1.41376563e-02\n",
      " 3.49399006e-01 2.22868104e-04 2.83884266e-02 6.47199333e-01\n",
      " 1.99231120e-05 6.80144867e-10 2.87323451e-02 6.12781980e-06\n",
      " 2.34628060e-03 1.94317915e-09 7.77520488e-03 2.96898211e-01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[3.99433645e-01 6.22144684e-09 8.14221876e-01 4.78264637e-01\n",
      " 6.64164452e-04 1.72103416e-05 2.62107394e-02 2.43481775e-01\n",
      " 3.26236473e-04 1.81175845e-01 1.11136994e+00 1.02418134e-07\n",
      " 3.38804179e-01 9.38857643e-01 1.36438388e-07 9.98664877e-01\n",
      " 3.05593648e-01 2.16018631e-01 6.20050448e-01 8.66230468e-04\n",
      " 3.56490338e-02 8.39018083e-01 1.06689487e-02 1.49067538e-01\n",
      " 3.09858204e-01 9.82796219e-01 2.17201881e-02 2.41210398e-01\n",
      " 1.59349864e-04 2.04593108e-02 3.63777040e-01 4.28180058e-02\n",
      " 1.10770782e-06 1.59855347e-01 9.10213788e-01 7.03179554e-01\n",
      " 2.37054430e-01 1.16372757e-02 1.25088968e-03 1.81411477e-01\n",
      " 3.59047498e-01 4.91738113e-07 6.70151252e-10 4.74947510e-01\n",
      " 8.71212989e-05 5.90124467e-01 9.17348761e-05 1.98090834e-01\n",
      " 4.24193229e-02 1.91238774e+00 5.55838039e-01 9.88214767e-04\n",
      " 2.36936806e+00 5.40063410e-03 7.99504543e-01 1.48377240e-01\n",
      " 6.23905356e-01 7.29384601e-05 1.55637148e+00 9.18014192e-03\n",
      " 5.67963149e-05 4.23604417e-04 1.07542255e-01 8.64826071e-09\n",
      " 1.23671220e-04 1.26153390e-03 7.89251828e-09 1.50427393e-10\n",
      " 5.71915304e-04 4.49499273e-01 2.84822694e-01 2.04592723e-02\n",
      " 6.88098880e-01 1.59415764e+00 7.99748100e-04 2.19617691e-01\n",
      " 3.39335757e-02 3.53640632e-02 8.05113043e-04 8.26047377e-08\n",
      " 2.09446831e+00 6.12251246e-02 1.04323623e+00 1.19360053e+00\n",
      " 1.64825626e-02 1.68583013e-02 6.85088235e-10 1.68336569e-02\n",
      " 3.52434650e-01 2.29710345e-04 2.69234676e-02 6.42298329e-01\n",
      " 2.11386465e-05 7.36072768e-10 2.96699818e-02 6.25529317e-06\n",
      " 2.27776158e-03 1.89849539e-09 7.51086293e-03 3.01920033e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[3.99633359e-01 6.37768052e-09 8.16381858e-01 4.78079508e-01\n",
      " 5.90275108e-04 1.65140277e-05 2.54982478e-02 2.45258886e-01\n",
      " 3.27177218e-04 1.80853624e-01 1.11307452e+00 1.03508485e-07\n",
      " 3.47746468e-01 1.65618731e+00 1.34615094e-07 9.98951830e-01\n",
      " 3.05288270e-01 2.16448163e-01 6.19689814e-01 9.28346174e-04\n",
      " 3.79294393e-02 8.40345144e-01 1.08406371e-02 2.46466179e-01\n",
      " 3.16649750e-01 9.83493006e-01 1.99526859e-02 2.41075301e-01\n",
      " 2.33194223e-04 2.03437292e-02 3.63778447e-01 4.37350606e-02\n",
      " 1.13710228e-06 1.55166680e-01 9.09796012e-01 7.02990558e-01\n",
      " 1.18683393e+00 1.15241444e-02 1.24161916e-03 1.83700616e-01\n",
      " 4.86414102e-01 4.72747975e-07 8.53748223e-10 4.72499591e-01\n",
      " 8.40083285e-05 5.90336246e-01 9.78715313e-05 1.98050444e-01\n",
      " 4.22664333e-02 1.90922733e+00 5.55369187e-01 9.92696527e-04\n",
      " 2.35936727e+00 5.36226424e-03 8.01874833e-01 1.48222579e-01\n",
      " 6.22899722e-01 7.23430916e-05 1.53903670e+00 9.20583965e-03\n",
      " 5.80948116e-05 4.56827392e-04 1.03514955e-01 8.78727283e-09\n",
      " 1.27476882e-04 1.15142046e-03 7.58888282e-09 1.47355669e-10\n",
      " 5.68681304e-04 4.47998345e-01 2.83110036e-01 2.00971315e-02\n",
      " 6.90280900e-01 1.59526055e+00 8.09827661e-04 2.20317549e-01\n",
      " 3.57709789e-02 3.77467236e-02 7.13744011e-04 7.93475417e-08\n",
      " 1.45711203e+00 1.12240959e-01 1.04399224e+00 1.19454757e+00\n",
      " 1.53337023e-02 1.52234160e-02 7.01648960e-10 1.40547801e-02\n",
      " 3.51341584e-01 2.23771490e-04 2.68782664e-02 6.37588994e-01\n",
      " 1.95740549e-05 7.60136768e-10 2.89906362e-02 6.02511962e-06\n",
      " 2.27912096e-03 1.88314561e-09 7.45292059e-03 3.03887648e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[3.98852219e-01 6.26285200e-09 8.14355515e-01 4.80020158e-01\n",
      " 6.11860153e-04 1.68670545e-05 2.57574845e-02 2.43728016e-01\n",
      " 3.25967718e-04 1.79097444e-01 1.11177701e+00 1.01970188e-07\n",
      " 3.44282983e-01 9.35495095e-01 1.35797389e-07 1.00047097e+00\n",
      " 3.11979904e-01 2.13702751e-01 6.19552357e-01 8.97911106e-04\n",
      " 3.56510183e-02 8.51378516e-01 1.07580144e-02 1.57193983e-01\n",
      " 3.13292284e-01 9.85148011e-01 1.96432969e-02 2.41383728e-01\n",
      " 1.27617771e-04 2.78347363e-02 3.64012518e-01 4.17136200e-02\n",
      " 1.12473298e-06 1.58967433e-01 9.14233996e-01 7.01427797e-01\n",
      " 2.36201987e-01 1.14353710e-02 1.23336311e-03 1.82215472e-01\n",
      " 3.59580449e-01 4.81696436e-07 1.93154325e-09 4.72780645e-01\n",
      " 8.53186043e-05 5.88680968e-01 9.10587150e-05 1.93773306e-01\n",
      " 4.22117702e-02 1.91272381e+00 5.56798654e-01 9.78410596e-04\n",
      " 2.36928137e+00 5.48108104e-03 7.99787052e-01 1.46590633e-01\n",
      " 6.22620208e-01 7.37469640e-05 1.55500000e+00 9.28546460e-03\n",
      " 5.60892706e-05 4.30502159e-04 1.06920596e-01 8.65177186e-09\n",
      " 1.32548204e-04 1.19388866e-03 7.72471964e-09 1.48738882e-10\n",
      " 5.43683610e-04 4.50610598e-01 2.83554512e-01 2.01086572e-02\n",
      " 6.88254638e-01 1.82158639e+00 7.91143642e-04 2.19027355e-01\n",
      " 3.38396528e-02 3.56107907e-02 1.45850975e-03 8.05913964e-08\n",
      " 1.46135090e+00 6.39254783e-02 1.04232719e+00 1.19506108e+00\n",
      " 1.62528325e-02 1.58826894e-02 6.93756870e-10 1.46682320e-02\n",
      " 3.51140133e-01 2.24336794e-04 2.68038956e-02 6.41685512e-01\n",
      " 1.99273565e-05 7.41613086e-10 2.87257815e-02 5.99208836e-06\n",
      " 2.26923153e-03 1.88488231e-09 7.56777989e-03 3.01773843e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[3.98713734e-01 6.22992995e-09 8.13929728e-01 4.79626554e-01\n",
      " 6.11077316e-04 1.68677251e-05 2.57199261e-02 2.44175060e-01\n",
      " 3.11352940e-04 1.79135286e-01 1.11384906e+00 1.01152235e-07\n",
      " 3.44049772e-01 9.44334705e-01 1.35594129e-07 1.00038560e+00\n",
      " 3.09229075e-01 2.14532394e-01 6.18917488e-01 8.63189301e-04\n",
      " 3.68200729e-02 8.31851325e-01 1.03560556e-02 1.49570010e-01\n",
      " 3.13100459e-01 1.53893553e+00 1.96213862e-02 2.41643421e-01\n",
      " 1.25862381e-04 2.01521707e-02 3.71745272e-01 4.98563739e-02\n",
      " 1.12242647e-06 1.56511802e-01 9.13622650e-01 7.01421955e-01\n",
      " 2.42277500e-01 1.13620711e-02 1.22416554e-03 2.47833809e-01\n",
      " 3.61702330e-01 4.79086983e-07 6.53682079e-10 4.73962608e-01\n",
      " 8.52651344e-05 5.88189889e-01 9.40472363e-05 1.94494168e-01\n",
      " 4.21297331e-02 1.91202075e+00 6.58920287e-01 9.73765613e-04\n",
      " 2.36504461e+00 5.81276207e-03 3.17290790e+00 1.46093703e-01\n",
      " 6.19872080e-01 7.23119860e-05 1.54618425e+00 8.84179509e-03\n",
      " 5.71104879e-05 4.33722106e-04 1.04906335e-01 8.60671722e-09\n",
      " 1.18490822e-04 1.19082251e-03 7.68449569e-09 1.47973328e-10\n",
      " 5.49809383e-04 4.49597294e-01 3.41158523e-01 2.01958262e-02\n",
      " 6.87981486e-01 1.59764527e+00 7.98813624e-04 2.72655435e-01\n",
      " 3.49617669e-02 3.70843940e-02 7.52006517e-04 8.02763241e-08\n",
      " 1.46089299e+00 6.35677586e-02 1.05648976e+00 1.19601550e+00\n",
      " 1.55812068e-02 1.57565535e-02 6.96823417e-10 1.45637491e-02\n",
      " 3.54454762e-01 2.20793643e-04 2.66731893e-02 6.39568922e-01\n",
      " 1.99156110e-05 7.34777720e-10 2.91772272e-02 5.93999202e-06\n",
      " 2.17001230e-03 1.87906424e-09 7.28545057e-03 3.01184412e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[3.98857092e-01 5.94899242e-09 8.09086477e-01 4.83184439e-01\n",
      " 6.37830774e-04 1.81607408e-05 2.69994439e-02 2.48413775e-01\n",
      " 3.02703693e-04 1.77489165e-01 1.11557557e+00 9.78511962e-08\n",
      " 3.40523947e-01 9.50859082e-01 1.39071491e-07 1.00342932e+00\n",
      " 3.12490020e-01 2.11152125e-01 6.13423507e-01 8.69740154e-04\n",
      " 3.84144202e-02 8.42345293e-01 1.00844461e-02 1.54806698e-01\n",
      " 3.10787698e-01 9.85019317e-01 1.92919530e-02 2.41821447e-01\n",
      " 1.28076926e-04 2.00958606e-02 3.60237681e-01 4.04257075e-02\n",
      " 1.06508134e-06 1.53488963e-01 9.20062959e-01 9.85287143e-01\n",
      " 2.40880827e-01 1.22747919e-02 1.22002395e-03 1.83382731e-01\n",
      " 3.70425195e-01 5.09272811e-07 6.74141463e-10 4.86408961e-01\n",
      " 9.02270091e-05 5.86080149e-01 9.94799101e-05 1.89640557e-01\n",
      " 4.58690483e-02 1.91017483e+00 5.62969941e-01 9.52811258e-04\n",
      " 2.35893569e+00 9.51152687e-03 7.94482399e-01 1.60047794e-01\n",
      " 6.16406370e-01 7.21105906e-05 1.53453255e+00 8.59395803e-03\n",
      " 5.73788454e-05 4.90335609e-04 1.02249030e-01 8.29612714e-09\n",
      " 1.17502471e-04 1.22564005e-03 8.17348648e-09 1.52543922e-10\n",
      " 5.12948861e-04 4.50458829e-01 2.84380518e-01 2.03657949e-02\n",
      " 6.83210930e-01 1.61491027e+00 8.01155847e-04 2.12982149e-01\n",
      " 3.63114188e-02 3.87720431e-02 7.88089092e-04 8.51237760e-08\n",
      " 1.46830529e+00 6.11885762e-02 1.06220228e+00 1.21694794e+00\n",
      " 1.48342939e-02 1.63782587e-02 6.57966999e-10 1.50492925e-02\n",
      " 3.57062154e-01 2.28736064e-04 2.68303051e-02 6.36754978e-01\n",
      " 2.16472428e-05 6.80881126e-10 5.13363362e-02 5.97624427e-06\n",
      " 2.10894076e-03 1.89702820e-09 7.10810555e-03 2.96229186e-01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[3.99890319e-01 6.63123362e-09 8.09003793e-01 4.87339167e-01\n",
      " 6.54361059e-04 1.80908608e-05 2.69671193e-02 2.47274954e-01\n",
      " 2.94177866e-04 1.75301551e-01 1.11921734e+00 9.73262706e-08\n",
      " 3.38203314e-01 9.56644437e-01 1.38677065e-07 1.00695415e+00\n",
      " 3.15900404e-01 2.06421325e-01 6.15246731e-01 8.85144012e-04\n",
      " 4.09833683e-02 8.03797791e-01 5.19526775e-02 1.53642872e-01\n",
      " 3.08867317e-01 9.84291536e-01 1.95297094e-02 2.41047854e-01\n",
      " 1.31413752e-04 2.04620860e-02 3.62258746e-01 4.08845245e-02\n",
      " 1.06000422e-06 1.49139418e-01 9.28562730e-01 6.99525262e-01\n",
      " 2.43058289e-01 1.10860846e-02 1.21302455e-03 1.85018414e-01\n",
      " 4.83852743e-01 5.08408212e-07 6.69875653e-10 4.82211309e-01\n",
      " 8.99795343e-05 5.85582847e-01 1.05697777e-04 1.83228009e-01\n",
      " 4.27844277e-02 1.90762930e+00 5.61576012e-01 9.47065503e-04\n",
      " 2.34946412e+00 6.03149037e-03 7.94306557e-01 1.45493277e-01\n",
      " 6.11070506e-01 7.28729702e-05 1.51716474e+00 8.32467648e-03\n",
      " 5.92271637e-05 4.07508200e-04 9.83363548e-02 8.24591774e-09\n",
      " 1.33726106e-04 1.24360008e-03 8.20672811e-09 1.80301538e-10\n",
      " 4.71851017e-04 4.51019593e-01 2.86106260e-01 2.02597350e-02\n",
      " 6.82916583e-01 1.60921372e+00 8.19613706e-04 2.15034783e-01\n",
      " 3.86781791e-02 4.15539939e-02 8.06366253e-04 8.48483532e-08\n",
      " 1.47744224e+00 5.98276527e-02 1.12576024e+00 1.20832115e+00\n",
      " 1.37933332e-02 3.25051919e-02 6.54074987e-10 1.54465642e-02\n",
      " 3.59316932e-01 4.89963544e-04 2.67116940e-02 6.32451668e-01\n",
      " 2.05911993e-05 6.77185485e-10 2.86161784e-02 6.00130610e-06\n",
      " 2.04641284e-03 1.88627127e-09 7.02781943e-03 2.95851552e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[3.99015764e-01 5.82475906e-09 8.08801156e-01 4.91443031e-01\n",
      " 6.13570880e-04 1.80634566e-05 2.70131569e-02 2.46480388e-01\n",
      " 2.88957602e-04 1.73352472e-01 1.11784231e+00 9.67510701e-08\n",
      " 3.42677295e-01 9.58833202e-01 1.38472943e-07 1.01037780e+00\n",
      " 3.17655058e-01 2.01953474e-01 6.24842263e-01 9.02357854e-04\n",
      " 3.88433167e-02 7.87754518e-01 9.72391976e-03 1.51581306e-01\n",
      " 3.11474598e-01 9.83985534e-01 2.43685774e-02 2.40200358e-01\n",
      " 1.38506637e-04 1.99738884e-02 3.63300869e-01 4.13179422e-02\n",
      " 1.05325489e-06 1.52513423e-01 9.37010614e-01 6.98331194e-01\n",
      " 2.44235318e-01 1.10978213e-02 1.20516494e-03 1.86879179e-01\n",
      " 3.64166521e-01 5.10966813e-07 6.67103399e-10 4.79088827e-01\n",
      " 8.99841483e-05 5.85090040e-01 9.90454199e-05 1.77266930e-01\n",
      " 4.22739847e-02 1.91115360e+00 5.58866806e-01 9.40319555e-04\n",
      " 2.35849833e+00 6.29266400e-03 7.93812860e-01 1.44454486e-01\n",
      " 6.20599837e-01 7.41598591e-05 1.53150534e+00 8.17102261e-03\n",
      " 5.69525430e-05 4.18366676e-04 1.01284550e-01 8.18342851e-09\n",
      " 1.12063742e-04 1.18697268e-03 8.18045271e-09 1.52086718e-10\n",
      " 4.35172499e-04 4.50472997e-01 2.88010497e-01 2.01421736e-02\n",
      " 6.82479790e-01 1.60583869e+00 7.98615829e-04 2.16426235e-01\n",
      " 3.68423366e-02 3.95315200e-02 1.47172285e-03 8.48585984e-08\n",
      " 1.48669228e+00 6.24585625e-02 1.07554489e+00 1.20373942e+00\n",
      " 1.45221734e-02 1.58547419e-02 6.49654079e-10 1.47259863e-02\n",
      " 3.60426140e-01 2.49517742e-04 2.76362344e-02 6.36286262e-01\n",
      " 1.96854298e-05 6.71027258e-10 2.84115703e-02 5.84288527e-06\n",
      " 2.00322276e-03 1.98767480e-09 6.73361913e-03 2.95409175e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[3.99392420e-01 5.63714691e-09 8.05480193e-01 4.90166306e-01\n",
      " 6.11292532e-04 1.88977686e-05 2.77705634e-02 2.46175850e-01\n",
      " 2.93419767e-04 1.89803425e-01 1.13266688e+00 9.48446606e-08\n",
      " 3.43053056e-01 9.54111808e-01 1.41392216e-07 1.00960507e+00\n",
      " 3.15097508e-01 2.03901989e-01 6.18537555e-01 8.84485251e-04\n",
      " 4.05210463e-02 7.94955220e-01 9.86041194e-03 1.50783828e-01\n",
      " 3.10921275e-01 9.85305839e-01 2.11080858e-02 2.39839509e-01\n",
      " 1.31507927e-04 1.99234277e-02 3.62864038e-01 4.14117928e-02\n",
      " 1.02543222e-06 1.49103878e-01 9.34746625e-01 6.99049621e-01\n",
      " 2.42744388e-01 1.11761515e-02 1.20076917e-03 1.86157442e-01\n",
      " 3.61594299e-01 5.30719588e-07 2.05547036e-09 4.78704084e-01\n",
      " 9.31709708e-05 5.83270774e-01 1.03512771e-04 1.79863484e-01\n",
      " 4.27372329e-02 1.90974100e+00 5.58403781e-01 9.27091097e-04\n",
      " 2.35307098e+00 6.13910607e-03 7.90336630e-01 1.44314057e-01\n",
      " 6.11765555e-01 7.29748824e-05 1.51997176e+00 8.31161643e-03\n",
      " 2.07801024e-04 4.14684130e-04 9.84474233e-02 8.00915034e-09\n",
      " 1.12688597e-04 1.18710291e-03 8.50568578e-09 1.55622556e-10\n",
      " 4.46086035e-04 4.50368587e-01 2.87391546e-01 2.05962414e-02\n",
      " 6.79216011e-01 2.92557413e+00 8.07113723e-04 4.51319915e-01\n",
      " 3.80173454e-02 4.12966577e-02 7.59921867e-04 8.79549607e-08\n",
      " 1.48383074e+00 6.22554929e-02 1.06821829e+00 1.20124286e+00\n",
      " 1.38176387e-02 1.58798957e-02 8.66369072e-10 1.47064972e-02\n",
      " 3.58656757e-01 2.43498520e-04 2.78092351e-02 6.33173908e-01\n",
      " 1.96552823e-05 6.40074657e-10 2.85265957e-02 5.79767422e-06\n",
      " 2.03338499e-03 1.90075793e-09 6.91941384e-03 2.92073362e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[3.99968793e-01 5.40344136e-09 8.01732903e-01 4.88731578e-01\n",
      " 6.93897734e-04 2.29288624e-05 2.88637084e-02 6.16181780e-01\n",
      " 3.10609703e-04 1.89906147e-01 1.12110687e+00 9.28387401e-08\n",
      " 3.32198958e-01 9.41973754e-01 1.44944474e-07 1.00879341e+00\n",
      " 3.07232673e-01 2.05164741e-01 6.18972522e-01 8.73841287e-04\n",
      " 4.18580352e-02 7.99264083e-01 1.04721112e-02 1.49591188e-01\n",
      " 3.03015474e-01 9.88162558e-01 1.92416964e-02 2.39413334e-01\n",
      " 1.29795404e-04 1.98541432e-02 3.63438189e-01 4.36141477e-02\n",
      " 9.82552819e-07 1.46548911e-01 9.31809258e-01 7.05971824e-01\n",
      " 2.38136964e-01 1.14674697e-02 1.27261869e-03 1.85444312e-01\n",
      " 3.59513141e-01 5.57007315e-07 6.98858733e-10 4.75949047e-01\n",
      " 9.74709716e-05 5.81797661e-01 1.06026499e-04 1.81553221e-01\n",
      " 4.16259367e-02 1.90893223e+00 5.57009875e-01 9.16664734e-04\n",
      " 2.34903292e+00 5.97588725e-03 7.86408315e-01 1.45413074e-01\n",
      " 6.09042661e-01 7.23818195e-05 1.51124641e+00 8.81626295e-03\n",
      " 5.87167261e-05 3.79931449e-04 9.63274642e-02 7.80977651e-09\n",
      " 1.15457236e-04 1.31129886e-03 9.85386877e-09 1.60031738e-10\n",
      " 4.56989299e-04 4.49649429e-01 2.86490582e-01 1.97773717e-02\n",
      " 6.75496517e-01 1.60206306e+00 8.13649224e-04 2.18516622e-01\n",
      " 3.92148280e-02 4.28305294e-02 8.69787253e-04 9.22004553e-08\n",
      " 1.48122708e+00 5.59500096e-02 1.04929869e+00 1.19753205e+00\n",
      " 1.32631472e-02 1.77473112e-02 6.27615334e-10 1.63646858e-02\n",
      " 3.53905816e-01 2.37628425e-04 2.69897757e-02 6.30955375e-01\n",
      " 2.13524718e-05 6.02009065e-10 2.85913610e-02 6.05470870e-06\n",
      " 2.13066826e-03 2.01747556e-09 7.42843590e-03 2.88221394e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[3.99848841e-01 5.33411166e-09 8.00638304e-01 4.89717104e-01\n",
      " 6.99948413e-04 2.02086875e-05 2.91143934e-02 2.46769001e-01\n",
      " 3.12621971e-04 1.74034566e-01 1.12177261e+00 9.21286086e-08\n",
      " 3.31343483e-01 9.40710420e-01 1.45980472e-07 1.00956669e+00\n",
      " 3.06035467e-01 2.04527253e-01 6.25562381e-01 9.00765935e-04\n",
      " 4.18871079e-02 1.14518592e+00 1.05087694e-02 1.59567075e-01\n",
      " 3.02022700e-01 9.87282844e-01 1.91016675e-02 2.39236146e-01\n",
      " 2.42895328e-04 1.98174029e-02 3.62795799e-01 4.15090642e-02\n",
      " 3.54955197e-06 1.46411057e-01 9.33563568e-01 6.99130415e-01\n",
      " 2.37717336e-01 1.12782227e-02 1.21033135e-03 1.85664950e-01\n",
      " 3.62345853e-01 5.64052155e-07 7.01742926e-10 4.77182328e-01\n",
      " 9.84067462e-05 5.80934015e-01 1.05977746e-04 1.81040782e-01\n",
      " 4.16135790e-02 1.90905409e+00 5.57557398e-01 2.29827788e-03\n",
      " 2.34916391e+00 6.02925377e-03 7.85151262e-01 1.44319122e-01\n",
      " 6.08836872e-01 7.22371959e-05 1.51096110e+00 8.85532291e-03\n",
      " 5.85381465e-05 3.77718341e-04 9.60612699e-02 7.74079398e-09\n",
      " 1.17300197e-04 1.31510377e-03 9.04189436e-09 1.61559377e-10\n",
      " 4.49181454e-04 4.49994612e-01 2.86914305e-01 1.97632029e-02\n",
      " 6.74234107e-01 3.22011712e+00 8.11589141e-04 2.17673519e-01\n",
      " 3.90201991e-02 4.27658632e-02 8.76920918e-04 9.31305352e-08\n",
      " 1.48301999e+00 5.53981630e-02 1.04517436e+00 1.20017441e+00\n",
      " 1.32250047e-02 1.77851646e-02 6.01110424e-10 1.64970272e-02\n",
      " 3.53361000e-01 2.38922276e-04 2.63365696e-02 6.30862360e-01\n",
      " 2.31299522e-05 5.91461655e-10 3.19865466e-02 6.05565293e-06\n",
      " 2.14287020e-03 1.92489771e-09 7.59265649e-03 2.86981974e-01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[3.99859112e-01 5.32501205e-09 8.00648921e-01 4.94224468e-01\n",
      " 6.49985829e-04 2.01093250e-05 2.90269216e-02 2.45516146e-01\n",
      " 3.10464046e-04 1.72644977e-01 1.12303269e+00 9.17847482e-08\n",
      " 3.36912787e-01 9.40864898e-01 1.45613031e-07 1.01250864e+00\n",
      " 3.05810558e-01 2.00971872e-01 6.33306702e-01 8.83370772e-04\n",
      " 4.25315653e-02 7.85449100e-01 1.05036455e-02 1.58530169e-01\n",
      " 3.05834322e-01 9.88294780e-01 2.04886434e-02 2.38862099e-01\n",
      " 1.31553185e-04 1.97648568e-02 3.63311995e-01 4.16514055e-02\n",
      " 9.73266343e-07 1.45415337e-01 9.40327038e-01 6.98705106e-01\n",
      " 2.37382452e-01 1.11117711e-02 1.19152388e-03 1.87267686e-01\n",
      " 3.59516488e-01 5.60386494e-07 6.97765233e-10 4.76167251e-01\n",
      " 9.80644153e-05 5.80611304e-01 1.07803255e-04 1.76382012e-01\n",
      " 4.15355525e-02 1.90906537e+00 5.77257928e-01 9.07240167e-04\n",
      " 2.34749251e+00 6.32115331e-03 7.85224507e-01 1.44471381e-01\n",
      " 6.07801197e-01 8.40392341e-05 1.50682261e+00 8.86059476e-03\n",
      " 5.88801131e-05 3.91374000e-04 9.51369883e-02 7.72137999e-09\n",
      " 1.15067622e-04 1.66497907e-03 8.98395425e-09 4.01795666e-10\n",
      " 4.19989362e-04 4.50603888e-01 3.36953738e-01 2.02277389e-02\n",
      " 6.74264889e-01 1.75748683e+00 8.15527485e-04 2.18004228e-01\n",
      " 3.96216452e-02 4.36518269e-02 8.10664821e-04 9.29282554e-08\n",
      " 1.49059550e+00 5.91694899e-02 1.04489784e+00 1.19839631e+00\n",
      " 1.29117310e-02 3.24024286e-02 6.00749546e-10 1.56119126e-02\n",
      " 3.53185546e-01 2.48628644e-04 2.65049091e-02 6.30030616e-01\n",
      " 2.03931280e-05 5.90055307e-10 2.79837334e-02 5.90000300e-06\n",
      " 2.13819351e-03 1.90759156e-09 7.58438377e-03 2.86840384e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[4.02000217e-01 5.39585205e-09 8.02081205e-01 4.92596946e-01\n",
      " 6.58794955e-04 1.95243015e-05 2.85227564e-02 2.46952664e-01\n",
      " 3.20543510e-04 1.73255191e-01 1.12571239e+00 9.21995223e-08\n",
      " 3.36278394e-01 9.33511481e-01 1.43812375e-07 1.01226478e+00\n",
      " 3.01477787e-01 2.02221396e-01 6.16491104e-01 8.68954923e-04\n",
      " 4.40637125e-02 7.90133961e-01 1.08282257e-02 1.60156450e-01\n",
      " 3.04744730e-01 3.22507434e+00 1.89985441e-02 2.38887627e-01\n",
      " 1.30208317e-04 2.40854548e-02 3.62189602e-01 4.12863951e-02\n",
      " 9.87802657e-07 1.43055453e-01 9.38573638e-01 6.98840436e-01\n",
      " 2.35032831e-01 1.11112374e-02 1.64602695e-03 1.86789924e-01\n",
      " 3.61800810e-01 5.48781787e-07 6.85988195e-10 4.79545238e-01\n",
      " 9.56992104e-05 5.80531385e-01 1.12432111e-04 1.78374165e-01\n",
      " 4.21263800e-02 1.90788437e+00 5.58850161e-01 9.04411475e-04\n",
      " 2.34263504e+00 6.20208896e-03 7.86586458e-01 1.47256825e-01\n",
      " 6.04870936e-01 9.01898760e-05 1.49699616e+00 9.19078343e-03\n",
      " 6.02286521e-05 3.85742760e-04 9.30927753e-02 7.75779598e-09\n",
      " 1.35144867e-04 1.24556964e-03 8.78959632e-09 1.58352428e-10\n",
      " 4.27699176e-04 4.49826990e-01 2.88020136e-01 1.96567161e-02\n",
      " 6.75485165e-01 1.60743523e+00 8.59243540e-04 2.16881905e-01\n",
      " 4.10507495e-02 4.54194170e-02 8.17517515e-04 9.04002388e-08\n",
      " 1.48892251e+00 5.79521669e-02 2.54601654e+00 1.20295997e+00\n",
      " 1.23663572e-02 1.67392082e-02 6.09385097e-10 1.57327657e-02\n",
      " 3.50244399e-01 5.05620415e-04 2.62706785e-02 6.27674729e-01\n",
      " 2.04078297e-05 6.02746225e-10 3.04688503e-02 5.84044631e-06\n",
      " 2.35148917e-03 1.88917725e-09 7.85670878e-03 2.88119006e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[4.01209165e-01 5.47023709e-09 8.03175460e-01 4.91609641e-01\n",
      " 6.70609560e-04 1.91218176e-05 2.80664854e-02 2.47697780e-01\n",
      " 3.19860363e-04 1.73630200e-01 1.12658324e+00 9.24769681e-08\n",
      " 3.34072806e-01 9.33640700e-01 1.42403707e-07 1.01153635e+00\n",
      " 3.01137827e-01 2.03429584e-01 1.01712498e+00 8.62489011e-04\n",
      " 4.48959368e-02 7.93831046e-01 1.07921266e-02 1.51974702e-01\n",
      " 3.03430671e-01 9.89959849e-01 2.34584873e-02 2.39653911e-01\n",
      " 1.27693414e-04 3.18281787e-02 9.34406813e-01 4.31971009e-02\n",
      " 1.00303279e-06 1.41580228e-01 9.36704527e-01 7.63255377e-01\n",
      " 1.17688655e+00 1.08182924e-02 1.18678413e-03 1.86370384e-01\n",
      " 3.63617956e-01 5.34463692e-07 6.76550592e-10 4.81992992e-01\n",
      " 3.75418189e-04 5.80592606e-01 1.14643600e-04 1.79518181e-01\n",
      " 4.20429924e-02 1.90762444e+00 5.60531866e-01 9.04968289e-04\n",
      " 2.33998948e+00 6.08813412e-03 7.88002646e-01 1.42738998e-01\n",
      " 6.03158197e-01 7.15357221e-05 1.49193309e+00 9.17449525e-03\n",
      " 6.07740423e-05 6.68396171e-04 9.17448593e-02 7.81230855e-09\n",
      " 1.16588492e-04 1.26922021e-03 8.56841740e-09 1.55869623e-10\n",
      " 4.36409247e-04 4.50088159e-01 2.87296347e-01 1.96677051e-02\n",
      " 2.70705911e+00 1.61182246e+00 8.37722438e-04 2.16958436e-01\n",
      " 4.17000590e-02 4.62062718e-02 8.41302916e-04 8.83013870e-08\n",
      " 1.48708038e+00 5.65816247e-02 1.03332671e+00 1.20629618e+00\n",
      " 1.21049432e-02 1.70526555e-02 6.18396250e-10 1.60684602e-02\n",
      " 3.50216065e-01 2.39514722e-04 2.61768253e-02 6.26274595e-01\n",
      " 2.06212384e-05 6.13810055e-10 2.80632585e-02 5.87615279e-06\n",
      " 2.20037723e-03 1.87719476e-09 7.88096495e-03 2.89135512e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[4.00827915e-01 5.39206583e-09 8.01780287e-01 4.91965329e-01\n",
      " 6.88307980e-04 1.94416726e-05 2.83796688e-02 2.47564122e-01\n",
      " 3.12639244e-04 1.73586793e-01 1.12735051e+00 9.17782045e-08\n",
      " 3.32211330e-01 9.37781904e-01 1.43780869e-07 1.01189396e+00\n",
      " 3.03603518e-01 2.03451696e-01 6.15064013e-01 8.58634376e-04\n",
      " 4.51869677e-02 7.94342770e-01 1.05860868e-02 1.51933020e-01\n",
      " 3.01797432e-01 9.89676957e-01 1.88047227e-02 2.39867572e-01\n",
      " 1.27129265e-04 1.95541258e-02 3.62111785e-01 4.11163567e-02\n",
      " 9.90244721e-07 1.40808309e-01 9.37001118e-01 6.98755160e-01\n",
      " 2.36224338e-01 1.08111820e-02 1.19080465e-03 1.86404607e-01\n",
      " 3.65054773e-01 5.43688571e-07 6.81667637e-10 4.82404824e-01\n",
      " 9.51439692e-05 5.79661863e-01 1.15277385e-04 1.79420943e-01\n",
      " 4.25084142e-02 1.90737444e+00 5.60852727e-01 8.98989633e-04\n",
      " 2.33957821e+00 6.09148645e-03 7.86466846e-01 1.42568109e-01\n",
      " 6.02498255e-01 7.17196990e-05 1.48938202e+00 8.96159559e-03\n",
      " 6.07256758e-05 3.73096227e-04 9.12304409e-02 7.74173278e-09\n",
      " 1.14724293e-04 1.29898799e-03 8.71611172e-09 1.57572969e-10\n",
      " 4.35117574e-04 4.50526838e-01 2.87310361e-01 1.96440349e-02\n",
      " 6.75357464e-01 1.61108491e+00 8.38629858e-04 2.15393335e-01\n",
      " 4.19538025e-02 4.67822640e-02 8.67805806e-04 8.95765229e-08\n",
      " 1.48765781e+00 9.37260416e-02 1.03943328e+00 1.20560329e+00\n",
      " 1.19141646e-02 1.75494756e-02 6.11140845e-10 1.63919737e-02\n",
      " 3.52015623e-01 2.38796920e-04 2.61284819e-02 6.25799948e-01\n",
      " 2.37496340e-05 6.01340183e-10 2.80301610e-02 5.92869327e-06\n",
      " 2.15183863e-03 1.88266888e-09 7.71348282e-03 2.87702786e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[3.99000047e-01 5.33880092e-09 8.00624671e-01 4.91695173e-01\n",
      " 6.93971160e-04 1.97144683e-05 1.14552732e-01 2.46656770e-01\n",
      " 3.24829167e-04 1.73892775e-01 1.12463224e+00 9.11066054e-08\n",
      " 3.30566706e-01 9.29263853e-01 1.44884090e-07 1.01204820e+00\n",
      " 3.29158782e-01 2.04369051e-01 6.16799845e-01 8.49859701e-04\n",
      " 4.19846060e-02 7.98415806e-01 1.09754476e-02 1.50308639e-01\n",
      " 3.01003620e-01 9.91113496e-01 1.96084272e-02 2.39335221e-01\n",
      " 1.25357269e-04 1.94191945e-02 3.63388633e-01 4.10617944e-02\n",
      " 9.81493692e-07 1.45656808e-01 9.35920681e-01 6.99423948e-01\n",
      " 2.33184874e-01 1.08182228e-02 1.19153915e-03 1.86013830e-01\n",
      " 3.60920409e-01 5.50228550e-07 6.85489649e-10 4.78581090e-01\n",
      " 9.61891123e-05 5.78859179e-01 1.05553052e-04 1.80686787e-01\n",
      " 4.14394484e-02 1.91158675e+00 5.59766981e-01 8.93593867e-04\n",
      " 2.35156995e+00 6.01611551e-03 7.85261595e-01 1.43352689e-01\n",
      " 4.80317732e+00 7.04952771e-05 1.50933590e+00 9.36222346e-03\n",
      " 5.75406756e-05 3.65271458e-04 9.54108468e-02 7.68071115e-09\n",
      " 1.19626202e-04 1.31602655e-03 8.82363830e-09 1.58805635e-10\n",
      " 4.37710987e-04 4.49831391e-01 2.87114497e-01 1.95576328e-02\n",
      " 6.74167204e-01 1.60730357e+00 8.02780202e-04 2.67180410e-01\n",
      " 3.93297653e-02 4.34938482e-02 8.83230088e-04 9.05613974e-08\n",
      " 1.48684920e+00 5.43658187e-02 1.02660200e+00 1.20098394e+00\n",
      " 1.57611770e-02 1.78607228e-02 6.05367880e-10 3.11621407e-02\n",
      " 1.85658091e+00 2.36044610e-04 2.60222128e-02 6.30971220e-01\n",
      " 2.10919790e-05 5.90648638e-10 2.79108004e-02 5.92793543e-06\n",
      " 2.23278115e-03 1.88650193e-09 8.07300392e-03 2.86457618e-01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[3.98709809e-01 5.31440659e-09 8.00429458e-01 1.12794677e+00\n",
      " 6.70372223e-04 1.97041527e-05 2.86028001e-02 2.46551895e-01\n",
      " 3.23564453e-04 1.72915177e-01 1.12475461e+00 9.07333030e-08\n",
      " 3.33046270e-01 9.28031955e-01 1.44848811e-07 2.31695617e+00\n",
      " 2.98414230e-01 2.01908101e-01 6.17381043e-01 9.01454375e-04\n",
      " 4.17320812e-02 7.89743396e-01 1.09862807e-02 1.49901391e-01\n",
      " 3.02465532e-01 9.91315880e-01 1.87546332e-02 2.39907978e-01\n",
      " 1.26787634e-04 1.94042899e-02 3.66879681e-01 4.19643179e-02\n",
      " 9.80878042e-07 1.45854241e-01 9.40793760e-01 6.99117570e-01\n",
      " 2.33376492e-01 1.07872637e-02 1.24251586e-03 1.87019278e-01\n",
      " 3.60085485e-01 5.48627522e-07 8.79535068e-10 4.78137784e-01\n",
      " 9.61665641e-05 5.78556477e-01 1.04608725e-04 1.77901224e-01\n",
      " 4.11938928e-02 1.91231396e+00 5.58810387e-01 8.91065895e-04\n",
      " 1.30540786e+01 6.19547390e-03 7.85142210e-01 1.82196601e-01\n",
      " 6.13693478e-01 7.14160860e-05 1.51057326e+00 9.36418169e-03\n",
      " 5.78839535e-05 3.69234919e-04 9.56314562e-02 7.67073319e-09\n",
      " 1.78275152e-04 1.28809102e-03 8.79543941e-09 2.22161345e-10\n",
      " 4.18039359e-04 4.49272418e-01 2.88297324e-01 1.94112378e-02\n",
      " 6.74032455e-01 1.60610921e+00 7.98246850e-04 2.17630363e-01\n",
      " 3.91290394e-02 4.35295863e-02 8.55220475e-04 1.96936796e-07\n",
      " 1.49178337e+00 5.56801377e-02 1.02727035e+00 1.19870615e+00\n",
      " 1.28842783e-02 1.72927905e-02 6.04693794e-10 1.61057775e-02\n",
      " 3.48426443e-01 2.44550537e-04 2.59684055e-02 6.31352318e-01\n",
      " 2.32420913e-05 5.87814267e-10 2.79960296e-02 5.81666512e-06\n",
      " 2.23398035e-03 1.88376345e-09 8.09757282e-03 2.86162367e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[3.98059646e-01 5.41540669e-09 8.02227390e-01 4.94870831e-01\n",
      " 6.85733350e-04 1.90203136e-05 2.79319332e-02 2.46124631e-01\n",
      " 3.28793703e-04 1.71951332e-01 1.12338945e+00 9.13713680e-08\n",
      " 3.30877648e-01 9.24483970e-01 1.42755240e-07 1.01459577e+00\n",
      " 2.96102728e-01 2.00340255e-01 6.17649123e-01 8.61775170e-04\n",
      " 4.05911275e-02 7.85641838e-01 1.11663858e-02 1.49343403e-01\n",
      " 3.00892191e-01 9.96315687e-01 1.87334797e-02 2.38909387e-01\n",
      " 1.28749990e-04 1.93722409e-02 3.64014799e-01 4.13196135e-02\n",
      " 1.00498802e-06 1.47612228e-01 9.43414996e-01 6.98271614e-01\n",
      " 2.32142177e-01 1.08043660e-02 1.18458957e-03 1.87419816e-01\n",
      " 3.62899685e-01 5.34047060e-07 6.69827039e-10 4.77363045e-01\n",
      " 9.34591206e-05 5.78453490e-01 1.01098734e-04 1.76063188e-01\n",
      " 4.28741698e-02 1.91377721e+00 5.58586704e-01 8.89308018e-04\n",
      " 2.35749227e+00 6.27965387e-03 7.86871664e-01 1.44455354e-01\n",
      " 6.11228897e-01 7.18830233e-05 1.51832044e+00 9.55640972e-03\n",
      " 5.61386398e-05 3.63640386e-04 9.71066012e-02 7.72610299e-09\n",
      " 1.18046042e-04 1.30694673e-03 8.55431332e-09 1.59153662e-10\n",
      " 4.07896471e-04 4.49102231e-01 2.88632988e-01 3.58473497e-02\n",
      " 6.75640117e-01 1.61726518e+00 8.19522295e-04 2.17905850e-01\n",
      " 3.81162398e-02 4.22104492e-02 8.79116191e-04 8.76627701e-08\n",
      " 1.49398722e+00 5.43751443e-02 1.02236778e+00 1.19733378e+00\n",
      " 1.32621995e-02 1.76545361e-02 6.19258518e-10 1.66878323e-02\n",
      " 3.47109045e-01 2.44512129e-04 2.59189053e-02 6.33308844e-01\n",
      " 2.08623511e-05 6.05487796e-10 2.78592369e-02 1.00662878e-05\n",
      " 2.26692854e-03 1.86859113e-09 8.18584194e-03 2.87813570e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n",
      "[3.98216704e-01 5.39356119e-09 8.01752151e-01 4.95975839e-01\n",
      " 6.53822323e-04 1.91097810e-05 2.79945789e-02 2.46596868e-01\n",
      " 3.30631045e-04 1.71414190e-01 1.12427043e+00 9.09431314e-08\n",
      " 3.34346730e-01 9.22428891e-01 1.43021272e-07 1.01542302e+00\n",
      " 2.95030201e-01 1.99162862e-01 6.18828897e-01 1.46242281e-03\n",
      " 4.11256425e-02 1.05432719e+00 1.12577028e-02 2.48257961e-01\n",
      " 3.02910558e-01 9.92717804e-01 1.87057459e-02 5.72455227e-01\n",
      " 1.30184646e-04 1.92885702e-02 3.63655317e-01 4.15081755e-02\n",
      " 1.00147973e-06 1.46636074e-01 9.45208008e-01 6.97827470e-01\n",
      " 2.31459652e-01 1.07231392e-02 1.17269674e-03 1.90843517e-01\n",
      " 3.61187274e-01 5.34784363e-07 6.69744092e-10 4.79424426e-01\n",
      " 9.37205121e-05 5.78014459e-01 1.02411580e-04 1.74530345e-01\n",
      " 4.13553227e-02 1.91335931e+00 5.60078097e-01 8.86122632e-04\n",
      " 2.35574791e+00 6.36658150e-03 7.86473015e-01 1.81662961e-01\n",
      " 6.10121595e-01 7.26529363e-05 1.51473362e+00 1.03421773e-02\n",
      " 5.62933601e-05 3.71081530e-04 9.62432952e-02 7.69442938e-09\n",
      " 1.18453956e-04 1.25816955e-03 1.62223737e-08 1.56003960e-10\n",
      " 4.05755619e-04 4.48929556e-01 2.89173488e-01 1.98014479e-02\n",
      " 6.75219467e-01 1.60859337e+00 7.88182650e-04 2.17040903e-01\n",
      " 3.85673974e-02 4.28609485e-02 8.36830201e-04 8.77799370e-08\n",
      " 1.52038370e+00 5.62710102e-02 1.01957261e+00 1.20076860e+00\n",
      " 1.30312086e-02 1.69250972e-02 6.16910674e-10 1.58677825e-02\n",
      " 3.46353805e-01 2.69364594e-04 2.94685256e-02 6.32444152e-01\n",
      " 2.02124980e-05 6.00581970e-10 2.83539877e-02 5.73648293e-06\n",
      " 2.27948502e-03 1.86745031e-09 8.26155725e-03 2.87237457e-01]\n",
      "[2 1 1 0 3 1 1 4 5 0 2 1 3 5 1 0 5 0 4 0 2 0 5 4 3 5 4 4 0 4 4 4 1 2 0 0 5\n",
      " 4 3 0 4 1 1 4 1 1 2 0 4 2 4 1 2 0 1 4 2 0 2 5 2 3 2 1 5 3 1 1 0 5 0 5 1 4\n",
      " 2 4 2 2 3 1 0 3 5 4 2 3 1 3 5 0 4 2 3 1 4 3 5 1 5 1]\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "from time import time\n",
    "import random\n",
    "\n",
    "seed = settings['seed']\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "p2p = IFCA(total_clients, train_data, train_partition2, val_partition, test_data, test_partition,settings['n_clients_UCB'], settings['alpha'],test)\n",
    "k = 6\n",
    "p2p.init_ifca(k)\n",
    "\n",
    "for i in range(100):\n",
    "    p2p.iteration = i\n",
    "    if i != 0: \n",
    "        p2p.cluster_ifca([x for x in range(total_clients)],k)\n",
    "    p2p.run_ifca([x for x in range(total_clients)])\n",
    "    p2p.combine_ifca([x for x in range(total_clients)],k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "198b5609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.51183793019283"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p2p.selected_clients = [x for x in range(total_clients)]\n",
    "p2p.calc_accuracy(p2p.dataloaders_really_test, p2p.len_really_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2357a66f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a1793b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31901ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class P2P_AFPL():\n",
    "    def __init__(self ,total_clients ,train_data ,train_partition ,val_partition ,test_data ,test_partition,n_clients_selected\n",
    "                 ,alpha = 0.25, test='AFPL'):\n",
    "        self.network = Net('MNIST_niid')\n",
    "        self.total_clients = total_clients\n",
    "        self.client_models = {}\n",
    "        self.optimizers = {}\n",
    "        self.dataloaders = {}\n",
    "        self.len = {}\n",
    "        self.len_test = {}\n",
    "        self.len_really_test = {}\n",
    "        self.dataloaders_test = {}\n",
    "        self.dataloaders_really_test = {}\n",
    "        self.best_test_loss = {}\n",
    "        self.best_test_loss_global = 1000000\n",
    "        self.current_test_loss = {}\n",
    "        self.current_train_loss = {}\n",
    "        self.test = test\n",
    "        if self.test == 'AFPL':\n",
    "            self.client_models_global = {}\n",
    "            self.alpha = alpha\n",
    "\n",
    "        if self.test == 'bandits':\n",
    "            self.comb_UCB = combinatorial_UCB(self.total_clients,n_clients_selected)\n",
    "\n",
    "        for i in range(total_clients):\n",
    "            self.client_models[str(i)] = copy.deepcopy(self.network).double().cuda()\n",
    "            self.optimizers[str(i)] = torch.optim.SGD(self.client_models[str(i)].parameters() ,lr=0.008 ,momentum=0.5)\n",
    "            if data_fraction != 1:\n",
    "                dataset_train= MNIST_NIID_dataset(train_data[0][blub] ,train_data[1][blub] ,train_partition ,i)\n",
    "            else:\n",
    "                dataset_train = MNIST_NIID_dataset(train_data[0], train_data[1], train_partition, i)\n",
    "\n",
    "            if i == 1:\n",
    "                if data_fraction != 1:\n",
    "                    dataset = torch.utils.data.ConcatDataset([dataset_train,MNIST_NIID_dataset(train_data[0][blub] ,train_data[1][blub] ,train_partition,i )])\n",
    "                else:\n",
    "                    dataset = torch.utils.data.ConcatDataset(\n",
    "                        [dataset_train, MNIST_NIID_dataset(train_data[0], train_data[1], train_partition, i)])\n",
    "            if i > 1 :\n",
    "                if data_fraction != 1:\n",
    "                    dataset = torch.utils.data.ConcatDataset(\n",
    "                    [dataset,MNIST_NIID_dataset(train_data[0][blub] ,train_data[1][blub] ,train_partition,i )])\n",
    "                else:\n",
    "                    dataset = torch.utils.data.ConcatDataset(\n",
    "                        [dataset, MNIST_NIID_dataset(train_data[0], train_data[1], train_partition, i)])\n",
    "\n",
    "\n",
    "            self.len[str(i) ]= len(dataset_train)\n",
    "            self.dataloaders[str(i)] = DataLoader(dataset_train ,batch_size=16 ,shuffle=True)\n",
    "            if data_fraction !=1:\n",
    "                dataset_test= MNIST_NIID_dataset(train_data[0][blub] ,train_data[1][blub] ,val_partition,i  )\n",
    "            else:\n",
    "                dataset_test = MNIST_NIID_dataset(train_data[0], train_data[1], val_partition, i)\n",
    "\n",
    "            dataset_really_test = MNIST_NIID_dataset(test_data[0],test_data[1],test_partition,i)\n",
    "            self.len_really_test[str(i)] = len(dataset_really_test)\n",
    "            self.dataloaders_really_test[str(i)] = DataLoader(dataset_really_test,batch_size=16,shuffle=True)\n",
    "            self.len_test[str(i)] = len(dataset_test)\n",
    "            self.dataloaders_test[str(i)] = DataLoader(dataset_test ,batch_size=16 ,shuffle=False)\n",
    "            self.best_test_loss[str(i)] = 10000000\n",
    "            self.current_test_loss[str(i)] = 100000\n",
    "            self.current_train_loss[str(i)] = 1000000\n",
    "            if self.test == 'AFPL':\n",
    "                self.client_models_global[str(i)] = copy.deepcopy(self.network).double().cuda()\n",
    "                self.shared_model = copy.deepcopy(self.network).double().cuda()\n",
    "        self.dataset_train = dataset_train\n",
    "        self.dataloader_centralized = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    def update_local_models(self ,selected_clients):\n",
    "        self.dw = {}\n",
    "        loss_test = 0\n",
    "        loss_test2 = 0\n",
    "        losses = 0\n",
    "        losses2 = 0\n",
    "        loss_test3 = 0\n",
    "        losses3 = 0\n",
    "\n",
    "        for idx ,i in enumerate(selected_clients):\n",
    "\n",
    "            dataloader = self.dataloaders[str(i)]\n",
    "            optimizer= torch.optim.Adam(self.client_models[str(i)].parameters() ,lr=0.001 *0.95**self.iteration)\n",
    "            self.client_models[str(i)].train()\n",
    "\n",
    "            if self.test == 'AFPL':\n",
    "                self.client_models_global[str(i)] = copy.deepcopy(self.shared_model)\n",
    "                self.client_models_global[str(i)].train()\n",
    "                optimizer_global = torch.optim.Adam(self.client_models_global[str(i)].parameters()\n",
    "                                                    ,lr=0.001 *0.95**self.iteration)\n",
    "\n",
    "            for batch_idx, (data, target) in enumerate(dataloader):\n",
    "                data = data.double().cuda()\n",
    "                target =target.long().cuda()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                output = self.client_models[str(i)](data)\n",
    "                loss = F.nll_loss(output ,target)\n",
    "\n",
    "                if self.test == 'AFPL':\n",
    "                    optimizer_global.zero_grad()\n",
    "                    output_global= self.client_models_global[str(i)](data)\n",
    "                    loss_global = F.nll_loss(output_global ,target)\n",
    "                    loss_global.backward()\n",
    "                    optimizer_global.step()\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            self.client_models[str(i)].eval()\n",
    "            dataloader_test = self.dataloaders_test[str(i)]\n",
    "            loss_test = 0\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (data, target) in enumerate(dataloader_test):\n",
    "                    data = data.double().cuda()\n",
    "                    target =target.long().cuda()\n",
    "\n",
    "                    output = self.client_models[str(i)](data)\n",
    "                    loss_test += F.nll_loss(output ,target)\n",
    "                self.current_test_loss[str(i)] = loss_test /self.len_test[str(i)]\n",
    "                if self.current_test_loss[str(i)] < self.best_test_loss[str(i)]:\n",
    "                    torch.save(self.client_models[str(i)].state_dict(), os.path.join(save_dir, 'model', 'best_model ' +str(i ) +'.pt'))\n",
    "                    self.best_test_loss[str(i)] = self.current_test_loss[str(i)]\n",
    "\n",
    "            losses += loss_test /self.len_test[str(i)]\n",
    "            loss_test2 = 0\n",
    "            self.client_models[str(i)].eval()\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (data, target) in enumerate(dataloader):\n",
    "                    data = data.double().cuda()\n",
    "                    target = target.long().cuda()\n",
    "\n",
    "                    output = self.client_models[str(i)](data)\n",
    "                    loss_test2 += F.nll_loss(output, target)\n",
    "\n",
    "            losses2 += loss_test2 / self.len[str(i)]\n",
    "            self.current_train_loss[str(i)] = loss_test2 / self.len[str(i)]\n",
    "\n",
    "        print('full train loss: ', losses2)\n",
    "        print('full loss: ', losses)\n",
    "\n",
    "        return losses2, losses\n",
    "\n",
    "    def centralized(self ,selected_clients):\n",
    "        self.dw = {}\n",
    "        loss_test = 0\n",
    "        loss_test2 = 0\n",
    "        losses = 0\n",
    "        losses2 = 0\n",
    "        loss_test3 = 0\n",
    "        losses3 = 0\n",
    "\n",
    "        dataloader = self.dataloader_centralized\n",
    "        optimizer= torch.optim.Adam(self.client_models[str(0)].parameters() ,lr=0.001 *0.95**self.iteration)\n",
    "        self.client_models[str(0)].train()\n",
    "\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(dataloader):\n",
    "            data = data.double().cuda()\n",
    "            target =target.long().cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = self.client_models[str(0)](data)\n",
    "            loss = F.nll_loss(output ,target)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        self.client_models[str(0)].eval()\n",
    "        for idx, i in enumerate(selected_clients):\n",
    "            dataloader_test = self.dataloaders_test[str(i)]\n",
    "            loss_test = 0\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (data, target) in enumerate(dataloader_test):\n",
    "                    data = data.double().cuda()\n",
    "                    target =target.long().cuda()\n",
    "\n",
    "                    output = self.client_models[str(0)](data)\n",
    "                    loss_test += F.nll_loss(output ,target)\n",
    "\n",
    "                losses += loss_test / self.len_test[str(i)]\n",
    "        self.current_test_loss[str(0)] = loss_test /self.len_test[str(0)]\n",
    "        if self.current_test_loss[str(0)] < self.best_test_loss[str(0)]:\n",
    "            torch.save(self.client_models[str(0)].state_dict(), os.path.join(save_dir, 'model', 'best_model ' +str(0 ) +'.pt'))\n",
    "            self.best_test_loss[str(0)] = self.current_test_loss[str(0)]\n",
    "\n",
    "\n",
    "        loss_test2 = 0\n",
    "        self.client_models[str(0)].eval()\n",
    "        for idx, i in enumerate(selected_clients):\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (data, target) in enumerate(self.dataloaders[str(i)]):\n",
    "                    data = data.double().cuda()\n",
    "                    target = target.long().cuda()\n",
    "\n",
    "                    output = self.client_models[str(0)](data)\n",
    "                    loss_test2 += F.nll_loss(output, target)\n",
    "\n",
    "                losses2 += loss_test2 / self.len[str(i)]\n",
    "        self.current_train_loss[str(i)] = loss_test2 / self.len[str(i)]\n",
    "\n",
    "        print('full train loss: ', losses2)\n",
    "        print('full loss: ', losses)\n",
    "        for idx, i in enumerate(selected_clients):\n",
    "            if i != 0:\n",
    "                for (name, param), (name2, param2) in zip(self.client_models[str(i)].named_parameters(),\n",
    "                                                          self.client_models[str(0)].named_parameters()):\n",
    "                    param.data = param2.data\n",
    "                self.client_models[str(i)].double()\n",
    "\n",
    "        return losses2, losses\n",
    "\n",
    "    def combine_models(self, i, client_numbers, set_as=True):\n",
    "        zero_copy = copy.deepcopy(self.client_models[str(i)])  # This is used to collect the model in\n",
    "        j = 0\n",
    "        client_numbers_plus_client = np.concatenate((client_numbers, np.array([int(i)])))  # This is more efficient\n",
    "        #  alphas = zero_copy.alphas.detach()\n",
    "        # alphas[i] = 1 - torch.sum(\n",
    "        #     torch.tensor([iii for idx, iii in enumerate(alphas) if idx != i and idx in client_numbers]))\n",
    "        # It's not possible to set the value of self.alphas[i], so instead we determine it manually here\n",
    "        alphas = torch.ones(len(client_numbers_plus_client)).cuda() / (len(client_numbers_plus_client))\n",
    "        # print(alphas)\n",
    "        for ii in client_numbers_plus_client:\n",
    "            #  print(ii)\n",
    "            for (name, param), (name2, param2) in zip(zero_copy.named_parameters(), self.client_models[\n",
    "                str(ii)].named_parameters()):  # self.client_models[str(ii)].named_parameters()):\n",
    "\n",
    "                if name != 'alphas':\n",
    "                    if j == 0:\n",
    "                        param.data = torch.zeros(param.shape).cuda()\n",
    "\n",
    "                    param.data += alphas[j] * param2.data  # we add all participating client's models to the one here.\n",
    "\n",
    "            j += 1\n",
    "\n",
    "        # self.client_models[str(i)] = zero_copy.double()\n",
    "        if set_as == True:\n",
    "            for (name, param), (name2, param2) in zip(self.client_models[str(i)].named_parameters(),\n",
    "                                                      zero_copy.named_parameters()):\n",
    "                param.data = param2.data\n",
    "            self.client_models[str(i)].double()\n",
    "        else:\n",
    "            return zero_copy.double()\n",
    "\n",
    "    def federated_averaging(self):\n",
    "        self.shared_model = copy.deepcopy(self.network).double().cuda()\n",
    "        n_clients = len(self.selected_clients)\n",
    "        weight = [self.len[str(x)] for x in self.selected_clients]\n",
    "        weight = weight / np.sum(weight)\n",
    "\n",
    "        losses = 0\n",
    "        losses2 = 0\n",
    "        # print(\"weights \",weight)\n",
    "        for idx, i in enumerate(self.selected_clients):\n",
    "            for (name, param), (name2, param2) in zip(self.shared_model.named_parameters()\n",
    "                    , self.client_models[str(i)].named_parameters()):\n",
    "                if idx == 0:\n",
    "                    param.data = torch.zeros(param.shape).cuda().double()\n",
    "                param.data += weight[idx] * param2.data\n",
    "\n",
    "        self.shared_model = self.shared_model.double().eval()\n",
    "\n",
    "        for i in self.selected_clients:\n",
    "            self.client_models[str(i)] = copy.deepcopy(self.shared_model)  # copy global model to the clients\n",
    "            loss_test = 0\n",
    "            for batch_idx, (data, target) in enumerate(self.dataloaders_test[str(i)]):\n",
    "                data = data.double().cuda()\n",
    "                target = target.long().cuda()\n",
    "\n",
    "                loss_test += F.nll_loss(self.shared_model(data), target).detach().cpu().numpy()\n",
    "\n",
    "            loss_test = loss_test / self.len_test[str(i)]\n",
    "            losses += loss_test\n",
    "            if loss_test < self.best_test_loss[str(i)]:\n",
    "                torch.save(self.client_models[str(i)].state_dict(),\n",
    "                           os.path.join(save_dir, 'model', 'best_model' + str(i) + '.pt'))\n",
    "                self.best_test_loss[str(i)] = loss_test\n",
    "            self.client_models[str(i)].eval()\n",
    "            loss_test2 = 0\n",
    "            for batch_idx, (data, target) in enumerate(self.dataloaders[str(i)]):\n",
    "                data = data.double().cuda()\n",
    "                target = target.long().cuda()\n",
    "\n",
    "                loss_test2 += F.nll_loss(self.shared_model(data), target).detach().cpu().numpy()\n",
    "\n",
    "            loss_test2 = loss_test2 / self.len[str(i)]\n",
    "            losses2 += loss_test2\n",
    "\n",
    "        return losses, losses2\n",
    "\n",
    "    def federated_averaging2(self):\n",
    "        # Accumulate global model\n",
    "        self.shared_model = copy.deepcopy(self.network).double().cuda()\n",
    "        n_clients = len(self.selected_clients)\n",
    "        weight = [self.len[str(x)] for x in self.selected_clients]\n",
    "        weight = weight / np.sum(weight)\n",
    "\n",
    "        losses = 0\n",
    "        losses2 = 0\n",
    "        # print(\"weights \",weight)\n",
    "        for idx, i in enumerate(self.selected_clients):\n",
    "            for (name, param), (name2, param2) in zip(self.shared_model.named_parameters()\n",
    "                    , self.client_models[str(i)].named_parameters()):\n",
    "                if idx == 0:\n",
    "                    param.data = torch.zeros(param.shape).cuda().double()\n",
    "                param.data += weight[idx] * param2.data\n",
    "\n",
    "        self.shared_model = self.shared_model.double().eval()\n",
    "\n",
    "        # loop over clients\n",
    "        for i in self.selected_clients:\n",
    "            # check if model improves performance\n",
    "            #self.client_models[str(i)] = copy.deepcopy(self.shared_model)  # copy global model to the clients\n",
    "            self.client_models[str(i)].eval().cuda()\n",
    "            loss_test = 0\n",
    "            loss_test2 = 0\n",
    "            for batch_idx, (data, target) in enumerate(self.dataloaders_test[str(i)]):\n",
    "                data = data.double().cuda()\n",
    "                target = target.long().cuda()\n",
    "\n",
    "                loss_test += F.nll_loss(self.shared_model(data), target).detach().cpu().numpy()\n",
    "                loss_test2 += F.nll_loss(self.client_models[str(i)](data), target).detach().cpu().numpy()\n",
    "\n",
    "            ey = loss_test / self.len_test[str(i)]\n",
    "            current_test = loss_test2 / self.len_test[str(i)]\n",
    "            if ey < current_test:\n",
    "               # print('replaced local model with global')\n",
    "                self.client_models[str(i)] = copy.deepcopy(self.shared_model)\n",
    "                losses += ey\n",
    "                if ey < self.best_test_loss[str(i)]:\n",
    "                    torch.save(self.client_models[str(i)].state_dict(),\n",
    "                               os.path.join(save_dir, 'model', 'best_model' + str(i) + '.pt'))\n",
    "                    self.best_test_loss[str(i)] = ey\n",
    "            else:\n",
    "              #  print('nothing')\n",
    "                losses += current_test\n",
    "                if current_test < self.best_test_loss[str(i)]:\n",
    "                    torch.save(self.client_models[str(i)].state_dict(),\n",
    "                               os.path.join(save_dir, 'model', 'best_model' + str(i) + '.pt'))\n",
    "                    self.best_test_loss[str(i)] = current_test\n",
    "\n",
    "\n",
    "            self.client_models[str(i)].eval()\n",
    "            loss_test2 = 0\n",
    "            for batch_idx, (data, target) in enumerate(self.dataloaders[str(i)]):\n",
    "                data = data.double().cuda()\n",
    "                target = target.long().cuda()\n",
    "\n",
    "                loss_test2 += F.nll_loss(self.client_models[str(i)](data), target).detach().cpu().numpy()\n",
    "\n",
    "            loss_test2 = loss_test2 / self.len[str(i)]\n",
    "        losses2 += loss_test2\n",
    "        return losses, losses2\n",
    "\n",
    "    def AFPL(self):  # use alpha = 0.25 = 0.75 global model + 0.25 local model\n",
    "        self.shared_model_old = copy.deepcopy(self.shared_model)\n",
    "        self.shared_model = copy.deepcopy(self.network).double().cuda()\n",
    "        n_clients = len(self.selected_clients)\n",
    "        weight = [self.len[str(x)] for x in self.selected_clients]\n",
    "        weight = weight / np.sum(weight)\n",
    "\n",
    "        losses = 0\n",
    "        losses2 = 0\n",
    "\n",
    "        # accumulate local weights\n",
    "        for idx, i in enumerate(self.selected_clients):\n",
    "            for (name, param), (name2, param2), (name3, param3), (name4, param4) in zip(\n",
    "                    self.shared_model.named_parameters()\n",
    "                    , self.client_models_global[str(i)].named_parameters(),\n",
    "                    self.shared_model_old.named_parameters(),\n",
    "                    self.client_models[str(i)].named_parameters()):\n",
    "                if idx == 0:\n",
    "                    param.data = torch.zeros(param.shape).cuda().double()\n",
    "                param.data += weight[idx] * param2.data  # accumulate local weights\n",
    "                param4.data = self.alpha * param4.data + (1-self.alpha) * param3.data  # do AFPL local model update: note that we take the previous global model\n",
    "            self.client_models[str(i)] = self.client_models[str(i)].double()\n",
    "            self.client_models[str(i)].eval()\n",
    "            loss_test = 0\n",
    "            for batch_idx, (data, target) in enumerate(self.dataloaders_test[str(i)]):\n",
    "                data = data.double().cuda()\n",
    "                target = target.long().cuda()\n",
    "\n",
    "                loss_test += F.nll_loss(self.client_models[str(i)](data), target).detach().cpu().numpy()\n",
    "\n",
    "            loss_test = loss_test / self.len_test[str(i)]\n",
    "            losses += loss_test\n",
    "            if loss_test < self.best_test_loss[str(i)]:\n",
    "                torch.save(self.client_models[str(i)].state_dict(),\n",
    "                           os.path.join(save_dir, 'model', 'best_model' + str(i) + '.pt'))\n",
    "                self.best_test_loss[str(i)] = loss_test\n",
    "            self.client_models[str(i)].eval()\n",
    "            loss_test2 = 0\n",
    "            for batch_idx, (data, target) in enumerate(self.dataloaders[str(i)]):\n",
    "                data = data.double().cuda()\n",
    "                target = target.long().cuda()\n",
    "\n",
    "                loss_test2 += F.nll_loss(self.client_models[str(i)](data), target).detach().cpu().numpy()\n",
    "\n",
    "            loss_test2 = loss_test2 / self.len[str(i)]\n",
    "            losses2 += loss_test2\n",
    "\n",
    "        self.shared_model = self.shared_model.double()\n",
    "        return losses, losses2\n",
    "\n",
    "    def optimal_fedavg(self):\n",
    "        losses = 0\n",
    "        losses2 = 0\n",
    "        for i in range(self.total_clients):\n",
    "\n",
    "            self.client_models[str(i)].eval()\n",
    "            dataloader_test = self.dataloaders_test[str(i)]\n",
    "            loss_test = 0\n",
    "            loss_test2 = 0\n",
    "            # print(np.where(adj_matrix[i,:]>0)[0])\n",
    "            label_informed_selected_clients = np.where(adj_matrix[i, :] > 0)[0]\n",
    "\n",
    "            label_informed_shared_model = self.combine_models(i, label_informed_selected_clients, set_as=False)\n",
    "            label_informed_shared_model.eval()\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (data, target) in enumerate(dataloader_test):\n",
    "                    data = data.double().cuda()\n",
    "                    target = target.long().cuda()\n",
    "\n",
    "                    output2 = label_informed_shared_model(data)\n",
    "\n",
    "                    loss_test += F.nll_loss(output2, target)\n",
    "                if loss_test / self.len_test[str(i)] < self.best_test_loss[str(i)]:\n",
    "                    torch.save(self.client_models[str(i)].state_dict(),\n",
    "                               os.path.join(save_dir, 'model', 'best_model' + str(i) + '.pt'))\n",
    "                    self.best_test_loss[str(i)] = loss_test / self.len_test[str(i)]\n",
    "            losses += loss_test / self.len_test[str(i)]\n",
    "\n",
    "            dataloader_test = self.dataloaders[str(i)]\n",
    "            loss_test2 = 0\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (data, target) in enumerate(dataloader_test):\n",
    "                    data = data.double().cuda()\n",
    "                    target = target.long().cuda()\n",
    "\n",
    "                    output = label_informed_shared_model(data)\n",
    "                    loss_test2 += F.nll_loss(output, target)\n",
    "            losses2 += loss_test2 / self.len[str(i)]\n",
    "\n",
    "            self.combine_models(i, label_informed_selected_clients)\n",
    "        return losses, losses2\n",
    "\n",
    "    def my_method(self, client):\n",
    "\n",
    "        selected_clients = []\n",
    "        other_clients = [x for x in range(self.total_clients) if x is not client]\n",
    "        ey = np.zeros(len(other_clients))  # fix indices\n",
    "        current_test = np.zeros(len(other_clients))\n",
    "        collected_clients = []\n",
    "        list1 = np.arange(len(other_clients))\n",
    "        np.random.shuffle(list1)\n",
    "        for i in list1:\n",
    "            # selected_clients_coalition = other_clients[i] +[client]\n",
    "            shared_model = self.combine_models(client, [other_clients[i]], set_as=False)\n",
    "\n",
    "            if len(collected_clients) > 0:\n",
    "                all_clients = collected_clients + [other_clients[i]]\n",
    "                shared_model2 = self.combine_models(client, all_clients, set_as=False)\n",
    "\n",
    "            # print(selected_clients_coalition)\n",
    "            shared_model.eval().cuda()\n",
    "            self.client_models[str(client)].eval().cuda()\n",
    "            loss_test = 0\n",
    "            loss_test2 = 0\n",
    "            loss_test3 = 0\n",
    "            for batch_idx, (data, target) in enumerate(self.dataloaders_test[str(client)]):\n",
    "                data = data.double().cuda()\n",
    "                target = target.long().cuda()\n",
    "\n",
    "                loss_test += F.nll_loss(shared_model(data), target).detach().cpu().numpy()\n",
    "                loss_test2 += F.nll_loss(self.client_models[str(client)](data), target).detach().cpu().numpy()\n",
    "\n",
    "                if len(collected_clients) > 0:\n",
    "                    loss_test3 += F.nll_loss(shared_model2(data), target).detach().cpu().numpy()\n",
    "\n",
    "            ey[i] = loss_test / self.len_test[str(client)]\n",
    "            current_test[i] = loss_test2 / self.len_test[str(client)]\n",
    "            if ey[i] < current_test[i]:\n",
    "                if len(collected_clients) > 0:\n",
    "                    test2 = loss_test3 / self.len_test[str(client)]\n",
    "                    if test2 < current_test[i]:\n",
    "                        collected_clients.append(other_clients[i])\n",
    "                else:\n",
    "                    collected_clients.append(other_clients[i])\n",
    "        loss_test = current_test[i]\n",
    "\n",
    "        selected_clients = np.where(ey <= self.current_test_loss[str(client)].detach().cpu().numpy())[0]\n",
    "        selected_clients = [other_clients[x] for x in selected_clients]\n",
    "        selected_clients = collected_clients\n",
    "\n",
    "        if len(selected_clients) > 0:\n",
    "            # self.client_models[str(client)] = copy.deepcopy(shared_model)\n",
    "            self.combine_models(client, selected_clients, set_as=True)\n",
    "            # self.client_models[str(client)].double().eval().cuda()\n",
    "            loss_test = 0\n",
    "            for batch_idx, (data, target) in enumerate(self.dataloaders_test[str(client)]):\n",
    "                data = data.double().cuda()\n",
    "                target = target.long().cuda()\n",
    "\n",
    "                loss_test += F.nll_loss(self.client_models[str(client)](data), target).detach().cpu().numpy()\n",
    "\n",
    "            loss_test = loss_test / self.len_test[str(client)]\n",
    "            if loss_test < self.best_test_loss[str(client)]:\n",
    "                torch.save(self.client_models[str(client)].state_dict(),\n",
    "                           os.path.join(save_dir, 'model', 'best_model' + str(i) + '.pt'))\n",
    "                self.best_test_loss[str(client)] = loss_test\n",
    "            self.client_models[str(client)].eval()\n",
    "            loss_test2 = 0\n",
    "            for batch_idx, (data, target) in enumerate(self.dataloaders[str(client)]):\n",
    "                data = data.double().cuda()\n",
    "                target = target.long().cuda()\n",
    "\n",
    "                loss_test2 += F.nll_loss(self.client_models[str(client)](data), target).detach().cpu().numpy()\n",
    "\n",
    "            loss_test2 = loss_test2 / self.len[str(client)]\n",
    "\n",
    "        #    print('test loss: ',loss_test)\n",
    "        # return ey, selected_clients\n",
    "        return loss_test, loss_test2, selected_clients\n",
    "\n",
    "    def calc_accuracy(self, dataloader, length):\n",
    "        accuracies = np.zeros(len(self.selected_clients))\n",
    "        total = 0\n",
    "        self.accuracy_list = []\n",
    "        for i in self.selected_clients:\n",
    "            intermediate_accuracy = 0\n",
    "\n",
    "            for batch_idx, (data, target) in enumerate(dataloader[str(i)]):\n",
    "                data = data.double().cuda()\n",
    "                target = target.long().cuda()\n",
    "                output = self.client_models[str(i)](data)\n",
    "                output_array = output.detach().cpu().numpy()\n",
    "                output_class = np.argmax(output_array, axis=-1)\n",
    "                target_array = target.detach().cpu().numpy()\n",
    "                intermediate_accuracy += np.sum(output_class == target_array)\n",
    "            accuracy = intermediate_accuracy / length[str(i)]* 100\n",
    "            total += length[str(i)]\n",
    "\n",
    "            self.accuracy_list.append(accuracy)\n",
    "            accuracies[i] = intermediate_accuracy\n",
    "        overall_accuracy = np.sum(accuracies) / total * 100\n",
    "        return overall_accuracy\n",
    "\n",
    "    def my_method2(self, client, k=10):\n",
    "\n",
    "        selected_clients = []\n",
    "        other_clients = [x for x in range(self.total_clients) if x is not client]\n",
    "        ey = np.zeros(len(other_clients))  # fix indices\n",
    "        current_test = np.zeros(len(other_clients))\n",
    "        collected_clients = []\n",
    "        list1 = np.arange(len(other_clients))\n",
    "        np.random.shuffle(list1)\n",
    "        for i in list1[:k]:\n",
    "            shared_model = self.combine_models(client, [other_clients[i]], set_as=False)\n",
    "\n",
    "            if len(collected_clients) > 0:\n",
    "                all_clients = collected_clients + [other_clients[i]]\n",
    "                shared_model2 = self.combine_models(client, all_clients, set_as=False)\n",
    "\n",
    "            shared_model.eval().cuda()\n",
    "            self.client_models[str(client)].eval().cuda()\n",
    "            loss_test = 0\n",
    "            loss_test2 = 0\n",
    "            loss_test3 = 0\n",
    "            for batch_idx, (data, target) in enumerate(self.dataloaders_test[str(client)]):\n",
    "                data = data.double().cuda()\n",
    "                target = target.long().cuda()\n",
    "\n",
    "                loss_test += F.nll_loss(shared_model(data), target).detach().cpu().numpy()\n",
    "                loss_test2 += F.nll_loss(self.client_models[str(client)](data), target).detach().cpu().numpy()\n",
    "\n",
    "                if len(collected_clients) > 0:\n",
    "                    loss_test3 += F.nll_loss(shared_model2(data), target).detach().cpu().numpy()\n",
    "\n",
    "            ey[i] = loss_test / self.len_test[str(client)]\n",
    "            current_test[i] = loss_test2 / self.len_test[str(client)]\n",
    "            if ey[i] < current_test[i]:\n",
    "                if len(collected_clients) > 0:\n",
    "                    test2 = loss_test3 / self.len_test[str(client)]\n",
    "                    if test2 < current_test[i]:\n",
    "                        collected_clients.append(other_clients[i])\n",
    "                else:\n",
    "                    collected_clients.append(other_clients[i])\n",
    "        loss_test = current_test[i]\n",
    "\n",
    "        selected_clients = np.where(ey <= self.current_test_loss[str(client)].detach().cpu().numpy())[0]\n",
    "        selected_clients = [other_clients[x] for x in selected_clients]\n",
    "        selected_clients = collected_clients\n",
    "\n",
    "        if len(selected_clients) > 0:\n",
    "            self.combine_models(client, selected_clients, set_as=True)\n",
    "            loss_test = 0\n",
    "            for batch_idx, (data, target) in enumerate(self.dataloaders_test[str(client)]):\n",
    "                data = data.double().cuda()\n",
    "                target = target.long().cuda()\n",
    "\n",
    "                loss_test += F.nll_loss(self.client_models[str(client)](data), target).detach().cpu().numpy()\n",
    "\n",
    "            loss_test = loss_test / self.len_test[str(client)]\n",
    "            if loss_test < self.best_test_loss[str(client)]:\n",
    "                torch.save(self.client_models[str(client)].state_dict(),\n",
    "                           os.path.join(save_dir, 'model', 'best_model' + str(i) + '.pt'))\n",
    "                self.best_test_loss[str(client)] = loss_test\n",
    "            self.client_models[str(client)].eval()\n",
    "            loss_test2 = 0\n",
    "            for batch_idx, (data, target) in enumerate(self.dataloaders[str(client)]):\n",
    "                data = data.double().cuda()\n",
    "                target = target.long().cuda()\n",
    "\n",
    "                loss_test2 += F.nll_loss(self.client_models[str(client)](data), target).detach().cpu().numpy()\n",
    "\n",
    "            loss_test2 = loss_test2 / self.len[str(client)]\n",
    "        return loss_test, loss_test2, selected_clients\n",
    "\n",
    "    def bandits(self, client, n):\n",
    "\n",
    "        selected_clients = []\n",
    "        other_clients = [x for x in range(self.total_clients) if x != client]\n",
    "        # print(other_clients)\n",
    "        ey = np.zeros(self.total_clients)  # fix indices\n",
    "        current_test = np.zeros(self.total_clients)\n",
    "        collected_clients = []\n",
    "\n",
    "        selected_clients_UCB = self.comb_UCB.to_client([client], n)\n",
    "        if client == 1:\n",
    "            print('selected clients UCB: ', selected_clients_UCB)\n",
    "        for i in selected_clients_UCB:\n",
    "            shared_model = self.combine_models(client, [i], set_as=False)\n",
    "\n",
    "            if len(collected_clients) > 0:\n",
    "                all_clients = collected_clients + [i]\n",
    "                shared_model2 = self.combine_models(client, all_clients, set_as=False)\n",
    "\n",
    "            shared_model.eval().cuda()\n",
    "            self.client_models[str(client)].eval().cuda()\n",
    "            loss_test = 0\n",
    "            loss_test2 = 0\n",
    "            loss_test3 = 0\n",
    "            for batch_idx, (data, target) in enumerate(self.dataloaders_test[str(client)]):\n",
    "                data = data.double().cuda()\n",
    "                target = target.long().cuda()\n",
    "\n",
    "                loss_test += F.nll_loss(shared_model(data), target).detach().cpu().numpy()\n",
    "                loss_test2 += F.nll_loss(self.client_models[str(client)](data), target).detach().cpu().numpy()\n",
    "\n",
    "                if len(collected_clients) > 0:\n",
    "                    loss_test3 += F.nll_loss(shared_model2(data), target).detach().cpu().numpy()\n",
    "\n",
    "            ey[i] = loss_test / self.len_test[str(client)]\n",
    "            current_test[i] = loss_test2 / self.len_test[str(client)]\n",
    "            if ey[i] < current_test[i]:\n",
    "                if len(collected_clients) > 0:\n",
    "                    test2 = loss_test3 / self.len_test[str(client)]\n",
    "                    if test2 < current_test[i]:\n",
    "                        collected_clients.append(i)\n",
    "                else:\n",
    "                    collected_clients.append(i)\n",
    "        loss_test = current_test[i]\n",
    "        selected_clients = np.where(ey <= self.current_test_loss[str(client)].detach().cpu().numpy())[0]\n",
    "        # selected_clients = [other_clients[x] for x in selected_clients]\n",
    "\n",
    "        selected_clients = collected_clients\n",
    "\n",
    "        observation = np.zeros(self.total_clients)\n",
    "        observation[selected_clients] = 1\n",
    "        if client == 1:\n",
    "            print(observation)\n",
    "\n",
    "        self.comb_UCB.to_server(client, observation)\n",
    "\n",
    "        if len(selected_clients) > 0:\n",
    "            self.combine_models(client, selected_clients, set_as=True)\n",
    "            loss_test = 0\n",
    "            for batch_idx, (data, target) in enumerate(self.dataloaders_test[str(client)]):\n",
    "                data = data.double().cuda()\n",
    "                target = target.long().cuda()\n",
    "\n",
    "                loss_test += F.nll_loss(self.client_models[str(client)](data), target).detach().cpu().numpy()\n",
    "\n",
    "            loss_test = loss_test / self.len_test[str(client)]\n",
    "            if loss_test < self.best_test_loss[str(client)]:\n",
    "                torch.save(self.client_models[str(client)].state_dict(),\n",
    "                           os.path.join(save_dir, 'model', 'best_model' + str(i) + '.pt'))\n",
    "                self.best_test_loss[str(client)] = loss_test\n",
    "            self.client_models[str(client)].eval()\n",
    "            loss_test2 = 0\n",
    "            for batch_idx, (data, target) in enumerate(self.dataloaders[str(client)]):\n",
    "                data = data.double().cuda()\n",
    "                target = target.long().cuda()\n",
    "\n",
    "                loss_test2 += F.nll_loss(self.client_models[str(client)](data), target).detach().cpu().numpy()\n",
    "\n",
    "            loss_test2 = loss_test2 / self.len[str(client)]\n",
    "        return loss_test, loss_test2, selected_clients, selected_clients_UCB\n",
    "\n",
    "    def loop(self, epochs, p2p, experiment_name):\n",
    "\n",
    "        loss_tests = []\n",
    "        loss_trains = []\n",
    "        loss_tests2 = []\n",
    "        loss_trains2 = []\n",
    "        accuracies = []\n",
    "        accuracies_train = []\n",
    "        best_accuracy = 0\n",
    "        self.p2p = p2p\n",
    "        self.phis = np.zeros((self.total_clients, self.total_clients))\n",
    "        self.phisUCB = np.zeros((self.total_clients, self.total_clients))\n",
    "        self.selected_clients_arr = np.zeros((epochs, self.total_clients, self.total_clients))\n",
    "\n",
    "        for i in range(epochs):\n",
    "            print(i)\n",
    "            self.iteration = i\n",
    "            list1 = []\n",
    "            self.selected_clients = [x for x in range(self.total_clients)]\n",
    "\n",
    "            if self.test != 'centralized':\n",
    "                loss_train, loss_test = self.update_local_models(self.selected_clients)\n",
    "            else:\n",
    "                loss_train, loss_test = self.centralized(self.selected_clients)\n",
    "            loss_tests.append(loss_test.detach().cpu().numpy())\n",
    "            loss_trains.append(loss_train.detach().cpu().numpy())\n",
    "\n",
    "            if self.test == 'AFPL':\n",
    "                losses2, losses3 = self.AFPL()\n",
    "\n",
    "            if self.test == 'prtfl':\n",
    "                losses2, losses3 = self.federated_averaging2()\n",
    "\n",
    "            if self.test == 'local':\n",
    "                print('we are done')\n",
    "\n",
    "            if self.test == 'federated':\n",
    "                losses2, losses3 = self.federated_averaging()\n",
    "\n",
    "            if self.test == 'bandits':\n",
    "                losses2 = 0\n",
    "                losses3 = 0\n",
    "                for client in range(self.total_clients):\n",
    "                    loss_test2, loss_train2, selected_clients2, selected_clients_UCB= self.bandits(client, i)\n",
    "                    losses2 += loss_test2\n",
    "                    if len(selected_clients2) < 1:\n",
    "                        losses3 += self.current_train_loss[str(client)].detach().cpu().numpy()\n",
    "                    else:\n",
    "                        losses3 += loss_train2\n",
    "                    self.phis[client, selected_clients2] += 1\n",
    "                    self.selected_clients_arr[i, client, selected_clients2] += 1\n",
    "                    self.phisUCB[client, selected_clients_UCB] += 1\n",
    "                fname = os.path.join('checkpoints_bandits', experiment_name, 'phi' + str(i) + '.txt')\n",
    "                np.savetxt(fname, self.phis)\n",
    "                fname = os.path.join('checkpoints_bandits', experiment_name, 'phi_UCB' + str(i) + '.txt')\n",
    "                np.savetxt(fname, self.phisUCB)\n",
    "\n",
    "            if self.test == 'mine':\n",
    "                losses2 = 0\n",
    "                losses3 = 0\n",
    "                for client in range(self.total_clients):\n",
    "                    loss_test2, loss_train2, selected_clients2 = self.my_method2(client)\n",
    "                    losses2 += loss_test2\n",
    "                    if len(selected_clients2) < 1:\n",
    "                        losses3 += self.current_train_loss[str(client)].detach().cpu().numpy()\n",
    "\n",
    "                    else:\n",
    "                        losses3 += loss_train2\n",
    "                    self.phis[client, selected_clients2] += 1\n",
    "                fname = os.path.join('checkpoints_bandits', experiment_name, 'phi' + str(i) + '.txt')\n",
    "                np.savetxt(fname, self.phis)\n",
    "\n",
    "            if self.test == 'optimal':\n",
    "                losses2, losses3 = self.optimal_fedavg()\n",
    "                losses2 = losses2.detach().cpu().numpy()\n",
    "                losses3 = losses3.detach().cpu().numpy()\n",
    "\n",
    "            if self.test != 'local' and self.test != 'centralized':\n",
    "                print('loss after my code: ', losses2)\n",
    "                print('train loss after my code: ', losses3)\n",
    "                loss_tests2.append(losses2)\n",
    "                loss_trains2.append(losses3)\n",
    "                fname = os.path.join('checkpoints_bandits', experiment_name, 'losses_test.txt')\n",
    "                np.savetxt(fname, loss_tests2)\n",
    "                fname = os.path.join('checkpoints_bandits', experiment_name, 'losses_train.txt')\n",
    "                np.savetxt(fname, loss_trains2)\n",
    "\n",
    "\n",
    "            else:\n",
    "                fname = os.path.join('checkpoints_bandits', experiment_name, 'losses_test.txt')\n",
    "                np.savetxt(fname, loss_tests)\n",
    "                fname = os.path.join('checkpoints_bandits', experiment_name, 'losses_train.txt')\n",
    "                np.savetxt(fname, loss_trains)\n",
    "\n",
    "            accuracy_val = self.calc_accuracy(self.dataloaders_test, self.len_test)\n",
    "            print('val accuracy: ', accuracy_val)\n",
    "\n",
    "            accuracy = self.calc_accuracy(self.dataloaders_really_test, self.len_really_test)\n",
    "            print('test accuracy: ', accuracy)\n",
    "            accuracies.append(accuracy)\n",
    "            if accuracy_val > best_accuracy:\n",
    "                print(best_accuracy)\n",
    "                print('accuracy is best accuracy')\n",
    "                print(self.accuracy_list)\n",
    "                best_accuracy = accuracy_val\n",
    "\n",
    "                # save all of this in a .txt file\n",
    "                fname = os.path.join('checkpoints_bandits', experiment_name, 'test_accuracies.txt')\n",
    "                np.savetxt(fname, self.accuracy_list)\n",
    "                fname = os.path.join('checkpoints_bandits', experiment_name, 'test_accuracy.txt')\n",
    "                np.savetxt(fname, [accuracy])\n",
    "            # accuracy_train = self.calc_accuracy(test=False)\n",
    "            # print(accuracy_train)\n",
    "            # accuracies_train.append(accuracy_train)\n",
    "            # print(self.phis)\n",
    "\n",
    "        # print(self.phis)\n",
    "        fname = os.path.join('checkpoints_bandits', experiment_name, 'accuracies.txt')\n",
    "        np.savetxt(fname, accuracies)\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(loss_trains, label='train loss before')\n",
    "        plt.plot(loss_tests, label='test loss before')\n",
    "        plt.plot(loss_trains2, label='train loss after')\n",
    "        plt.plot(loss_tests2, label='test loss after')\n",
    "        plt.title('loss curve')\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.savefig(os.path.join('checkpoints_bandits', experiment_name, 'loss_curve.png'))\n",
    "        plt.clf()\n",
    "        plt.plot(accuracies, label='test')\n",
    "        plt.plot(accuracies_train, label='train')\n",
    "        plt.title('accuracy progression')\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join('checkpoints_bandits', experiment_name, 'accuracy_progression.png'))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9d1fee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
